#!/usr/bin/env python3
"""
ç‰©ç†çº¦æŸä¼ æ’­ç½‘ç»œé›†æˆæµ‹è¯•
æµ‹è¯•å¢å¼ºç‰©ç†çº¦æŸç½‘ç»œä¸ç°æœ‰ç³»ç»Ÿçš„é›†æˆæ•ˆæœ
"""

import sys
import os
import logging
import time
import json
from typing import Dict, List, Any

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥åç«¯æ¨¡å—
sys.path.append(os.path.join(os.path.dirname(__file__), 'refactored_backend'))

from enhanced_physical_constraint_network import EnhancedPhysicalConstraintNetwork
from integrated_reasoning_pipeline import IntegratedReasoningPipeline

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class PhysicsConstraintIntegrationTester:
    """ç‰©ç†çº¦æŸé›†æˆæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.test_results = []
        
    def run_comprehensive_tests(self) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        
        print("ğŸ§ª ç‰©ç†çº¦æŸä¼ æ’­ç½‘ç»œé›†æˆæµ‹è¯•")
        print("=" * 60)
        
        test_results = {
            "test_summary": {
                "total_tests": 0,
                "passed_tests": 0,
                "failed_tests": 0,
                "total_execution_time": 0.0
            },
            "individual_tests": [],
            "performance_metrics": {},
            "integration_analysis": {}
        }
        
        start_time = time.time()
        
        # æµ‹è¯•1: åŸºç¡€çº¦æŸç½‘ç»œåŠŸèƒ½
        self.logger.info("æ‰§è¡Œæµ‹è¯•1: åŸºç¡€çº¦æŸç½‘ç»œåŠŸèƒ½")
        test1_result = self._test_basic_constraint_network()
        test_results["individual_tests"].append(test1_result)
        
        # æµ‹è¯•2: é›†æˆæ¨ç†ç®¡é“
        self.logger.info("æ‰§è¡Œæµ‹è¯•2: é›†æˆæ¨ç†ç®¡é“")
        test2_result = self._test_integrated_pipeline()
        test_results["individual_tests"].append(test2_result)
        
        # æµ‹è¯•3: æ€§èƒ½åŸºå‡†æµ‹è¯•
        self.logger.info("æ‰§è¡Œæµ‹è¯•3: æ€§èƒ½åŸºå‡†æµ‹è¯•")
        test3_result = self._test_performance_benchmarks()
        test_results["individual_tests"].append(test3_result)
        
        # æµ‹è¯•4: é”™è¯¯å¤„ç†å’Œè¾¹ç•Œæƒ…å†µ
        self.logger.info("æ‰§è¡Œæµ‹è¯•4: é”™è¯¯å¤„ç†æµ‹è¯•")
        test4_result = self._test_error_handling()
        test_results["individual_tests"].append(test4_result)
        
        # æµ‹è¯•5: å‰ç«¯æ•°æ®æ ¼å¼å…¼å®¹æ€§
        self.logger.info("æ‰§è¡Œæµ‹è¯•5: å‰ç«¯å…¼å®¹æ€§æµ‹è¯•")
        test5_result = self._test_frontend_compatibility()
        test_results["individual_tests"].append(test5_result)
        
        # è®¡ç®—æ±‡æ€»ç»“æœ
        total_time = time.time() - start_time
        passed_tests = sum(1 for test in test_results["individual_tests"] if test["passed"])
        
        test_results["test_summary"].update({
            "total_tests": len(test_results["individual_tests"]),
            "passed_tests": passed_tests,
            "failed_tests": len(test_results["individual_tests"]) - passed_tests,
            "total_execution_time": total_time,
            "success_rate": passed_tests / len(test_results["individual_tests"]) * 100
        })
        
        # ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡
        test_results["performance_metrics"] = self._generate_performance_metrics(test_results["individual_tests"])
        
        # ç”Ÿæˆé›†æˆåˆ†æ
        test_results["integration_analysis"] = self._analyze_integration_quality(test_results["individual_tests"])
        
        return test_results
    
    def _test_basic_constraint_network(self) -> Dict[str, Any]:
        """æµ‹è¯•åŸºç¡€çº¦æŸç½‘ç»œåŠŸèƒ½"""
        
        test_result = {
            "test_name": "åŸºç¡€çº¦æŸç½‘ç»œåŠŸèƒ½",
            "passed": False,
            "execution_time": 0.0,
            "details": {},
            "errors": []
        }
        
        try:
            start_time = time.time()
            
            # åˆ›å»ºçº¦æŸç½‘ç»œå®ä¾‹
            network = EnhancedPhysicalConstraintNetwork()
            
            # æ‰§è¡ŒåŸºç¡€æµ‹è¯•
            basic_test_result = network.test_constraint_network()
            
            test_result["execution_time"] = time.time() - start_time
            test_result["details"] = {
                "constraint_network_created": True,
                "test_execution_success": basic_test_result["test_success"],
                "laws_identified": basic_test_result["laws_identified"],
                "constraints_generated": basic_test_result["constraints_generated"],
                "satisfaction_rate": basic_test_result["constraint_satisfaction_rate"],
                "physical_consistency": basic_test_result["physical_consistency"]
            }
            
            # éªŒè¯ç»“æœ
            if (basic_test_result["test_success"] and 
                basic_test_result["laws_identified"] > 0 and
                basic_test_result["constraints_generated"] > 0):
                test_result["passed"] = True
            else:
                test_result["errors"].append("åŸºç¡€çº¦æŸç½‘ç»œæµ‹è¯•æœªé€šè¿‡é¢„æœŸæ ‡å‡†")
                
        except Exception as e:
            test_result["errors"].append(f"çº¦æŸç½‘ç»œæµ‹è¯•å¤±è´¥: {str(e)}")
            test_result["execution_time"] = time.time() - start_time
        
        return test_result
    
    def _test_integrated_pipeline(self) -> Dict[str, Any]:
        """æµ‹è¯•é›†æˆæ¨ç†ç®¡é“"""
        
        test_result = {
            "test_name": "é›†æˆæ¨ç†ç®¡é“",
            "passed": False,
            "execution_time": 0.0,
            "details": {},
            "errors": []
        }
        
        try:
            start_time = time.time()
            
            # åˆ›å»ºé›†æˆç®¡é“
            pipeline = IntegratedReasoningPipeline()
            
            # æµ‹è¯•é—®é¢˜
            test_problem = "å°æ˜æœ‰5ä¸ªè‹¹æœï¼Œåˆä¹°äº†3ä¸ªè‹¹æœï¼Œç°åœ¨æ€»å…±æœ‰å¤šå°‘ä¸ªè‹¹æœï¼Ÿ"
            
            # æ‰§è¡Œæ¨ç†
            result = pipeline.solve_problem(test_problem)
            
            test_result["execution_time"] = time.time() - start_time
            test_result["details"] = {
                "pipeline_created": True,
                "reasoning_success": result.success,
                "final_answer": result.final_solution.get("answer"),
                "confidence_score": result.confidence_score,
                "steps_completed": len(result.reasoning_steps),
                "constraint_integration": result.enhanced_constraints.get("success", False),
                "constraint_satisfaction_rate": result.enhanced_constraints.get("network_metrics", {}).get("satisfaction_rate", 0)
            }
            
            # éªŒè¯é›†æˆæ•ˆæœ
            if (result.success and 
                result.final_solution.get("answer") == 8 and  # 5 + 3 = 8
                result.confidence_score > 0.6 and
                result.enhanced_constraints.get("success", False)):
                test_result["passed"] = True
            else:
                test_result["errors"].append(f"é›†æˆæ¨ç†ç»“æœä¸ç¬¦åˆé¢„æœŸ: ç­”æ¡ˆ={result.final_solution.get('answer')}, ç½®ä¿¡åº¦={result.confidence_score}")
                
        except Exception as e:
            test_result["errors"].append(f"é›†æˆæ¨ç†æµ‹è¯•å¤±è´¥: {str(e)}")
            test_result["execution_time"] = time.time() - start_time
        
        return test_result
    
    def _test_performance_benchmarks(self) -> Dict[str, Any]:
        """æµ‹è¯•æ€§èƒ½åŸºå‡†"""
        
        test_result = {
            "test_name": "æ€§èƒ½åŸºå‡†æµ‹è¯•",
            "passed": False,
            "execution_time": 0.0,
            "details": {},
            "errors": []
        }
        
        try:
            start_time = time.time()
            
            pipeline = IntegratedReasoningPipeline()
            
            # æ€§èƒ½æµ‹è¯•ç”¨ä¾‹
            performance_tests = [
                "å°æ˜æœ‰10ä¸ªè‹¹æœï¼Œåƒäº†3ä¸ªï¼Œè¿˜å‰©å‡ ä¸ªï¼Ÿ",
                "æ•™å®¤é‡Œæœ‰25ä¸ªå­¦ç”Ÿï¼Œå…¶ä¸­12ä¸ªæ˜¯å¥³ç”Ÿï¼Œç”·ç”Ÿæœ‰å¤šå°‘ä¸ªï¼Ÿ", 
                "å•†åº—æœ‰50ä¸ªæ©™å­ï¼Œå–å‡º20ä¸ªï¼Œåˆè¿›è´§15ä¸ªï¼Œç°åœ¨æœ‰å¤šå°‘ä¸ªï¼Ÿ",
                "ä¸€æœ¬ä¹¦æœ‰120é¡µï¼Œå°çº¢æ¯å¤©è¯»8é¡µï¼Œéœ€è¦å¤šå°‘å¤©è¯»å®Œï¼Ÿ",
                "å­¦æ ¡æœ‰3ä¸ªç­çº§ï¼Œæ¯ç­30ä¸ªå­¦ç”Ÿï¼Œæ€»å…±æœ‰å¤šå°‘å­¦ç”Ÿï¼Ÿ"
            ]
            
            execution_times = []
            accuracy_scores = []
            constraint_success_rates = []
            
            for problem in performance_tests:
                problem_start = time.time()
                result = pipeline.solve_problem(problem)
                problem_time = time.time() - problem_start
                
                execution_times.append(problem_time)
                accuracy_scores.append(result.confidence_score)
                
                if result.enhanced_constraints.get("success"):
                    satisfaction_rate = result.enhanced_constraints.get("network_metrics", {}).get("satisfaction_rate", 0)
                    constraint_success_rates.append(satisfaction_rate)
            
            avg_execution_time = sum(execution_times) / len(execution_times)
            avg_accuracy = sum(accuracy_scores) / len(accuracy_scores)
            avg_constraint_success = sum(constraint_success_rates) / len(constraint_success_rates) if constraint_success_rates else 0
            
            test_result["execution_time"] = time.time() - start_time
            test_result["details"] = {
                "problems_tested": len(performance_tests),
                "average_execution_time": avg_execution_time,
                "max_execution_time": max(execution_times),
                "min_execution_time": min(execution_times),
                "average_accuracy": avg_accuracy,
                "average_constraint_success_rate": avg_constraint_success,
                "performance_meets_target": avg_execution_time < 1.0  # ç›®æ ‡ï¼š1ç§’å†…
            }
            
            # æ€§èƒ½éªŒè¯
            if (avg_execution_time < 1.0 and 
                avg_accuracy > 0.6 and
                avg_constraint_success > 0.8):
                test_result["passed"] = True
            else:
                test_result["errors"].append(f"æ€§èƒ½æœªè¾¾æ ‡: å¹³å‡æ—¶é—´={avg_execution_time:.3f}s, å¹³å‡å‡†ç¡®åº¦={avg_accuracy:.3f}")
                
        except Exception as e:
            test_result["errors"].append(f"æ€§èƒ½æµ‹è¯•å¤±è´¥: {str(e)}")
            test_result["execution_time"] = time.time() - start_time
        
        return test_result
    
    def _test_error_handling(self) -> Dict[str, Any]:
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        
        test_result = {
            "test_name": "é”™è¯¯å¤„ç†æµ‹è¯•",
            "passed": False,
            "execution_time": 0.0,
            "details": {},
            "errors": []
        }
        
        try:
            start_time = time.time()
            
            pipeline = IntegratedReasoningPipeline()
            
            # é”™è¯¯æµ‹è¯•ç”¨ä¾‹
            error_test_cases = [
                "",  # ç©ºå­—ç¬¦ä¸²
                "è¿™ä¸æ˜¯ä¸€ä¸ªæ•°å­¦é—®é¢˜",  # éæ•°å­¦é—®é¢˜
                "å°æ˜æœ‰è‹¹æœä¸ª",  # è¯­æ³•é”™è¯¯
                "1/0 ç­‰äºå¤šå°‘ï¼Ÿ",  # é™¤é›¶é”™è¯¯
                "å°æ˜æœ‰-5ä¸ªè‹¹æœ",  # è´Ÿæ•°é—®é¢˜
            ]
            
            error_handling_scores = []
            
            for test_case in error_test_cases:
                try:
                    result = pipeline.solve_problem(test_case)
                    # æ£€æŸ¥æ˜¯å¦ä¼˜é›…å¤„ç†é”™è¯¯
                    if not result.success and result.error_message:
                        error_handling_scores.append(1.0)  # æ­£ç¡®å¤„ç†é”™è¯¯
                    elif result.success:
                        error_handling_scores.append(0.5)  # å¯èƒ½çš„å‡é˜³æ€§
                    else:
                        error_handling_scores.append(0.0)  # å¤„ç†ä¸å½“
                except Exception:
                    error_handling_scores.append(0.0)  # æœªæ•è·å¼‚å¸¸
            
            avg_error_handling = sum(error_handling_scores) / len(error_handling_scores)
            
            test_result["execution_time"] = time.time() - start_time
            test_result["details"] = {
                "error_cases_tested": len(error_test_cases),
                "error_handling_score": avg_error_handling,
                "graceful_degradation": avg_error_handling > 0.7
            }
            
            if avg_error_handling > 0.7:
                test_result["passed"] = True
            else:
                test_result["errors"].append(f"é”™è¯¯å¤„ç†ä¸å¤Ÿå¥å£®: å¾—åˆ†={avg_error_handling:.3f}")
                
        except Exception as e:
            test_result["errors"].append(f"é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {str(e)}")
            test_result["execution_time"] = time.time() - start_time
        
        return test_result
    
    def _test_frontend_compatibility(self) -> Dict[str, Any]:
        """æµ‹è¯•å‰ç«¯å…¼å®¹æ€§"""
        
        test_result = {
            "test_name": "å‰ç«¯å…¼å®¹æ€§æµ‹è¯•",
            "passed": False,
            "execution_time": 0.0,
            "details": {},
            "errors": []
        }
        
        try:
            start_time = time.time()
            
            pipeline = IntegratedReasoningPipeline()
            result = pipeline.solve_problem("å°æ˜æœ‰5ä¸ªè‹¹æœï¼Œä¹°äº†3ä¸ªï¼Œæ€»å…±æœ‰å¤šå°‘ä¸ªï¼Ÿ")
            
            # æ£€æŸ¥å‰ç«¯æ‰€éœ€çš„æ•°æ®ç»“æ„
            required_fields = [
                "enhanced_constraints",
                "final_solution", 
                "reasoning_steps",
                "confidence_score"
            ]
            
            frontend_compatibility_checks = []
            
            # æ£€æŸ¥åŸºç¡€å­—æ®µ
            for field in required_fields:
                if hasattr(result, field):
                    frontend_compatibility_checks.append(True)
                else:
                    frontend_compatibility_checks.append(False)
                    test_result["errors"].append(f"ç¼ºå°‘å‰ç«¯å¿…éœ€å­—æ®µ: {field}")
            
            # æ£€æŸ¥çº¦æŸæ•°æ®æ ¼å¼
            if result.enhanced_constraints:
                constraint_format_valid = all(key in result.enhanced_constraints for key in [
                    "applicable_physics_laws", "generated_constraints", "constraint_solution"
                ])
                frontend_compatibility_checks.append(constraint_format_valid)
                if not constraint_format_valid:
                    test_result["errors"].append("çº¦æŸæ•°æ®æ ¼å¼ä¸ç¬¦åˆå‰ç«¯è¦æ±‚")
            
            # æ£€æŸ¥å¯åºåˆ—åŒ–æ€§
            try:
                json.dumps(result.enhanced_constraints, default=str)
                frontend_compatibility_checks.append(True)
            except Exception as e:
                frontend_compatibility_checks.append(False)
                test_result["errors"].append(f"æ•°æ®ä¸å¯åºåˆ—åŒ–: {str(e)}")
            
            compatibility_score = sum(frontend_compatibility_checks) / len(frontend_compatibility_checks)
            
            test_result["execution_time"] = time.time() - start_time
            test_result["details"] = {
                "compatibility_checks": len(frontend_compatibility_checks),
                "compatibility_score": compatibility_score,
                "serializable": len(test_result["errors"]) == 0
            }
            
            if compatibility_score >= 0.9:
                test_result["passed"] = True
            else:
                test_result["errors"].append(f"å‰ç«¯å…¼å®¹æ€§ä¸è¶³: å¾—åˆ†={compatibility_score:.3f}")
                
        except Exception as e:
            test_result["errors"].append(f"å‰ç«¯å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {str(e)}")
            test_result["execution_time"] = time.time() - start_time
        
        return test_result
    
    def _generate_performance_metrics(self, test_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡"""
        
        total_execution_time = sum(test["execution_time"] for test in test_results)
        passed_tests = [test for test in test_results if test["passed"]]
        
        return {
            "total_execution_time": total_execution_time,
            "average_test_time": total_execution_time / len(test_results),
            "fastest_test": min(test["execution_time"] for test in test_results),
            "slowest_test": max(test["execution_time"] for test in test_results),
            "pass_rate": len(passed_tests) / len(test_results) * 100,
            "performance_grade": self._calculate_performance_grade(test_results)
        }
    
    def _analyze_integration_quality(self, test_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æé›†æˆè´¨é‡"""
        
        integration_scores = []
        
        for test in test_results:
            if test["passed"]:
                integration_scores.append(1.0)
            elif test["details"]:
                # éƒ¨åˆ†æˆåŠŸçš„æµ‹è¯•ç»™äºˆéƒ¨åˆ†åˆ†æ•°
                integration_scores.append(0.5)
            else:
                integration_scores.append(0.0)
        
        avg_integration_quality = sum(integration_scores) / len(integration_scores)
        
        return {
            "integration_quality_score": avg_integration_quality,
            "integration_grade": "A" if avg_integration_quality >= 0.9 else 
                               "B" if avg_integration_quality >= 0.7 else
                               "C" if avg_integration_quality >= 0.5 else "D",
            "key_strengths": self._identify_strengths(test_results),
            "improvement_areas": self._identify_improvement_areas(test_results),
            "recommendation": self._generate_recommendation(avg_integration_quality)
        }
    
    def _calculate_performance_grade(self, test_results: List[Dict[str, Any]]) -> str:
        """è®¡ç®—æ€§èƒ½ç­‰çº§"""
        
        performance_test = next((test for test in test_results if test["test_name"] == "æ€§èƒ½åŸºå‡†æµ‹è¯•"), None)
        
        if not performance_test or not performance_test["passed"]:
            return "D"
        
        avg_time = performance_test["details"].get("average_execution_time", 999)
        
        if avg_time < 0.2:
            return "A+"
        elif avg_time < 0.5:
            return "A"
        elif avg_time < 1.0:
            return "B"
        elif avg_time < 2.0:
            return "C"
        else:
            return "D"
    
    def _identify_strengths(self, test_results: List[Dict[str, Any]]) -> List[str]:
        """è¯†åˆ«ä¼˜åŠ¿"""
        
        strengths = []
        
        for test in test_results:
            if test["passed"]:
                if test["test_name"] == "åŸºç¡€çº¦æŸç½‘ç»œåŠŸèƒ½":
                    strengths.append("çº¦æŸç½‘ç»œæ ¸å¿ƒåŠŸèƒ½ç¨³å®š")
                elif test["test_name"] == "é›†æˆæ¨ç†ç®¡é“":
                    strengths.append("ç³»ç»Ÿé›†æˆæ•ˆæœè‰¯å¥½")
                elif test["test_name"] == "æ€§èƒ½åŸºå‡†æµ‹è¯•":
                    strengths.append("æ€§èƒ½æŒ‡æ ‡è¾¾åˆ°é¢„æœŸ")
                elif test["test_name"] == "é”™è¯¯å¤„ç†æµ‹è¯•":
                    strengths.append("é”™è¯¯å¤„ç†æœºåˆ¶å¥å£®")
                elif test["test_name"] == "å‰ç«¯å…¼å®¹æ€§æµ‹è¯•":
                    strengths.append("å‰ç«¯é›†æˆå…¼å®¹æ€§å¥½")
        
        return strengths
    
    def _identify_improvement_areas(self, test_results: List[Dict[str, Any]]) -> List[str]:
        """è¯†åˆ«æ”¹è¿›é¢†åŸŸ"""
        
        improvements = []
        
        for test in test_results:
            if not test["passed"]:
                improvements.extend(test["errors"])
        
        return improvements
    
    def _generate_recommendation(self, integration_quality: float) -> str:
        """ç”Ÿæˆå»ºè®®"""
        
        if integration_quality >= 0.9:
            return "ç³»ç»Ÿé›†æˆè´¨é‡ä¼˜ç§€ï¼Œå¯ä»¥è¿›å…¥ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²"
        elif integration_quality >= 0.7:
            return "ç³»ç»Ÿé›†æˆè´¨é‡è‰¯å¥½ï¼Œå»ºè®®ä¿®å¤å°‘é‡é—®é¢˜åéƒ¨ç½²"
        elif integration_quality >= 0.5:
            return "ç³»ç»Ÿé›†æˆè´¨é‡ä¸€èˆ¬ï¼Œéœ€è¦è§£å†³ä¸»è¦é—®é¢˜åå†è€ƒè™‘éƒ¨ç½²"
        else:
            return "ç³»ç»Ÿé›†æˆè´¨é‡éœ€è¦å¤§å¹…æ”¹è¿›ï¼Œä¸å»ºè®®å½“å‰éƒ¨ç½²"

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    
    tester = PhysicsConstraintIntegrationTester()
    results = tester.run_comprehensive_tests()
    
    # æ‰“å°æµ‹è¯•ç»“æœ
    print(f"\nğŸ“Š æµ‹è¯•æ±‡æ€»æŠ¥å‘Š")
    print("=" * 60)
    
    summary = results["test_summary"]
    print(f"æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
    print(f"é€šè¿‡æµ‹è¯•: {summary['passed_tests']}")
    print(f"å¤±è´¥æµ‹è¯•: {summary['failed_tests']}")
    print(f"æˆåŠŸç‡: {summary['success_rate']:.1f}%")
    print(f"æ€»æ‰§è¡Œæ—¶é—´: {summary['total_execution_time']:.3f}ç§’")
    
    print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡")
    print("-" * 40)
    perf = results["performance_metrics"]
    print(f"æ€§èƒ½ç­‰çº§: {perf['performance_grade']}")
    print(f"å¹³å‡æµ‹è¯•æ—¶é—´: {perf['average_test_time']:.3f}ç§’")
    print(f"æœ€å¿«æµ‹è¯•: {perf['fastest_test']:.3f}ç§’")
    print(f"æœ€æ…¢æµ‹è¯•: {perf['slowest_test']:.3f}ç§’")
    
    print(f"\nğŸ” é›†æˆè´¨é‡åˆ†æ")
    print("-" * 40)
    integration = results["integration_analysis"]
    print(f"é›†æˆè´¨é‡ç­‰çº§: {integration['integration_grade']}")
    print(f"é›†æˆè´¨é‡å¾—åˆ†: {integration['integration_quality_score']:.3f}")
    print(f"å»ºè®®: {integration['recommendation']}")
    
    print(f"\nâœ… ä¸»è¦ä¼˜åŠ¿:")
    for strength in integration["key_strengths"]:
        print(f"  â€¢ {strength}")
    
    if integration["improvement_areas"]:
        print(f"\nâš ï¸  æ”¹è¿›é¢†åŸŸ:")
        for improvement in integration["improvement_areas"][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"  â€¢ {improvement}")
    
    print(f"\nğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ:")
    for test in results["individual_tests"]:
        status = "âœ… é€šè¿‡" if test["passed"] else "âŒ å¤±è´¥"
        print(f"  {status} - {test['test_name']} ({test['execution_time']:.3f}s)")
        if test["errors"]:
            for error in test["errors"][:2]:  # åªæ˜¾ç¤ºå‰2ä¸ªé”™è¯¯
                print(f"    âš ï¸  {error}")
    
    print(f"\nğŸ¯ ç»“è®º:")
    if summary['success_rate'] >= 80:
        print("âœ… ç‰©ç†çº¦æŸä¼ æ’­ç½‘ç»œé›†æˆæˆåŠŸï¼Œç³»ç»Ÿè¿è¡Œç¨³å®šï¼")
        print("ğŸ“¦ å»ºè®®: å¯ä»¥ç»§ç»­è¿›è¡Œå‰ç«¯é›†æˆå’Œç”¨æˆ·æµ‹è¯•")
    elif summary['success_rate'] >= 60:
        print("âš ï¸  ç‰©ç†çº¦æŸä¼ æ’­ç½‘ç»œåŸºæœ¬é›†æˆæˆåŠŸï¼Œå­˜åœ¨å°‘é‡é—®é¢˜")
        print("ğŸ”§ å»ºè®®: ä¿®å¤å‘ç°çš„é—®é¢˜åè¿›è¡Œè¿›ä¸€æ­¥æµ‹è¯•")
    else:
        print("âŒ ç‰©ç†çº¦æŸä¼ æ’­ç½‘ç»œé›†æˆå­˜åœ¨é‡å¤§é—®é¢˜")
        print("ğŸš§ å»ºè®®: éœ€è¦è§£å†³æ ¸å¿ƒé—®é¢˜åé‡æ–°æµ‹è¯•")

if __name__ == "__main__":
    main()