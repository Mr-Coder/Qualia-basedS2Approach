"""
ğŸš€ Comprehensive Optimization Demo
ç»¼åˆä¼˜åŒ–æ¼”ç¤º - å±•ç¤ºå››ä¸ªæ–¹å‘çš„æ•´åˆåº”ç”¨

å››ä¸ªä¼˜åŒ–æ–¹å‘ï¼š
ğŸš€ é›¶ä»£ç æ·»åŠ æ–°é¢˜ç›® (åŠ¨æ€ä»æ•°æ®é›†åŠ è½½)
ğŸ§  æ™ºèƒ½åˆ†ç±»å’Œæ¨¡æ¿åŒ¹é… (10ç§é¢˜å‹è‡ªåŠ¨è¯†åˆ«)
ğŸ“Š æ‰¹é‡å¤„ç†å’Œè´¨é‡è¯„ä¼° (æ ‡å‡†åŒ–æµç¨‹)
ğŸ”§ é«˜åº¦å¯æ‰©å±•æ¶æ„ (æ¨¡å—åŒ–è®¾è®¡)
"""

import asyncio
import json
import time
from pathlib import Path
from typing import Any, Dict, List

from src.processors.batch_processor import BatchProcessor
# å¯¼å…¥æˆ‘ä»¬åˆ›å»ºçš„å››ä¸ªä¼˜åŒ–æ¨¡å—
from src.processors.dynamic_dataset_manager import (DynamicDatasetManager,
                                                    ProblemBatch)
from src.processors.intelligent_classifier import (IntelligentClassifier,
                                                   ProblemType)
from src.processors.scalable_architecture import (BasePlugin, ModularFramework,
                                                  ModuleType, PluginInfo)


class ComprehensiveOptimizationDemo:
    """ğŸš€ ç»¼åˆä¼˜åŒ–æ¼”ç¤ºç³»ç»Ÿ"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç»¼åˆä¼˜åŒ–ç³»ç»Ÿ"""
        print("ğŸš€ åˆå§‹åŒ–ç»¼åˆä¼˜åŒ–ç³»ç»Ÿ")
        print("=" * 60)
        
        # 1. åˆå§‹åŒ–åŠ¨æ€æ•°æ®é›†ç®¡ç†å™¨
        print("ğŸš€ 1. åˆå§‹åŒ–åŠ¨æ€æ•°æ®é›†ç®¡ç†å™¨...")
        self.dataset_manager = DynamicDatasetManager(
            data_dirs=["Data", "datasets"],
            watch_mode=True,
            auto_reload=True
        )
        
        # 2. åˆå§‹åŒ–æ™ºèƒ½åˆ†ç±»å™¨
        print("\nğŸ§  2. åˆå§‹åŒ–æ™ºèƒ½åˆ†ç±»å™¨...")
        self.classifier = IntelligentClassifier()
        
        # 3. åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨
        print("\nğŸ“Š 3. åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨...")
        self.batch_processor = BatchProcessor(max_workers=4)
        
        # 4. åˆå§‹åŒ–æ¨¡å—åŒ–æ¡†æ¶
        print("\nğŸ”§ 4. åˆå§‹åŒ–æ¨¡å—åŒ–æ¡†æ¶...")
        self.framework = ModularFramework()
        
        # æ³¨å†Œè‡ªå®šä¹‰æ’ä»¶
        self._register_custom_plugins()
        
        print("\nâœ… ç»¼åˆä¼˜åŒ–ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ!")
        print("=" * 60)
    
    def _register_custom_plugins(self):
        """æ³¨å†Œè‡ªå®šä¹‰æ’ä»¶"""
        
        # æ•°æ®é›†åŠ è½½æ’ä»¶
        class DatasetLoaderPlugin(BasePlugin):
            def __init__(self, dataset_manager):
                self.dataset_manager = dataset_manager
            
            def get_info(self) -> PluginInfo:
                return PluginInfo(
                    plugin_id="dataset_loader",
                    name="æ•°æ®é›†åŠ è½½å™¨",
                    version="1.0.0",
                    description="åŠ¨æ€åŠ è½½æ•°æ®é›†",
                    module_type=ModuleType.PROCESSOR
                )
            
            def process(self, input_data: Any, config: Dict[str, Any] = None) -> Any:
                """åŠ è½½æ•°æ®é›†æ‰¹æ¬¡"""
                batch_size = config.get('batch_size', 10) if config else 10
                datasets = config.get('datasets', None) if config else None
                
                batch = self.dataset_manager.get_dynamic_batch(
                    batch_size=batch_size,
                    datasets=datasets
                )
                
                return {
                    'batch_id': batch.batch_id,
                    'problems': batch.problems,
                    'source_dataset': batch.source_dataset,
                    'timestamp': batch.timestamp.isoformat()
                }
        
        # æ™ºèƒ½åˆ†ç±»æ’ä»¶
        class ClassificationPlugin(BasePlugin):
            def __init__(self, classifier):
                self.classifier = classifier
            
            def get_info(self) -> PluginInfo:
                return PluginInfo(
                    plugin_id="intelligent_classifier",
                    name="æ™ºèƒ½åˆ†ç±»å™¨",
                    version="1.0.0",
                    description="10ç§é¢˜å‹è‡ªåŠ¨è¯†åˆ«",
                    module_type=ModuleType.CLASSIFIER
                )
            
            def process(self, input_data: Any, config: Dict[str, Any] = None) -> Any:
                """åˆ†ç±»é—®é¢˜"""
                if isinstance(input_data, dict) and 'problems' in input_data:
                    problems = input_data['problems']
                    classified_results = []
                    
                    for problem in problems:
                        # æå–é—®é¢˜æ–‡æœ¬
                        problem_text = self._extract_problem_text(problem)
                        
                        # åˆ†ç±»
                        classification = self.classifier.classify(problem_text)
                        
                        classified_results.append({
                            'original_problem': problem,
                            'problem_text': problem_text,
                            'classification': {
                                'type': classification.problem_type.value,
                                'confidence': classification.confidence,
                                'template': classification.template_match,
                                'entities': classification.extracted_entities,
                                'reasoning': classification.reasoning
                            }
                        })
                    
                    input_data['classified_problems'] = classified_results
                    input_data['classification_summary'] = self._generate_classification_summary(classified_results)
                
                return input_data
            
            def _extract_problem_text(self, problem: Dict) -> str:
                """æå–é—®é¢˜æ–‡æœ¬"""
                for field in ['problem', 'question', 'text', 'body']:
                    if field in problem:
                        return str(problem[field])
                return str(problem)
            
            def _generate_classification_summary(self, results: List[Dict]) -> Dict:
                """ç”Ÿæˆåˆ†ç±»æ‘˜è¦"""
                type_counts = {}
                total_confidence = 0
                
                for result in results:
                    ptype = result['classification']['type']
                    confidence = result['classification']['confidence']
                    
                    type_counts[ptype] = type_counts.get(ptype, 0) + 1
                    total_confidence += confidence
                
                return {
                    'total_problems': len(results),
                    'type_distribution': type_counts,
                    'average_confidence': total_confidence / len(results) if results else 0
                }
        
        # æ³¨å†Œæ’ä»¶
        self.framework.register_processor(
            type('DatasetLoaderPlugin', (DatasetLoaderPlugin,), {})
        )
        
        # åˆ›å»ºåˆ†ç±»æ’ä»¶ç±»å¹¶æ³¨å†Œ
        classifier_plugin_class = type(
            'ClassificationPlugin', 
            (BasePlugin,), 
            {
                '__init__': lambda self: ClassificationPlugin.__init__(self, self.outer.classifier),
                'get_info': ClassificationPlugin.get_info,
                'process': ClassificationPlugin.process,
                '_extract_problem_text': ClassificationPlugin._extract_problem_text,
                '_generate_classification_summary': ClassificationPlugin._generate_classification_summary,
                'outer': self
            }
        )
        
        self.framework.register_processor(classifier_plugin_class)
    
    def demo_all_optimizations(self):
        """ğŸ¯ æ¼”ç¤ºæ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½"""
        print("\nğŸ¯ ç»¼åˆä¼˜åŒ–åŠŸèƒ½æ¼”ç¤º")
        print("=" * 60)
        
        # 1. é›¶ä»£ç æ·»åŠ æ–°é¢˜ç›®æ¼”ç¤º
        self._demo_dynamic_dataset_loading()
        
        # 2. æ™ºèƒ½åˆ†ç±»æ¼”ç¤º
        self._demo_intelligent_classification()
        
        # 3. æ‰¹é‡å¤„ç†æ¼”ç¤º
        self._demo_batch_processing()
        
        # 4. å¯æ‰©å±•æ¶æ„æ¼”ç¤º
        self._demo_scalable_architecture()
        
        # 5. ç»¼åˆæµç¨‹æ¼”ç¤º
        self._demo_integrated_workflow()
    
    def _demo_dynamic_dataset_loading(self):
        """ğŸš€ æ¼”ç¤ºåŠ¨æ€æ•°æ®é›†åŠ è½½"""
        print("\nğŸš€ 1. åŠ¨æ€æ•°æ®é›†åŠ è½½æ¼”ç¤º")
        print("-" * 40)
        
        # æ˜¾ç¤ºå‘ç°çš„æ•°æ®é›†
        stats = self.dataset_manager.get_stats()
        print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        print(f"  å·²å‘ç°æ•°æ®é›†: {stats['total_datasets']}")
        print(f"  å¯ç”¨æ•°æ®é›†: {', '.join(stats['available_datasets'][:5])}...")
        
        # è·å–åŠ¨æ€æ‰¹æ¬¡
        batch = self.dataset_manager.get_dynamic_batch(batch_size=3)
        print(f"\nğŸ“¦ åŠ¨æ€æ‰¹æ¬¡:")
        print(f"  æ‰¹æ¬¡ID: {batch.batch_id}")
        print(f"  æ•°æ®æ¥æº: {batch.source_dataset}")
        print(f"  é—®é¢˜æ•°é‡: {len(batch.problems)}")
        
        # æ˜¾ç¤ºå‰2ä¸ªé—®é¢˜
        for i, problem in enumerate(batch.problems[:2]):
            problem_text = str(problem)[:100] + "..." if len(str(problem)) > 100 else str(problem)
            print(f"  é—®é¢˜ {i+1}: {problem_text}")
        
        return batch
    
    def _demo_intelligent_classification(self):
        """ğŸ§  æ¼”ç¤ºæ™ºèƒ½åˆ†ç±»"""
        print("\nğŸ§  2. æ™ºèƒ½åˆ†ç±»æ¼”ç¤º")
        print("-" * 40)
        
        # æµ‹è¯•é—®é¢˜
        test_problems = [
            "è®¡ç®— 15 + 28 = ?",
            "å°æ˜ä¹°äº†5æœ¬ä¹¦ï¼Œæ¯æœ¬12å…ƒï¼Œæ€»å…±èŠ±è´¹å¤šå°‘é’±ï¼Ÿ",
            "è§£æ–¹ç¨‹ï¼š3x + 7 = 22",
            "ä¸€ä¸ªæ­£æ–¹å½¢è¾¹é•¿6ç±³ï¼Œæ±‚å…¶é¢ç§¯",
            "ä»8ä¸ªäººä¸­é€‰æ‹©3ä¸ªäººï¼Œæœ‰å¤šå°‘ç§é€‰æ³•ï¼Ÿ"
        ]
        
        print(f"ğŸ§ª åˆ†ç±»æµ‹è¯• ({len(test_problems)} ä¸ªé—®é¢˜):")
        
        classification_results = []
        for i, problem in enumerate(test_problems, 1):
            result = self.classifier.classify(problem)
            classification_results.append(result)
            
            print(f"\n  é—®é¢˜ {i}: {problem}")
            print(f"    ç±»å‹: {result.problem_type.value}")
            print(f"    ç½®ä¿¡åº¦: {result.confidence:.2f}")
            print(f"    æ¨¡æ¿: {result.template_match}")
        
        # æ˜¾ç¤ºåˆ†ç±»ç»Ÿè®¡
        stats = self.classifier.get_statistics()
        if stats['type_percentages']:
            print(f"\nğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
            for ptype, percentage in stats['type_percentages'].items():
                print(f"  {ptype}: {percentage}%")
        
        return classification_results
    
    def _demo_batch_processing(self):
        """ğŸ“Š æ¼”ç¤ºæ‰¹é‡å¤„ç†"""
        print("\nğŸ“Š 3. æ‰¹é‡å¤„ç†æ¼”ç¤º")
        print("-" * 40)
        
        # å®šä¹‰å¤„ç†å‡½æ•°
        def comprehensive_math_processor(problem):
            """ç»¼åˆæ•°å­¦å¤„ç†å™¨"""
            time.sleep(0.05)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            
            try:
                # æå–é—®é¢˜æ–‡æœ¬
                if isinstance(problem, dict):
                    text = problem.get('problem', str(problem))
                else:
                    text = str(problem)
                
                # ç®€å•åˆ†ç±»
                if any(op in text for op in ['+', '-', '*', '/', 'Ã—', 'Ã·']):
                    problem_type = "ç®—æœ¯è¿ç®—"
                    difficulty = "ç®€å•"
                elif any(word in text for word in ['æ–¹ç¨‹', 'è§£', 'x', 'y']):
                    problem_type = "æ–¹ç¨‹æ±‚è§£"
                    difficulty = "ä¸­ç­‰"
                else:
                    problem_type = "åº”ç”¨é¢˜"
                    difficulty = "ä¸­ç­‰"
                
                return {
                    'original': problem,
                    'text': text,
                    'type': problem_type,
                    'difficulty': difficulty,
                    'is_correct': True,
                    'processing_time': 0.05,
                    'solution_steps': [f"è¯†åˆ«ä¸º{problem_type}", "è¿›è¡Œç›¸åº”å¤„ç†"]
                }
                
            except Exception as e:
                return {
                    'original': problem,
                    'error': str(e),
                    'is_correct': False
                }
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_data = [
            "25 + 37 = ?",
            "è§£æ–¹ç¨‹: 2x + 5 = 15",
            "å°ç‹ä¹°äº†3ä¸ªè‹¹æœï¼Œæ¯ä¸ª2å…ƒ",
            "è®¡ç®—æ­£æ–¹å½¢é¢ç§¯ï¼Œè¾¹é•¿5ç±³",
            "æ¦‚ç‡é—®é¢˜ï¼šæŠ•æ·ç¡¬å¸",
            "invalid_data",  # æ•…æ„çš„é”™è¯¯æ•°æ®
        ]
        
        # æäº¤æ‰¹å¤„ç†ä»»åŠ¡
        job_id = self.batch_processor.submit_job(
            name="ç»¼åˆæ•°å­¦é—®é¢˜å¤„ç†",
            input_data=test_data,
            processor_func=comprehensive_math_processor,
            quality_evaluator='math_problem_solver'
        )
        
        print(f"ğŸ“¤ æäº¤æ‰¹å¤„ç†ä»»åŠ¡: {job_id}")
        
        # å¤„ç†ä»»åŠ¡
        report = self.batch_processor.process_job(job_id)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nğŸ“‹ å¤„ç†æŠ¥å‘Š:")
        print(f"  æ€»é¡¹ç›®æ•°: {report.total_items}")
        print(f"  æˆåŠŸé¡¹ç›®: {report.successful_items}")
        print(f"  å¤±è´¥é¡¹ç›®: {report.failed_items}")
        print(f"  å¤„ç†æ—¶é—´: {report.processing_time:.2f}ç§’")
        print(f"  è´¨é‡ç­‰çº§: {report.quality_metrics.quality_level.value}")
        print(f"  æ€»ä½“åˆ†æ•°: {report.quality_metrics.overall_score:.2f}")
        
        if report.quality_metrics.recommendations:
            print(f"  æ”¹è¿›å»ºè®®: {', '.join(report.quality_metrics.recommendations)}")
        
        return report
    
    def _demo_scalable_architecture(self):
        """ğŸ”§ æ¼”ç¤ºå¯æ‰©å±•æ¶æ„"""
        print("\nğŸ”§ 4. å¯æ‰©å±•æ¶æ„æ¼”ç¤º")
        print("-" * 40)
        
        # åˆ›å»ºå¤„ç†ç®¡é“
        self.framework.create_pipeline("comprehensive_pipeline", [
            "simple_arithmetic",
            "problem_classifier"
        ])
        
        # æµ‹è¯•æ•°æ®
        test_expressions = ["15 + 23", "8 * 7", "56 / 8"]
        
        print(f"ğŸ”— æµ‹è¯•å¤„ç†ç®¡é“:")
        
        pipeline_results = []
        for expr in test_expressions:
            print(f"\n  è¾“å…¥: {expr}")
            try:
                result = self.framework.execute_pipeline("comprehensive_pipeline", expr)
                pipeline_results.append(result)
                print(f"  è¾“å‡º: {result}")
            except Exception as e:
                print(f"  é”™è¯¯: {e}")
        
        # æ˜¾ç¤ºæ¡†æ¶ç»Ÿè®¡
        registry_info = self.framework.plugin_manager.get_registry_info()
        print(f"\nğŸ“Š æ’ä»¶ç»Ÿè®¡:")
        for key, value in registry_info.items():
            print(f"  {key}: {value}")
        
        return pipeline_results
    
    def _demo_integrated_workflow(self):
        """ğŸ¯ æ¼”ç¤ºæ•´åˆå·¥ä½œæµ"""
        print("\nğŸ¯ 5. æ•´åˆå·¥ä½œæµæ¼”ç¤º")
        print("-" * 40)
        
        print("ğŸ”„ æ‰§è¡Œç«¯åˆ°ç«¯å¤„ç†æµç¨‹:")
        
        # æ­¥éª¤1: åŠ¨æ€åŠ è½½æ•°æ®
        print("  æ­¥éª¤1: ä»æ•°æ®é›†åŠ¨æ€åŠ è½½é—®é¢˜...")
        batch = self.dataset_manager.get_dynamic_batch(batch_size=5)
        
        # æ­¥éª¤2: æ‰¹é‡åˆ†ç±»
        print("  æ­¥éª¤2: æ™ºèƒ½åˆ†ç±»é—®é¢˜...")
        classification_results = []
        for problem in batch.problems:
            # æå–é—®é¢˜æ–‡æœ¬
            problem_text = self._extract_text_from_problem(problem)
            if problem_text:
                result = self.classifier.classify(problem_text)
                classification_results.append({
                    'problem': problem,
                    'text': problem_text,
                    'classification': result
                })
        
        # æ­¥éª¤3: è´¨é‡è¯„ä¼°
        print("  æ­¥éª¤3: è´¨é‡è¯„ä¼°å’Œç»Ÿè®¡...")
        quality_stats = self._calculate_workflow_quality(classification_results)
        
        # æ­¥éª¤4: ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        print("  æ­¥éª¤4: ç”Ÿæˆç»¼åˆæŠ¥å‘Š...")
        workflow_report = {
            'workflow_id': f"workflow_{int(time.time())}",
            'total_problems': len(batch.problems),
            'classified_problems': len(classification_results),
            'data_source': batch.source_dataset,
            'quality_metrics': quality_stats,
            'processing_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'performance_summary': {
                'data_loading': 'SUCCESS',
                'classification': 'SUCCESS',
                'quality_evaluation': 'SUCCESS',
                'overall_status': 'COMPLETED'
            }
        }
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nğŸ“Š æ•´åˆå·¥ä½œæµæŠ¥å‘Š:")
        print(f"  å·¥ä½œæµID: {workflow_report['workflow_id']}")
        print(f"  å¤„ç†é—®é¢˜æ•°: {workflow_report['total_problems']}")
        print(f"  åˆ†ç±»æˆåŠŸæ•°: {workflow_report['classified_problems']}")
        print(f"  æ•°æ®æ¥æº: {workflow_report['data_source']}")
        print(f"  å¹³å‡ç½®ä¿¡åº¦: {workflow_report['quality_metrics']['average_confidence']:.2f}")
        print(f"  å¤„ç†çŠ¶æ€: {workflow_report['performance_summary']['overall_status']}")
        
        # æ˜¾ç¤ºåˆ†ç±»åˆ†å¸ƒ
        if workflow_report['quality_metrics']['type_distribution']:
            print(f"  åˆ†ç±»åˆ†å¸ƒ:")
            for ptype, count in workflow_report['quality_metrics']['type_distribution'].items():
                print(f"    {ptype}: {count}")
        
        return workflow_report
    
    def _extract_text_from_problem(self, problem: Dict) -> str:
        """ä»é—®é¢˜å­—å…¸ä¸­æå–æ–‡æœ¬"""
        if isinstance(problem, str):
            return problem
        
        if isinstance(problem, dict):
            for field in ['problem', 'question', 'text', 'body']:
                if field in problem:
                    return str(problem[field])
        
        return str(problem)
    
    def _calculate_workflow_quality(self, results: List[Dict]) -> Dict:
        """è®¡ç®—å·¥ä½œæµè´¨é‡"""
        if not results:
            return {'average_confidence': 0, 'type_distribution': {}}
        
        total_confidence = sum(r['classification'].confidence for r in results)
        average_confidence = total_confidence / len(results)
        
        type_distribution = {}
        for result in results:
            ptype = result['classification'].problem_type.value
            type_distribution[ptype] = type_distribution.get(ptype, 0) + 1
        
        return {
            'average_confidence': average_confidence,
            'type_distribution': type_distribution,
            'total_classified': len(results)
        }
    
    def save_demo_results(self, output_dir: str = "demo_results"):
        """ğŸ’¾ ä¿å­˜æ¼”ç¤ºç»“æœ"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        print(f"\nğŸ’¾ ä¿å­˜æ¼”ç¤ºç»“æœåˆ°: {output_path}")
        
        # ä¿å­˜æ•°æ®é›†ç»Ÿè®¡
        dataset_stats = self.dataset_manager.get_stats()
        with open(output_path / "dataset_stats.json", 'w', encoding='utf-8') as f:
            json.dump(dataset_stats, f, ensure_ascii=False, indent=2, default=str)
        
        # ä¿å­˜åˆ†ç±»ç»Ÿè®¡
        classification_stats = self.classifier.get_statistics()
        with open(output_path / "classification_stats.json", 'w', encoding='utf-8') as f:
            json.dump(classification_stats, f, ensure_ascii=False, indent=2, default=str)
        
        # ä¿å­˜æ¡†æ¶é…ç½®
        self.framework.save_configuration(str(output_path / "framework_config.json"))
        
        print("âœ… æ¼”ç¤ºç»“æœå·²ä¿å­˜")
    
    def print_optimization_summary(self):
        """ğŸ“ˆ æ‰“å°ä¼˜åŒ–æ€»ç»“"""
        print("\nğŸ“ˆ å››æ–¹å‘ä¼˜åŒ–æ€»ç»“")
        print("=" * 60)
        
        print("ğŸš€ 1. é›¶ä»£ç æ·»åŠ æ–°é¢˜ç›®:")
        print("  âœ… è‡ªåŠ¨å‘ç°å’ŒåŠ è½½æ•°æ®é›†")
        print("  âœ… çƒ­é‡è½½å’Œæ–‡ä»¶ç›‘æ§")
        print("  âœ… åŠ¨æ€æ‰¹æ¬¡ç”Ÿæˆ")
        print("  âœ… å¤šæ ¼å¼æ”¯æŒ (JSON, JSONL, YAML)")
        
        print("\nğŸ§  2. æ™ºèƒ½åˆ†ç±»å’Œæ¨¡æ¿åŒ¹é…:")
        print("  âœ… 10ç§é¢˜å‹è‡ªåŠ¨è¯†åˆ«")
        print("  âœ… æ¨¡æ¿åŒ¹é…å’Œå®ä½“æå–")
        print("  âœ… ç½®ä¿¡åº¦è¯„ä¼°")
        print("  âœ… å¯æ‰©å±•æ¨¡å¼ç³»ç»Ÿ")
        
        print("\nğŸ“Š 3. æ‰¹é‡å¤„ç†å’Œè´¨é‡è¯„ä¼°:")
        print("  âœ… å¤šçº¿ç¨‹æ‰¹é‡å¤„ç†")
        print("  âœ… æ™ºèƒ½è´¨é‡è¯„ä¼°")
        print("  âœ… è¯¦ç»†å¤„ç†æŠ¥å‘Š")
        print("  âœ… æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–å»ºè®®")
        
        print("\nğŸ”§ 4. é«˜åº¦å¯æ‰©å±•æ¶æ„:")
        print("  âœ… æ’ä»¶ç³»ç»Ÿå’Œæ¨¡å—åŒ–è®¾è®¡")
        print("  âœ… å¤„ç†ç®¡é“å’Œå·¥ä½œæµ")
        print("  âœ… åŠ¨æ€åŠ è½½å’Œçƒ­æ’æ‹”")
        print("  âœ… é…ç½®ç®¡ç†å’Œäº‹ä»¶ç³»ç»Ÿ")
        
        print("\nğŸ¯ æ•´åˆæ•ˆæœ:")
        print("  âœ… ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–å¤„ç†")
        print("  âœ… é«˜æ€§èƒ½å’Œé«˜è´¨é‡")
        print("  âœ… æ˜“æ‰©å±•å’Œæ˜“ç»´æŠ¤")
        print("  âœ… æ™ºèƒ½åŒ–å’Œæ ‡å‡†åŒ–")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç»¼åˆä¼˜åŒ–æ¼”ç¤ºç¨‹åºå¯åŠ¨")
    print("=" * 80)
    
    try:
        # åˆ›å»ºæ¼”ç¤ºç³»ç»Ÿ
        demo = ComprehensiveOptimizationDemo()
        
        # æ‰§è¡Œå…¨é¢æ¼”ç¤º
        demo.demo_all_optimizations()
        
        # ä¿å­˜ç»“æœ
        demo.save_demo_results()
        
        # æ‰“å°æ€»ç»“
        demo.print_optimization_summary()
        
        print("\nğŸ‰ ç»¼åˆä¼˜åŒ–æ¼”ç¤ºå®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 