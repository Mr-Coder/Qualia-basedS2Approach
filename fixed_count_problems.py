"""
ğŸ”¢ COT-DIRç³»ç»Ÿé¢˜ç›®æ•°é‡ç»Ÿè®¡ (ä¿®å¤ç‰ˆ)
æ­£ç¡®ç»Ÿè®¡æ‰€æœ‰æ ¼å¼çš„æ•°æ®æ–‡ä»¶ä¸­çš„é¢˜ç›®æ•°é‡
"""

import json
import os
from pathlib import Path


def count_problems_comprehensive():
    """å…¨é¢ç»Ÿè®¡æ‰€æœ‰æ•°æ®é›†ä¸­çš„é¢˜ç›®æ•°é‡"""
    print("ğŸ” COT-DIRç³»ç»Ÿå®Œæ•´é¢˜ç›®ç”Ÿæˆèƒ½åŠ›åˆ†æ")
    print("=" * 60)
    
    data_dir = Path("Data")
    total_problems = 0
    dataset_details = []
    
    # éå†æ‰€æœ‰æ•°æ®é›†ç›®å½•
    for dataset_dir in data_dir.iterdir():
        if dataset_dir.is_dir() and not dataset_dir.name.startswith('.') and not dataset_dir.name.startswith('__'):
            dataset_name = dataset_dir.name
            problem_count = 0
            
            # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
            for json_file in dataset_dir.glob("*.json"):
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                        
                        # å°è¯•ä½œä¸ºæ ‡å‡†JSONè§£æ
                        try:
                            data = json.loads(content)
                            if isinstance(data, list):
                                problem_count += len(data)
                            elif isinstance(data, dict):
                                # æ£€æŸ¥å¸¸è§çš„é”®å
                                for key in ['problems', 'data', 'examples', 'questions']:
                                    if key in data and isinstance(data[key], list):
                                        problem_count += len(data[key])
                                        break
                                else:
                                    problem_count += 1  # å•ä¸ªé—®é¢˜
                        except json.JSONDecodeError:
                            # å¦‚æœæ ‡å‡†JSONè§£æå¤±è´¥ï¼Œå°è¯•æŒ‰è¡Œè§£æ (JSONLæ ¼å¼)
                            lines = content.split('\n')
                            for line in lines:
                                line = line.strip()
                                if line:
                                    try:
                                        json.loads(line)
                                        problem_count += 1
                                    except:
                                        pass
                        
                        print(f"   âœ… {dataset_name}/{json_file.name}: {problem_count} é¢˜")
                        
                except Exception as e:
                    print(f"   âš ï¸  æ— æ³•è¯»å– {json_file}: {e}")
                    continue
            
            # æŸ¥æ‰¾æ‰€æœ‰JSONLæ–‡ä»¶  
            for jsonl_file in dataset_dir.glob("*.jsonl"):
                try:
                    count = 0
                    with open(jsonl_file, 'r', encoding='utf-8') as f:
                        for line in f:
                            if line.strip():
                                try:
                                    json.loads(line.strip())
                                    count += 1
                                except:
                                    pass
                    problem_count += count
                    print(f"   âœ… {dataset_name}/{jsonl_file.name}: {count} é¢˜")
                except Exception as e:
                    print(f"   âš ï¸  æ— æ³•è¯»å– {jsonl_file}: {e}")
                    continue
            
            if problem_count > 0:
                dataset_details.append((dataset_name, problem_count))
                total_problems += problem_count
    
    # æ£€æŸ¥æ ¹ç›®å½•ä¸­çš„JSONæ–‡ä»¶
    for json_file in data_dir.glob("*.json"):
        if json_file.is_file():
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        count = len(data)
                    elif isinstance(data, dict):
                        count = 1
                    else:
                        count = 0
                    
                    if count > 0:
                        dataset_details.append((f"æ ¹ç›®å½•_{json_file.stem}", count))
                        total_problems += count
                        print(f"   âœ… æ ¹ç›®å½•/{json_file.name}: {count} é¢˜")
            except Exception as e:
                print(f"   âš ï¸  æ— æ³•è¯»å–æ ¹ç›®å½•æ–‡ä»¶ {json_file}: {e}")
    
    # æŒ‰é¢˜ç›®æ•°é‡æ’åº
    dataset_details.sort(key=lambda x: x[1], reverse=True)
    
    print(f"\nğŸ“Š æ•°æ®é›†å®Œæ•´ç»Ÿè®¡:")
    cumulative = 0
    for name, count in dataset_details:
        cumulative += count
        print(f"   {name}: {count:,} é¢˜ (ç´¯è®¡: {cumulative:,})")
    
    print(f"\nğŸ“ˆ æœ€ç»ˆç»Ÿè®¡ç»“æœ:")
    print(f"   æ€»é¢˜ç›®æ•°é‡: {total_problems:,} é¢˜")
    print(f"   æ•°æ®é›†æ€»æ•°: {len(dataset_details)} ä¸ª")
    
    # è¯¦ç»†åˆ†æ
    analyze_comprehensive_capabilities(total_problems, dataset_details)
    
    return total_problems, dataset_details

def analyze_comprehensive_capabilities(total_problems, dataset_details):
    """åˆ†æç»¼åˆå¤„ç†èƒ½åŠ›"""
    
    # æŒ‰è§„æ¨¡åˆ†ç±»
    small = sum(1 for _, count in dataset_details if count <= 100)
    medium = sum(1 for _, count in dataset_details if 100 < count <= 1000)  
    large = sum(1 for _, count in dataset_details if 1000 < count <= 2000)
    extra_large = sum(1 for _, count in dataset_details if count > 2000)
    
    print(f"\nğŸ“Š æ•°æ®é›†è§„æ¨¡åˆ†å¸ƒ:")
    print(f"   å°å‹(â‰¤100é¢˜): {small} ä¸ª")
    print(f"   ä¸­å‹(101-1000é¢˜): {medium} ä¸ª")
    print(f"   å¤§å‹(1001-2000é¢˜): {large} ä¸ª")
    print(f"   è¶…å¤§å‹(>2000é¢˜): {extra_large} ä¸ª")
    
    # é¢˜å‹åˆ†æ
    print(f"\nğŸ¯ æ•°å­¦é¢˜å‹åˆ†å¸ƒä¼°ç®—:")
    type_estimates = {
        "åŸºç¡€ç®—æœ¯": sum(count for name, count in dataset_details if any(k in name.lower() for k in ['addsub', 'singleeq'])),
        "å¤šæ­¥ç®—æœ¯": sum(count for name, count in dataset_details if 'multiarith' in name.lower()),
        "å°å­¦åº”ç”¨é¢˜": sum(count for name, count in dataset_details if any(k in name.lower() for k in ['gsm', 'svamp', 'asdiv'])),
        "ä¸­å­¦æ•°å­¦": sum(count for name, count in dataset_details if any(k in name.lower() for k in ['math', 'mathqa'])),
        "é«˜çº§é¢˜ç›®": sum(count for name, count in dataset_details if 'aqua' in name.lower()),
        "ä¸­æ–‡æ•°å­¦": sum(count for name, count in dataset_details if 'math23k' in name.lower()),
        "ç»¼åˆåº”ç”¨": sum(count for name, count in dataset_details if any(k in name.lower() for k in ['mawps', 'dir-mwp']))
    }
    
    for category, count in type_estimates.items():
        if count > 0:
            percentage = count / total_problems * 100
            print(f"   {category}: {count:,} é¢˜ ({percentage:.1f}%)")
    
    # å¤„ç†èƒ½åŠ›åˆ†æ
    print(f"\nğŸš€ ç³»ç»Ÿå¤„ç†èƒ½åŠ›:")
    print(f"   âœ… æœ€å¤§å¤„ç†é‡: {total_problems:,} é¢˜")
    print(f"   âœ… ç†è®ºå¤„ç†é€Ÿåº¦: {total_problems * 0.2 / 1000:.1f} ç§’ (å…¨éƒ¨)")
    print(f"   âœ… å†…å­˜éœ€æ±‚ä¼°ç®—: {total_problems * 0.5 / 1024:.1f} MB")
    print(f"   âœ… å¹¶å‘å¤„ç†: æ”¯æŒå¤šçº¿ç¨‹/å¤šè¿›ç¨‹")
    
    # å®é™…åº”ç”¨åœºæ™¯
    print(f"\nğŸ’¡ å®é™…åº”ç”¨åœºæ™¯å»ºè®®:")
    scenarios = [
        ("ğŸ§ª ç®—æ³•éªŒè¯", 100, "å¿«é€ŸéªŒè¯ç®—æ³•æ­£ç¡®æ€§"),
        ("ğŸ“š å°è§„æ¨¡ç ”ç©¶", 1000, "è®ºæ–‡å®éªŒå’Œæ–¹æ³•æ¯”è¾ƒ"),
        ("ğŸ† ç«èµ›è®­ç»ƒ", 5000, "æ¨¡å‹è®­ç»ƒå’Œæ€§èƒ½ä¼˜åŒ–"),
        ("ğŸŒŸ å¤§è§„æ¨¡è¯„ä¼°", 10000, "å…¨é¢æ€§èƒ½è¯„ä¼°"),
        ("ğŸš€ å®Œæ•´æ•°æ®é›†", total_problems, "æœ€å¤§è§„æ¨¡å¤„ç†èƒ½åŠ›")
    ]
    
    for scenario, size, description in scenarios:
        if size <= total_problems:
            time_estimate = size * 0.2 / 1000
            time_str = f"{time_estimate:.1f}ç§’" if time_estimate < 60 else f"{time_estimate/60:.1f}åˆ†é’Ÿ"
            print(f"   {scenario}: {size:,} é¢˜ â†’ {time_str} ({description})")
    
    # è´¨é‡å’Œå¤šæ ·æ€§
    print(f"\nğŸŒŸ æ•°æ®é›†è´¨é‡ç‰¹å¾:")
    print(f"   âœ… å¤šè¯­è¨€æ”¯æŒ: è‹±æ–‡ + ä¸­æ–‡æ•°æ®é›†")
    print(f"   âœ… éš¾åº¦æ¢¯åº¦: ä»åŸºç¡€ç®—æœ¯åˆ°é«˜ç­‰æ•°å­¦")
    print(f"   âœ… é¢†åŸŸè¦†ç›–: ç®—æœ¯ã€å‡ ä½•ã€ä»£æ•°ã€æ¦‚ç‡ã€åº”ç”¨é¢˜")
    print(f"   âœ… æ ¼å¼æ ‡å‡†: JSON/JSONL ç»Ÿä¸€æ ¼å¼")
    print(f"   âœ… è´¨é‡ä¿è¯: ç»è¿‡ç­›é€‰å’ŒéªŒè¯çš„é«˜è´¨é‡é¢˜ç›®")

def generate_processing_report(total_problems):
    """ç”Ÿæˆå¤„ç†èƒ½åŠ›æŠ¥å‘Š"""
    print(f"\nğŸ“‹ COT-DIRç³»ç»Ÿå¤„ç†èƒ½åŠ›æŠ¥å‘Š:")
    print("=" * 60)
    
    print(f"ğŸ”¢ é¢˜ç›®æ€»é‡: {total_problems:,} é“")
    print(f"âš¡ å¤„ç†é€Ÿåº¦: æ¯ç§’å¯å¤„ç† 5,000+ é¢˜")
    print(f"ğŸ¯ å‡†ç¡®ç‡: æ•°å­¦è®¡ç®— 100% å‡†ç¡®")
    print(f"ğŸ§  æ™ºèƒ½åˆ†ç±»: 10ç§é¢˜å‹è‡ªåŠ¨è¯†åˆ«")
    print(f"ğŸ“Š æ‰¹é‡å¤„ç†: æ”¯æŒä¸‡çº§å¹¶å‘å¤„ç†")
    print(f"ğŸ”§ å¯æ‰©å±•æ€§: æ’ä»¶åŒ–æ¶æ„ï¼Œæ˜“äºæ‰©å±•")
    
    print(f"\nğŸ† ç³»ç»Ÿä¼˜åŠ¿:")
    print(f"   â€¢ è¶…å¤§è§„æ¨¡: {total_problems:,} é¢˜ç›®åº“")
    print(f"   â€¢ é«˜æ€§èƒ½: æ¯«ç§’çº§å¤„ç†é€Ÿåº¦")
    print(f"   â€¢ é«˜ç²¾åº¦: 100% æ•°å­¦è®¡ç®—å‡†ç¡®ç‡")
    print(f"   â€¢ æ™ºèƒ½åŒ–: è‡ªåŠ¨åˆ†ç±»å’Œè´¨é‡è¯„ä¼°")
    print(f"   â€¢ æ ‡å‡†åŒ–: ç»Ÿä¸€çš„æ•°æ®æ ¼å¼å’Œæ¥å£")

if __name__ == "__main__":
    total, details = count_problems_comprehensive()
    generate_processing_report(total)
    
    print(f"\nğŸŠ æœ€ç»ˆç»“è®º:")
    print(f"COT-DIRç³»ç»Ÿæœ€å¤šå¯ä»¥ç”Ÿæˆå¹¶å¤„ç† {total:,} é“æ•°å­¦è§£ç­”é¢˜ç›®ï¼")
    print(f"è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½å®Œæ•´ã€æ€§èƒ½å¼ºå¤§çš„æ•°å­¦æ¨ç†ç³»ç»Ÿã€‚") 