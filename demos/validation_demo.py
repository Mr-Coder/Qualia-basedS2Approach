#!/usr/bin/env python3
"""
COT-DIR éªŒè¯å’Œæ€§èƒ½æµ‹è¯•æ¼”ç¤º
========================

å±•ç¤ºCOT-DIRæ•°å­¦æ¨ç†ç³»ç»Ÿçš„éªŒè¯å’Œæ€§èƒ½åˆ†æåŠŸèƒ½ï¼š
1. ç³»ç»ŸåŠŸèƒ½éªŒè¯
2. æ€§èƒ½åŸºå‡†æµ‹è¯•
3. å‡†ç¡®ç‡è¯„ä¼°
4. é”™è¯¯åˆ†æ

Author: COT-DIR Team
Date: 2025-01-31
"""

import json
import os
import sys
import time
from typing import Any, Dict, List

sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from data.loader import DataLoader
from data.preprocessor import Preprocessor
from reasoning_core.meta_knowledge import MetaKnowledge
from src.bridge.reasoning_bridge import ReasoningEngine


def validate_system_functionality():
    """éªŒè¯ç³»ç»ŸåŸºæœ¬åŠŸèƒ½"""
    print("\nğŸ” ç³»ç»ŸåŠŸèƒ½éªŒè¯")
    print("=" * 50)
    
    validation_results = {
        "component_initialization": False,
        "basic_reasoning": False,
        "meta_knowledge": False,
        "data_processing": False,
        "error_handling": False
    }
    
    try:
        # 1. ç»„ä»¶åˆå§‹åŒ–éªŒè¯
        print("1. ğŸ”§ éªŒè¯ç»„ä»¶åˆå§‹åŒ–...")
        loader = DataLoader()
        preprocessor = Preprocessor()
        engine = ReasoningEngine()
        meta_knowledge = MetaKnowledge()
        validation_results["component_initialization"] = True
        print("   âœ… æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
        
        # 2. åŸºç¡€æ¨ç†éªŒè¯
        print("2. ğŸ§  éªŒè¯åŸºç¡€æ¨ç†...")
        test_sample = {
            "problem": "3 + 5 = ?",
            "answer": "8",
            "id": "validation_test"
        }
        processed = preprocessor.process(test_sample)
        result = engine.solve(processed)
        
        if result and 'final_answer' in result:
            validation_results["basic_reasoning"] = True
            print("   âœ… åŸºç¡€æ¨ç†åŠŸèƒ½æ­£å¸¸")
        else:
            print("   âŒ åŸºç¡€æ¨ç†åŠŸèƒ½å¼‚å¸¸")
        
        # 3. å…ƒçŸ¥è¯†ç³»ç»ŸéªŒè¯
        print("3. ğŸ§  éªŒè¯å…ƒçŸ¥è¯†ç³»ç»Ÿ...")
        concepts = meta_knowledge.identify_concepts_in_text("è®¡ç®—åˆ†æ•° 1/2 + 1/3")
        strategies = meta_knowledge.suggest_strategies("è§£æ–¹ç¨‹ x + 5 = 10")
        
        if concepts and strategies:
            validation_results["meta_knowledge"] = True
            print("   âœ… å…ƒçŸ¥è¯†ç³»ç»ŸåŠŸèƒ½æ­£å¸¸")
        else:
            print("   âŒ å…ƒçŸ¥è¯†ç³»ç»ŸåŠŸèƒ½å¼‚å¸¸")
        
        # 4. æ•°æ®å¤„ç†éªŒè¯
        print("4. ğŸ“¦ éªŒè¯æ•°æ®å¤„ç†...")
        try:
            # å°è¯•å¤„ç†ä¸åŒæ ¼å¼çš„æ•°æ®
            test_cases = [
                {"problem": "ç®€å•é—®é¢˜", "answer": "ç­”æ¡ˆ"},
                {"question": "å¦ä¸€ç§æ ¼å¼", "solution": "è§£ç­”"},
                {"text": "ç¬¬ä¸‰ç§æ ¼å¼"}
            ]
            
            for case in test_cases:
                processed = preprocessor.process(case)
                if 'cleaned_text' in processed:
                    validation_results["data_processing"] = True
                    break
            
            if validation_results["data_processing"]:
                print("   âœ… æ•°æ®å¤„ç†åŠŸèƒ½æ­£å¸¸")
            else:
                print("   âŒ æ•°æ®å¤„ç†åŠŸèƒ½å¼‚å¸¸")
                
        except Exception as e:
            print(f"   âŒ æ•°æ®å¤„ç†éªŒè¯å¤±è´¥: {e}")
        
        # 5. é”™è¯¯å¤„ç†éªŒè¯
        print("5. âš ï¸ éªŒè¯é”™è¯¯å¤„ç†...")
        try:
            # æµ‹è¯•å¼‚å¸¸è¾“å…¥å¤„ç†
            invalid_inputs = [
                None,
                {},
                {"invalid": "data"},
                {"problem": ""}
            ]
            
            error_handled_count = 0
            for invalid_input in invalid_inputs:
                try:
                    if invalid_input is not None:
                        preprocessor.process(invalid_input)
                    error_handled_count += 1
                except:
                    error_handled_count += 1
            
            if error_handled_count >= len(invalid_inputs) // 2:
                validation_results["error_handling"] = True
                print("   âœ… é”™è¯¯å¤„ç†åŠŸèƒ½æ­£å¸¸")
            else:
                print("   âŒ é”™è¯¯å¤„ç†åŠŸèƒ½éœ€è¦æ”¹è¿›")
                
        except Exception as e:
            print(f"   âš ï¸ é”™è¯¯å¤„ç†éªŒè¯å¼‚å¸¸: {e}")
    
    except Exception as e:
        print(f"âŒ ç³»ç»ŸéªŒè¯å¤±è´¥: {e}")
    
    # è¾“å‡ºéªŒè¯æ€»ç»“
    print(f"\nğŸ“Š éªŒè¯ç»“æœæ€»ç»“:")
    passed_tests = sum(validation_results.values())
    total_tests = len(validation_results)
    
    for test_name, passed in validation_results.items():
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"   {test_name}: {status}")
    
    print(f"\nğŸ¯ éªŒè¯é€šè¿‡ç‡: {passed_tests}/{total_tests} ({passed_tests/total_tests*100:.1f}%)")
    
    return validation_results


def performance_benchmark():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\nâš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 50)
    
    try:
        engine = ReasoningEngine()
        preprocessor = Preprocessor()
        
        # æµ‹è¯•ç”¨ä¾‹ - ä¸åŒå¤æ‚åº¦çš„é—®é¢˜
        test_cases = [
            {
                "name": "ç®€å•ç®—æœ¯",
                "problems": [
                    "3 + 5",
                    "10 - 4",
                    "6 Ã— 7",
                    "24 Ã· 3"
                ]
            },
            {
                "name": "åº”ç”¨é—®é¢˜",
                "problems": [
                    "å°æ˜æœ‰5ä¸ªè‹¹æœï¼Œåƒäº†2ä¸ªï¼Œè¿˜å‰©å‡ ä¸ªï¼Ÿ",
                    "ä¸€ä»¶è¡£æœ50å…ƒï¼Œä¹°3ä»¶å¤šå°‘é’±ï¼Ÿ",
                    "ç­çº§æœ‰30äººï¼Œæ¥äº†25äººï¼Œç¼ºå¸­å‡ äººï¼Ÿ"
                ]
            },
            {
                "name": "å¤æ‚æ¨ç†",
                "problems": [
                    "ä¸€ä¸ªæ•°çš„3å€åŠ ä¸Š8ç­‰äº20ï¼Œè¿™ä¸ªæ•°æ˜¯å¤šå°‘ï¼Ÿ",
                    "é•¿æ–¹å½¢é•¿6ç±³å®½4ç±³ï¼Œå‘¨é•¿å’Œé¢ç§¯åˆ†åˆ«æ˜¯å¤šå°‘ï¼Ÿ"
                ]
            }
        ]
        
        performance_results = {}
        
        for test_category in test_cases:
            category_name = test_category["name"]
            problems = test_category["problems"]
            
            print(f"\nğŸ“Š æµ‹è¯•ç±»åˆ«: {category_name}")
            
            times = []
            success_count = 0
            
            for i, problem in enumerate(problems):
                try:
                    # å‡†å¤‡æ•°æ®
                    sample = {"problem": problem, "id": f"{category_name}_{i}"}
                    processed = preprocessor.process(sample)
                    
                    # è®¡æ—¶æµ‹è¯•
                    start_time = time.time()
                    result = engine.solve(processed)
                    end_time = time.time()
                    
                    execution_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                    times.append(execution_time)
                    
                    if result and 'final_answer' in result:
                        success_count += 1
                    
                    print(f"   é—®é¢˜ {i+1}: {execution_time:.1f}ms")
                    
                except Exception as e:
                    print(f"   é—®é¢˜ {i+1}: å¤±è´¥ ({e})")
                    times.append(float('inf'))
            
            # è®¡ç®—ç»Ÿè®¡æ•°æ®
            valid_times = [t for t in times if t != float('inf')]
            if valid_times:
                avg_time = sum(valid_times) / len(valid_times)
                max_time = max(valid_times)
                min_time = min(valid_times)
                
                performance_results[category_name] = {
                    "average_time_ms": avg_time,
                    "max_time_ms": max_time,
                    "min_time_ms": min_time,
                    "success_rate": success_count / len(problems),
                    "total_problems": len(problems)
                }
                
                print(f"   ğŸ“ˆ å¹³å‡æ—¶é—´: {avg_time:.1f}ms")
                print(f"   ğŸ“ˆ æœ€å¤§æ—¶é—´: {max_time:.1f}ms")
                print(f"   ğŸ“ˆ æœ€å°æ—¶é—´: {min_time:.1f}ms")
                print(f"   ğŸ¯ æˆåŠŸç‡: {success_count}/{len(problems)} ({success_count/len(problems)*100:.1f}%)")
        
        # è¾“å‡ºæ€§èƒ½æ€»ç»“
        print(f"\nğŸ† æ€§èƒ½æµ‹è¯•æ€»ç»“:")
        overall_avg = sum(r["average_time_ms"] for r in performance_results.values()) / len(performance_results)
        overall_success = sum(r["success_rate"] * r["total_problems"] for r in performance_results.values()) / sum(r["total_problems"] for r in performance_results.values())
        
        print(f"   æ•´ä½“å¹³å‡å“åº”æ—¶é—´: {overall_avg:.1f}ms")
        print(f"   æ•´ä½“æˆåŠŸç‡: {overall_success*100:.1f}%")
        
        # æ€§èƒ½è¯„ä¼°
        if overall_avg < 100:
            print("   ğŸš€ æ€§èƒ½è¯„çº§: ä¼˜ç§€ (< 100ms)")
        elif overall_avg < 500:
            print("   âœ… æ€§èƒ½è¯„çº§: è‰¯å¥½ (< 500ms)")
        elif overall_avg < 1000:
            print("   âš ï¸ æ€§èƒ½è¯„çº§: ä¸€èˆ¬ (< 1s)")
        else:
            print("   âŒ æ€§èƒ½è¯„çº§: éœ€è¦ä¼˜åŒ– (> 1s)")
        
        return performance_results
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return {}


def accuracy_evaluation():
    """å‡†ç¡®ç‡è¯„ä¼°"""
    print("\nğŸ¯ å‡†ç¡®ç‡è¯„ä¼°")
    print("=" * 50)
    
    try:
        engine = ReasoningEngine()
        preprocessor = Preprocessor()
        
        # å·²çŸ¥ç­”æ¡ˆçš„æµ‹è¯•é¢˜
        test_problems = [
            {"problem": "5 + 3", "expected": "8"},
            {"problem": "10 - 4", "expected": "6"},
            {"problem": "7 Ã— 6", "expected": "42"},
            {"problem": "20 Ã· 4", "expected": "5"},
            {"problem": "å°æ˜æœ‰8ä¸ªè‹¹æœï¼Œåƒäº†3ä¸ªï¼Œè¿˜å‰©å‡ ä¸ªï¼Ÿ", "expected": "5"},
            {"problem": "ä¸€æ”¯ç¬”2å…ƒï¼Œä¹°5æ”¯å¤šå°‘é’±ï¼Ÿ", "expected": "10"},
            {"problem": "ç­çº§æœ‰25ä¸ªå­¦ç”Ÿï¼Œæ¥äº†20ä¸ªï¼Œç¼ºå¸­å‡ ä¸ªï¼Ÿ", "expected": "5"},
            {"problem": "ä¸€ä¸ªæ­£æ–¹å½¢è¾¹é•¿3ç±³ï¼Œé¢ç§¯å¤šå°‘å¹³æ–¹ç±³ï¼Ÿ", "expected": "9"}
        ]
        
        correct_count = 0
        total_count = len(test_problems)
        results = []
        
        print("ğŸ“ é€é¢˜æµ‹è¯•:")
        
        for i, test_case in enumerate(test_problems, 1):
            problem = test_case["problem"]
            expected = test_case["expected"]
            
            try:
                sample = {"problem": problem, "id": f"accuracy_test_{i}"}
                processed = preprocessor.process(sample)
                result = engine.solve(processed)
                
                predicted = result.get('final_answer', '') if result else ''
                is_correct = str(predicted).strip() == str(expected).strip()
                
                if is_correct:
                    correct_count += 1
                    status = "âœ…"
                else:
                    status = "âŒ"
                
                print(f"   {i}. {problem}")
                print(f"      é¢„æœŸ: {expected}, å®é™…: {predicted} {status}")
                
                results.append({
                    "problem": problem,
                    "expected": expected,
                    "predicted": predicted,
                    "correct": is_correct
                })
                
            except Exception as e:
                print(f"   {i}. {problem}")
                print(f"      é”™è¯¯: {e} âŒ")
                results.append({
                    "problem": problem,
                    "expected": expected,
                    "predicted": "ERROR",
                    "correct": False
                })
        
        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = correct_count / total_count if total_count > 0 else 0
        
        print(f"\nğŸ“Š å‡†ç¡®ç‡è¯„ä¼°ç»“æœ:")
        print(f"   æ­£ç¡®ç­”æ¡ˆ: {correct_count}/{total_count}")
        print(f"   å‡†ç¡®ç‡: {accuracy*100:.1f}%")
        
        # å‡†ç¡®ç‡è¯„çº§
        if accuracy >= 0.9:
            print("   ğŸ† å‡†ç¡®ç‡è¯„çº§: ä¼˜ç§€ (â‰¥ 90%)")
        elif accuracy >= 0.8:
            print("   âœ… å‡†ç¡®ç‡è¯„çº§: è‰¯å¥½ (â‰¥ 80%)")
        elif accuracy >= 0.7:
            print("   âš ï¸ å‡†ç¡®ç‡è¯„çº§: ä¸€èˆ¬ (â‰¥ 70%)")
        else:
            print("   âŒ å‡†ç¡®ç‡è¯„çº§: éœ€è¦æ”¹è¿› (< 70%)")
        
        return {
            "accuracy": accuracy,
            "correct_count": correct_count,
            "total_count": total_count,
            "results": results
        }
        
    except Exception as e:
        print(f"âŒ å‡†ç¡®ç‡è¯„ä¼°å¤±è´¥: {e}")
        return {}


def save_validation_report(validation_data: Dict[str, Any]):
    """ä¿å­˜éªŒè¯æŠ¥å‘Š"""
    try:
        report_file = "validation_report.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(validation_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
    except Exception as e:
        print(f"âš ï¸ æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ COT-DIR éªŒè¯å’Œæ€§èƒ½æµ‹è¯•")
    print("å…¨é¢æµ‹è¯•ç³»ç»ŸåŠŸèƒ½ã€æ€§èƒ½å’Œå‡†ç¡®æ€§")
    
    validation_data = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "system_validation": {},
        "performance_benchmark": {},
        "accuracy_evaluation": {}
    }
    
    try:
        # 1. ç³»ç»ŸåŠŸèƒ½éªŒè¯
        validation_data["system_validation"] = validate_system_functionality()
        
        # 2. æ€§èƒ½åŸºå‡†æµ‹è¯•
        validation_data["performance_benchmark"] = performance_benchmark()
        
        # 3. å‡†ç¡®ç‡è¯„ä¼°
        validation_data["accuracy_evaluation"] = accuracy_evaluation()
        
        # 4. ä¿å­˜éªŒè¯æŠ¥å‘Š
        save_validation_report(validation_data)
        
        print("\nğŸ‰ éªŒè¯å’Œæ€§èƒ½æµ‹è¯•å®Œæˆï¼")
        print("\nğŸ“ˆ æ€»ä½“è¯„ä¼°:")
        
        # ç³»ç»Ÿå¥åº·åº¦è¯„ä¼°
        if validation_data["system_validation"]:
            system_health = sum(validation_data["system_validation"].values()) / len(validation_data["system_validation"])
            print(f"   ç³»ç»Ÿå¥åº·åº¦: {system_health*100:.1f}%")
        
        if validation_data["accuracy_evaluation"]:
            accuracy = validation_data["accuracy_evaluation"].get("accuracy", 0)
            print(f"   ç³»ç»Ÿå‡†ç¡®ç‡: {accuracy*100:.1f}%")
        
        print("\nğŸ“š æŸ¥çœ‹è¯¦ç»†ç»“æœ:")
        print("   - validation_report.json (å®Œæ•´éªŒè¯æŠ¥å‘Š)")
        print("   - validation_results.json (å†å²éªŒè¯æ•°æ®)")
        
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ éªŒè¯æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 