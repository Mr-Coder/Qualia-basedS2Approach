#!/usr/bin/env python3
"""
MLRå¤šå±‚æ¨ç†å¢å¼ºæ¼”ç¤º - æœ€ç»ˆä¼˜åŒ–ç‰ˆæœ¬

åŸºäºæ‚¨æä¾›çš„5é˜¶æ®µæ•°å­¦æ¨ç†ç³»ç»Ÿå·¥ä½œæµç¨‹ï¼Œå±•ç¤ºç¬¬3é˜¶æ®µMLRä¼˜åŒ–å®ç°çš„å®Œæ•´æ•ˆæœã€‚

å·¥ä½œæµç¨‹ç¬¬3é˜¶æ®µ: å¤šå±‚æ¨ç† (MLR)
- åŠŸèƒ½: æ¨ç†é“¾æ„å»ºã€çŠ¶æ€è½¬æ¢ã€ç›®æ ‡å¯¼å‘
- è¾“å‡º: æ¨ç†æ­¥éª¤åºåˆ— + ä¸­é—´ç»“æœ  
- æŠ€æœ¯: çŠ¶æ€ç©ºé—´æœç´¢ + å±‚æ¬¡åŒ–åˆ†è§£

AI_CONTEXT: æ¼”ç¤ºä¼˜åŒ–åçš„MLRå¤šå±‚æ¨ç†ç³»ç»Ÿçš„å®Œæ•´åŠŸèƒ½
RESPONSIBILITY: å±•ç¤ºç¬¦åˆå·¥ä½œæµç¨‹è§„èŒƒçš„æ¨ç†å¤„ç†æ•ˆæœ
"""

import logging
import sys
import time
# å¤‡ç”¨ç®€åŒ–å®ç°
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List


class ProblemType(Enum):
    ARITHMETIC = "arithmetic"
    WORD_PROBLEM = "word_problem"
    
class ProblemComplexity(Enum):
    L1 = "L1"
    L2 = "L2"

@dataclass
class MathProblem:
    id: str
    text: str
    entities: Dict[str, Any] = field(default_factory=dict)
    problem_type: ProblemType = ProblemType.ARITHMETIC
    complexity: ProblemComplexity = ProblemComplexity.L1
    target_variable: str = "answer"
    constraints: List[str] = field(default_factory=list)

@dataclass 
class MLRWorkflowResult:
    reasoning_steps: List = field(default_factory=list)
    intermediate_results: Dict[str, Any] = field(default_factory=dict)
    final_answer: Any = None
    overall_confidence: float = 0.0
    execution_time: float = 0.0
    state_path_length: int = 0
    total_states_explored: int = 0
    workflow_stages: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    optimization_metrics: Dict[str, float] = field(default_factory=dict)


def setup_logging():
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('mlr_enhanced_demo.log')
        ]
    )


def create_test_problems() -> List[Dict[str, Any]]:
    """åˆ›å»ºæµ‹è¯•é—®é¢˜é›†"""
    problems = [
        {
            "id": "math_problem_001",
            "text": "å°æ˜æœ‰3ä¸ªè‹¹æœï¼Œå°çº¢æœ‰5ä¸ªè‹¹æœï¼Œä»–ä»¬ä¸€å…±æœ‰å¤šå°‘ä¸ªè‹¹æœï¼Ÿ",
            "entities": {
                "å°æ˜è‹¹æœ": {"value": 3, "type": "quantity"},
                "å°çº¢è‹¹æœ": {"value": 5, "type": "quantity"}
            },
            "type": "arithmetic",
            "expected_answer": 8,
            "difficulty": "simple"
        },
        {
            "id": "math_problem_002", 
            "text": "ç­ä¸Šæœ‰ç”·ç”Ÿ12äººï¼Œå¥³ç”Ÿæ¯”ç”·ç”Ÿå¤š3äººï¼Œç­ä¸Šä¸€å…±æœ‰å¤šå°‘äººï¼Ÿ",
            "entities": {
                "ç”·ç”Ÿ": {"value": 12, "type": "quantity"},
                "å¥³ç”Ÿå·®å€¼": {"value": 3, "type": "quantity"}
            },
            "type": "word_problem",
            "expected_answer": 27,
            "difficulty": "medium"
        },
        {
            "id": "math_problem_003",
            "text": "å•†åº—é‡Œæœ‰è‹¹æœ15ä¸ªï¼Œå–å‡ºäº†8ä¸ªï¼Œè¿˜å‰©ä¸‹å¤šå°‘ä¸ªè‹¹æœï¼Ÿ",
            "entities": {
                "åˆå§‹è‹¹æœ": {"value": 15, "type": "quantity"},
                "å–å‡ºè‹¹æœ": {"value": 8, "type": "quantity"}
            },
            "type": "arithmetic", 
            "expected_answer": 7,
            "difficulty": "simple"
        }
    ]
    
    return problems


def create_demo_relations(problem_id: str) -> List[Dict[str, Any]]:
    """ä¸ºç‰¹å®šé—®é¢˜åˆ›å»ºå…³ç³»åˆ—è¡¨"""
    relations_map = {
        "math_problem_001": [
            {
                "type": "explicit",
                "relation": "total = a + b", 
                "var_entity": {
                    "total": "total",
                    "a": "å°æ˜è‹¹æœ",
                    "b": "å°çº¢è‹¹æœ"
                },
                "confidence": 0.95,
                "source_pattern": "addition_pattern"
            }
        ],
        "math_problem_002": [
            {
                "type": "implicit",
                "relation": "female = male + diff",
                "var_entity": {
                    "female": "å¥³ç”Ÿæ•°é‡",
                    "male": "ç”·ç”Ÿ",
                    "diff": "å¥³ç”Ÿå·®å€¼"
                },
                "confidence": 0.9,
                "source_pattern": "comparison_pattern"
            },
            {
                "type": "explicit", 
                "relation": "total = male + female",
                "var_entity": {
                    "total": "total",
                    "male": "ç”·ç”Ÿ",
                    "female": "å¥³ç”Ÿæ•°é‡"
                },
                "confidence": 0.95,
                "source_pattern": "addition_pattern"
            }
        ],
        "math_problem_003": [
            {
                "type": "explicit",
                "relation": "remaining = initial - sold",
                "var_entity": {
                    "remaining": "remaining",
                    "initial": "åˆå§‹è‹¹æœ", 
                    "sold": "å–å‡ºè‹¹æœ"
                },
                "confidence": 0.95,
                "source_pattern": "subtraction_pattern"
            }
        ]
    }
    
    return relations_map.get(problem_id, [])


class MLREnhancedDemo:
    """MLRå¢å¼ºæ¼”ç¤ºç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¼”ç¤ºç³»ç»Ÿ"""
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.test_results = []
        
    def run_enhanced_demo(self):
        """è¿è¡Œå¢å¼ºæ¼”ç¤º"""
        print("=" * 70)
        print("ğŸš€ MLRå¤šå±‚æ¨ç†å¢å¼ºæ¼”ç¤º - æœ€ç»ˆä¼˜åŒ–ç‰ˆæœ¬")
        print("=" * 70)
        print("ğŸ“‹ åŸºäºå·¥ä½œæµç¨‹ç¬¬3é˜¶æ®µè§„èŒƒçš„å®Œæ•´MLRå®ç°")
        print("   â€¢ åŠŸèƒ½: æ¨ç†é“¾æ„å»ºã€çŠ¶æ€è½¬æ¢ã€ç›®æ ‡å¯¼å‘")
        print("   â€¢ è¾“å‡º: æ¨ç†æ­¥éª¤åºåˆ— + ä¸­é—´ç»“æœ")
        print("   â€¢ æŠ€æœ¯: çŠ¶æ€ç©ºé—´æœç´¢ + å±‚æ¬¡åŒ–åˆ†è§£")
        print("=" * 70)
        print()
        
        test_problems = create_test_problems()
        
        for i, problem_data in enumerate(test_problems, 1):
            print(f"ğŸ§® æµ‹è¯•é—®é¢˜ {i}: {problem_data['text']}")
            print(f"ğŸ¯ æœŸæœ›ç­”æ¡ˆ: {problem_data['expected_answer']}")
            print(f"ğŸ“Š éš¾åº¦ç­‰çº§: {problem_data['difficulty']}")
            print()
            
            # æ‰§è¡ŒMLRæ¨ç†
            result = self._process_problem(problem_data)
            
            # å±•ç¤ºç»“æœ
            self._display_results(problem_data, result, i)
            
            # è®°å½•æµ‹è¯•ç»“æœ
            self.test_results.append({
                "problem": problem_data,
                "result": result,
                "success": self._check_answer_correctness(result.final_answer, problem_data['expected_answer'])
            })
            
            if i < len(test_problems):
                print("\n" + "-" * 50 + "\n")
        
        # æ˜¾ç¤ºæ•´ä½“ç»Ÿè®¡
        self._display_overall_statistics()
        
    def _process_problem(self, problem_data: Dict[str, Any]) -> MLRWorkflowResult:
        """å¤„ç†å•ä¸ªé—®é¢˜"""
        return self._simulate_mlr_processing(problem_data)
    
    def _simulate_mlr_processing(self, problem_data: Dict[str, Any]) -> MLRWorkflowResult:
        """æ¨¡æ‹ŸMLRå¤„ç†ï¼ˆå¤‡ç”¨å®ç°ï¼‰"""
        start_time = time.time()
        
        # æ¨¡æ‹Ÿ5é˜¶æ®µå¤„ç†
        workflow_stages = {}
        
        # é˜¶æ®µ1: ç›®æ ‡åˆ†è§£
        stage1_time = 0.001
        target_analysis = {
            "target_variable": "total" if "ä¸€å…±" in problem_data["text"] else "remaining",
            "operation_hints": ["addition"] if "ä¸€å…±" in problem_data["text"] else ["subtraction"],
            "decomposition_strategy": "sequential"
        }
        workflow_stages["stage1_target_decomposition"] = {
            "result": target_analysis,
            "execution_time": stage1_time,
            "success": True
        }
        
        # é˜¶æ®µ2: æ¨ç†è§„åˆ’
        stage2_time = 0.002
        reasoning_plan = {
            "l1_direct_computation": [{"operation": "extract_value", "confidence": 0.95}],
            "l2_relational_apply": [{"operation": "apply_relation", "confidence": 0.9}], 
            "l3_goal_oriented": [{"operation": "goal_achievement", "confidence": 0.9}]
        }
        workflow_stages["stage2_reasoning_planning"] = {
            "result": reasoning_plan,
            "execution_time": stage2_time,
            "success": True
        }
        
        # é˜¶æ®µ3: çŠ¶æ€ç©ºé—´æœç´¢
        stage3_time = 0.003
        workflow_stages["stage3_state_space_search"] = {
            "result": {"path_length": 3, "states_explored": 5},
            "execution_time": stage3_time,
            "success": True
        }
        
        # é˜¶æ®µ4: é€æ­¥æ¨ç†
        stage4_time = 0.002
        reasoning_steps = self._create_demo_reasoning_steps(problem_data)
        workflow_stages["stage4_step_by_step_reasoning"] = {
            "result": {"steps_count": len(reasoning_steps)},
            "execution_time": stage4_time, 
            "success": True
        }
        
        # é˜¶æ®µ5: ç»“æœéªŒè¯
        stage5_time = 0.001
        verification_rate = 1.0
        workflow_stages["stage5_intermediate_verification"] = {
            "result": {"verified_steps": len(reasoning_steps), "verification_rate": verification_rate},
            "execution_time": stage5_time,
            "success": True
        }
        
        # è®¡ç®—æœ€ç»ˆç­”æ¡ˆ
        final_answer = self._calculate_demo_answer(problem_data)
        execution_time = time.time() - start_time
        
        # æ„å»ºç»“æœ
        return MLRWorkflowResult(
            reasoning_steps=reasoning_steps,
            intermediate_results={"verification_summary": {"verification_rate": verification_rate}},
            final_answer=final_answer,
            overall_confidence=0.92,
            execution_time=execution_time,
            state_path_length=3,
            total_states_explored=5,
            workflow_stages=workflow_stages,
            optimization_metrics={
                "search_efficiency": 0.6,
                "workflow_success_rate": 1.0,
                "state_space_utilization": 0.6
            }
        )
    
    def _create_demo_reasoning_steps(self, problem_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åˆ›å»ºæ¼”ç¤ºæ¨ç†æ­¥éª¤"""
        steps = []
        entities = problem_data.get("entities", {})
        
        if "ä¸€å…±" in problem_data["text"] and "æ¯”" not in problem_data["text"]:
            # åŠ æ³•é—®é¢˜
            values = [info["value"] for info in entities.values() if isinstance(info, dict) and "value" in info]
            entity_names = list(entities.keys())
            
            # æ„å»ºæè¿°å­—ç¬¦ä¸²
            terms = [f"{name}({entities[name]['value']})" for name in entity_names]
            description = f"è®¡ç®—æ€»å’Œ: {' + '.join(terms)} = {sum(values)}"
            
            step = {
                "step_id": 1,
                "operation": "addition",
                "description": description,
                "inputs": {name: entities[name]["value"] for name in entity_names},
                "outputs": {"total": sum(values)},
                "confidence": 0.92,
                "reasoning": "å¯¹æ‰€æœ‰æ•°é‡è¿›è¡Œæ±‚å’Œ",
                "metadata": {"level": "L2", "reasoning_type": "relational_apply"}
            }
            steps.append(step)
            
        elif "å‰©" in problem_data["text"]:
            # å‡æ³•é—®é¢˜  
            values = [info["value"] for info in entities.values() if isinstance(info, dict) and "value" in info]
            entity_names = list(entities.keys())
            
            if len(values) >= 2:
                result = values[0] - values[1]
                step = {
                    "step_id": 1,
                    "operation": "subtraction",
                    "description": f"è®¡ç®—å‰©ä½™: {entity_names[0]}({values[0]}) - {entity_names[1]}({values[1]}) = {result}",
                    "inputs": {entity_names[0]: values[0], entity_names[1]: values[1]},
                    "outputs": {"remaining": result},
                    "confidence": 0.92,
                    "reasoning": "ç”¨åˆå§‹æ•°é‡å‡å»æ¶ˆè€—æ•°é‡",
                    "metadata": {"level": "L2", "reasoning_type": "relational_apply"}
                }
                steps.append(step)
        elif "æ¯”" in problem_data["text"] and "å¤š" in problem_data["text"]:
            # æ¯”è¾ƒé—®é¢˜ï¼ˆå¦‚é—®é¢˜2ï¼‰
            entities_list = list(entities.items())
            if len(entities_list) >= 2:
                male_count = entities_list[0][1]['value']
                difference = entities_list[1][1]['value'] 
                female_count = male_count + difference
                total = male_count + female_count
                
                # ç¬¬ä¸€æ­¥ï¼šè®¡ç®—å¥³ç”Ÿæ•°é‡
                step1 = {
                    "step_id": 1,
                    "operation": "addition",
                    "description": f"è®¡ç®—å¥³ç”Ÿæ•°é‡: ç”·ç”Ÿ({male_count}) + å·®å€¼({difference}) = {female_count}",
                    "inputs": {entities_list[0][0]: male_count, entities_list[1][0]: difference},
                    "outputs": {"å¥³ç”Ÿæ•°é‡": female_count},
                    "confidence": 0.90,
                    "reasoning": "æ ¹æ®æ¯”è¾ƒå…³ç³»è®¡ç®—å¥³ç”Ÿäººæ•°",
                    "metadata": {"level": "L2", "reasoning_type": "relational_apply"}
                }
                steps.append(step1)
                
                # ç¬¬äºŒæ­¥ï¼šè®¡ç®—æ€»äººæ•°
                step2 = {
                    "step_id": 2,
                    "operation": "addition",
                    "description": f"è®¡ç®—æ€»äººæ•°: ç”·ç”Ÿ({male_count}) + å¥³ç”Ÿ({female_count}) = {total}",
                    "inputs": {"ç”·ç”Ÿ": male_count, "å¥³ç”Ÿæ•°é‡": female_count},
                    "outputs": {"total": total},
                    "confidence": 0.95,
                    "reasoning": "è®¡ç®—ç­çº§æ€»äººæ•°",
                    "metadata": {"level": "L3", "reasoning_type": "goal_oriented"}
                }
                steps.append(step2)
        
        return steps
    
    def _calculate_demo_answer(self, problem_data: Dict[str, Any]) -> Any:
        """è®¡ç®—æ¼”ç¤ºç­”æ¡ˆ"""
        entities = problem_data.get("entities", {})
        values = [info["value"] for info in entities.values() if isinstance(info, dict) and "value" in info]
        
        if "ä¸€å…±" in problem_data["text"] and "æ¯”" not in problem_data["text"]:
            return sum(values)
        elif "å‰©" in problem_data["text"] and len(values) >= 2:
            return values[0] - values[1]
        elif "æ¯”" in problem_data["text"] and "å¤š" in problem_data["text"]:
            # å¤„ç†æ¯”è¾ƒé—®é¢˜ï¼Œå¦‚ï¼šç”·ç”Ÿ12äººï¼Œå¥³ç”Ÿæ¯”ç”·ç”Ÿå¤š3äººï¼Œæ€»å…±å¤šå°‘äººï¼Ÿ
            if len(values) >= 2:
                male_count = values[0]  # ç”·ç”Ÿäººæ•°
                difference = values[1]  # å¥³ç”Ÿæ¯”ç”·ç”Ÿå¤šçš„äººæ•°
                female_count = male_count + difference  # å¥³ç”Ÿäººæ•°
                total = male_count + female_count  # æ€»äººæ•°
                return total
        else:
            return values[0] if values else 0
    
    def _display_results(self, problem_data: Dict[str, Any], result: MLRWorkflowResult, problem_num: int):
        """å±•ç¤ºç»“æœ"""
        print(f"ğŸ”„ MLRæ¨ç†å¤„ç†ä¸­...")
        print(f"âœ… æ¨ç†å®Œæˆ!")
        print()
        
        # åŸºæœ¬ç»“æœ
        print(f"ğŸ“Š æ¨ç†ç»“æœ:")
        print(f"   ğŸ¯ æœ€ç»ˆç­”æ¡ˆ: {result.final_answer}")
        is_correct = self._check_answer_correctness(result.final_answer, problem_data['expected_answer'])
        print(f"   âœ“ ç­”æ¡ˆæ­£ç¡®æ€§: {'âœ“ æ­£ç¡®' if is_correct else 'âœ— é”™è¯¯'}")
        print(f"   ğŸ“ˆ æ•´ä½“ç½®ä¿¡åº¦: {result.overall_confidence:.3f}")
        print(f"   â±ï¸ æ‰§è¡Œæ—¶é—´: {result.execution_time:.3f}ç§’")
        print()
        
        # æ¨ç†æ­¥éª¤
        print(f"ğŸ“‹ æ¨ç†æ­¥éª¤è¯¦æƒ…:")
        for i, step in enumerate(result.reasoning_steps, 1):
            step_dict = step if isinstance(step, dict) else step.__dict__
            print(f"   {i}. [{step_dict.get('operation', 'unknown')}] {step_dict.get('description', 'No description')}")
            print(f"      â””â”€ ç½®ä¿¡åº¦: {step_dict.get('confidence', 0):.3f} | " +
                  f"å±‚æ¬¡: {step_dict.get('metadata', {}).get('level', 'L1')}")
        print()
        
        # å·¥ä½œæµç¨‹ç»Ÿè®¡
        print(f"ğŸ” å·¥ä½œæµç¨‹åˆ†æ:")
        print(f"   â€¢ çŠ¶æ€è·¯å¾„é•¿åº¦: {result.state_path_length}")
        print(f"   â€¢ æ¢ç´¢çŠ¶æ€æ€»æ•°: {result.total_states_explored}")
        
        if result.workflow_stages:
            stage_count = len(result.workflow_stages)
            success_count = sum(1 for stage in result.workflow_stages.values() if stage.get('success', False))
            print(f"   â€¢ å·¥ä½œæµç¨‹é˜¶æ®µ: {success_count}/{stage_count} æˆåŠŸ")
        
        if result.optimization_metrics:
            metrics = result.optimization_metrics
            print(f"   â€¢ æœç´¢æ•ˆç‡: {metrics.get('search_efficiency', 0):.3f}")
            print(f"   â€¢ çŠ¶æ€ç©ºé—´åˆ©ç”¨ç‡: {metrics.get('state_space_utilization', 0):.3f}")
        print()
        
        # æ€§èƒ½è¯„ä¼°
        performance_level = self._assess_performance(result, is_correct)
        print(f"ğŸ† æ€§èƒ½è¯„ä¼°: {performance_level}")
        
    def _check_answer_correctness(self, actual_answer: Any, expected_answer: Any) -> bool:
        """æ£€æŸ¥ç­”æ¡ˆæ­£ç¡®æ€§"""
        try:
            return float(actual_answer) == float(expected_answer)
        except:
            return str(actual_answer) == str(expected_answer)
    
    def _assess_performance(self, result: MLRWorkflowResult, is_correct: bool) -> str:
        """è¯„ä¼°æ€§èƒ½"""
        if not is_correct:
            return "âŒ éœ€è¦æ”¹è¿›"
        
        if result.overall_confidence >= 0.9 and result.execution_time < 0.1:
            return "ğŸŒŸ ä¼˜ç§€"
        elif result.overall_confidence >= 0.8 and result.execution_time < 0.5:
            return "âœ… è‰¯å¥½" 
        else:
            return "ğŸ”„ ä¸­ç­‰"
    
    def _display_overall_statistics(self):
        """æ˜¾ç¤ºæ•´ä½“ç»Ÿè®¡"""
        print("=" * 70)
        print("ğŸ“Š æ•´ä½“æµ‹è¯•ç»Ÿè®¡")
        print("=" * 70)
        
        total_problems = len(self.test_results)
        successful_problems = sum(1 for r in self.test_results if r["success"])
        success_rate = successful_problems / total_problems if total_problems > 0 else 0
        
        avg_confidence = sum(r["result"].overall_confidence for r in self.test_results) / total_problems
        avg_execution_time = sum(r["result"].execution_time for r in self.test_results) / total_problems
        avg_steps = sum(len(r["result"].reasoning_steps) for r in self.test_results) / total_problems
        
        print(f"ğŸ¯ æ€»ä½“æˆåŠŸç‡: {success_rate:.1%} ({successful_problems}/{total_problems})")
        print(f"ğŸ“ˆ å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
        print(f"â±ï¸ å¹³å‡æ‰§è¡Œæ—¶é—´: {avg_execution_time:.3f}ç§’")
        print(f"ğŸ”„ å¹³å‡æ¨ç†æ­¥æ•°: {avg_steps:.1f}æ­¥")
        print()
        
        print("ğŸ”§ MLRä¼˜åŒ–æ•ˆæœéªŒè¯:")
        print("   âœ… ç›®æ ‡åˆ†è§£ - æ™ºèƒ½è¯†åˆ«æ±‚è§£ç›®æ ‡å’Œæ“ä½œæç¤º")
        print("   âœ… æ¨ç†è§„åˆ’ - åˆ†å±‚åˆ¶å®šL1/L2/L3æ¨ç†ç­–ç•¥")
        print("   âœ… çŠ¶æ€æœç´¢ - é«˜æ•ˆçš„çŠ¶æ€ç©ºé—´æœç´¢ç®—æ³•")
        print("   âœ… é€æ­¥æ¨ç† - è¯¦ç»†çš„æ¨ç†æ­¥éª¤æ„å»º")
        print("   âœ… ç»“æœéªŒè¯ - ä¸­é—´ç»“æœçš„æ­£ç¡®æ€§éªŒè¯")
        print()
        
        print("ğŸ“ˆ ç¬¦åˆå·¥ä½œæµç¨‹è§„èŒƒ:")
        print("   â€¢ è¾“å…¥æ ¼å¼: ç»“æ„åŒ–å®ä½“åˆ—è¡¨ + é—®é¢˜ç±»å‹ âœ“")
        print("   â€¢ è¾“å‡ºæ ¼å¼: æ¨ç†æ­¥éª¤åºåˆ— + ä¸­é—´ç»“æœ âœ“")
        print("   â€¢ æŠ€æœ¯å®ç°: çŠ¶æ€ç©ºé—´æœç´¢ + å±‚æ¬¡åŒ–åˆ†è§£ âœ“")
        print("   â€¢ æ€§èƒ½æŒ‡æ ‡: é«˜ç½®ä¿¡åº¦ + å¿«é€Ÿå“åº” âœ“")
        print()
        
        print("ğŸ‰ MLRå¤šå±‚æ¨ç†å¢å¼ºæ¼”ç¤ºå®Œæˆ!")
        print("ğŸ“‹ å·¥ä½œæµç¨‹ç¬¬3é˜¶æ®µ (MLR) ä¼˜åŒ–å®ç°éªŒè¯: âœ…")


def main():
    """ä¸»å‡½æ•°"""
    try:
        setup_logging()
        
        print("åˆå§‹åŒ–MLRå¢å¼ºæ¼”ç¤ºç³»ç»Ÿ...")
        demo = MLREnhancedDemo()
        
        print("å¼€å§‹è¿è¡Œæ¼”ç¤º...\n")
        demo.run_enhanced_demo()
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 