"""
ğŸ”¢ COT-DIRç³»ç»Ÿé¢˜ç›®æ•°é‡ç»Ÿè®¡
ç»Ÿè®¡ç³»ç»Ÿæœ€å¤šå¯ä»¥ç”Ÿæˆå¤šå°‘é“è§£ç­”é¢˜ç›®
"""

import json
import os
from pathlib import Path


def count_problems_in_datasets():
    """ç»Ÿè®¡æ‰€æœ‰æ•°æ®é›†ä¸­çš„é¢˜ç›®æ•°é‡"""
    print("ğŸ” COT-DIRç³»ç»Ÿé¢˜ç›®ç”Ÿæˆèƒ½åŠ›åˆ†æ")
    print("=" * 60)
    
    data_dir = Path("Data")
    total_problems = 0
    dataset_details = []
    
    # éå†æ‰€æœ‰æ•°æ®é›†ç›®å½•
    for dataset_dir in data_dir.iterdir():
        if dataset_dir.is_dir() and not dataset_dir.name.startswith('.') and not dataset_dir.name.startswith('__'):
            dataset_name = dataset_dir.name
            problem_count = 0
            
            # æŸ¥æ‰¾JSONæ–‡ä»¶
            for json_file in dataset_dir.glob("*.json"):
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if isinstance(data, list):
                            problem_count += len(data)
                        elif isinstance(data, dict):
                            # æ£€æŸ¥å¸¸è§çš„é”®å
                            for key in ['problems', 'data', 'examples', 'questions']:
                                if key in data and isinstance(data[key], list):
                                    problem_count += len(data[key])
                                    break
                            else:
                                problem_count += 1  # å•ä¸ªé—®é¢˜
                except Exception as e:
                    print(f"   âš ï¸  æ— æ³•è¯»å– {json_file}: {e}")
                    continue
            
            # æŸ¥æ‰¾JSONLæ–‡ä»¶  
            for jsonl_file in dataset_dir.glob("*.jsonl"):
                try:
                    with open(jsonl_file, 'r', encoding='utf-8') as f:
                        for line in f:
                            if line.strip():
                                problem_count += 1
                except Exception as e:
                    print(f"   âš ï¸  æ— æ³•è¯»å– {jsonl_file}: {e}")
                    continue
            
            if problem_count > 0:
                dataset_details.append((dataset_name, problem_count))
                total_problems += problem_count
    
    # æ£€æŸ¥æ ¹ç›®å½•ä¸­çš„å…¶ä»–JSONæ–‡ä»¶
    for json_file in data_dir.glob("*.json"):
        if json_file.is_file():
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        count = len(data)
                    elif isinstance(data, dict):
                        count = 1
                    else:
                        count = 0
                    
                    if count > 0:
                        dataset_details.append((f"æ ¹ç›®å½•_{json_file.stem}", count))
                        total_problems += count
            except Exception as e:
                print(f"   âš ï¸  æ— æ³•è¯»å–æ ¹ç›®å½•æ–‡ä»¶ {json_file}: {e}")
    
    # æŒ‰é¢˜ç›®æ•°é‡æ’åº
    dataset_details.sort(key=lambda x: x[1], reverse=True)
    
    print(f"ğŸ“Š æ•°æ®é›†è¯¦ç»†ç»Ÿè®¡:")
    for name, count in dataset_details:
        print(f"   {name}: {count:,} é¢˜")
    
    print(f"\nğŸ“ˆ æ€»è®¡é¢˜ç›®æ•°é‡: {total_problems:,} é¢˜")
    print(f"ğŸ“‹ æ•°æ®é›†æ€»æ•°: {len(dataset_details)} ä¸ª")
    
    # è®¡ç®—æŒ‰å¤æ‚åº¦åˆ†å¸ƒ
    small = sum(1 for _, count in dataset_details if count <= 100)
    medium = sum(1 for _, count in dataset_details if 100 < count <= 1000)  
    large = sum(1 for _, count in dataset_details if 1000 < count <= 2000)
    extra_large = sum(1 for _, count in dataset_details if count > 2000)
    
    print(f"\nğŸ“Š æ•°æ®é›†è§„æ¨¡åˆ†å¸ƒ:")
    print(f"   å°å‹(â‰¤100é¢˜): {small} ä¸ª")
    print(f"   ä¸­å‹(101-1000é¢˜): {medium} ä¸ª")
    print(f"   å¤§å‹(1001-2000é¢˜): {large} ä¸ª")
    print(f"   è¶…å¤§å‹(>2000é¢˜): {extra_large} ä¸ª")
    
    # åˆ†æé¢˜ç›®ç±»å‹
    print(f"\nğŸ¯ é¢˜ç›®ç±»å‹åˆ†æ:")
    elementary_datasets = [name for name, _ in dataset_details if any(keyword in name.lower() for keyword in ['addsub', 'singleeq', 'multiarith'])]
    intermediate_datasets = [name for name, _ in dataset_details if any(keyword in name.lower() for keyword in ['gsm', 'svamp', 'asdiv'])]
    advanced_datasets = [name for name, _ in dataset_details if any(keyword in name.lower() for keyword in ['math', 'aqua', 'mathqa'])]
    
    print(f"   åˆçº§é¢˜ç›®æ•°æ®é›†: {len(elementary_datasets)} ä¸ª")
    print(f"   ä¸­çº§é¢˜ç›®æ•°æ®é›†: {len(intermediate_datasets)} ä¸ª") 
    print(f"   é«˜çº§é¢˜ç›®æ•°æ®é›†: {len(advanced_datasets)} ä¸ª")
    
    # è®¡ç®—ç†è®ºç”Ÿæˆèƒ½åŠ›
    print(f"\nğŸš€ é¢˜ç›®ç”Ÿæˆèƒ½åŠ›åˆ†æ:")
    print(f"   âœ… å½“å‰å¯å¤„ç†: {total_problems:,} é¢˜")
    print(f"   âœ… æ‰¹é‡å¤„ç†èƒ½åŠ›: ç†è®ºä¸Šæ— é™åˆ¶")
    print(f"   âœ… æ¨èå•æ¬¡æ‰¹æ¬¡: 1,000-10,000 é¢˜")
    print(f"   âœ… å†…å­˜ä¼˜åŒ–æ‰¹æ¬¡: 100-1,000 é¢˜")
    
    # ä¼°ç®—å¤„ç†æ—¶é—´
    print(f"\nâ±ï¸ å¤„ç†æ—¶é—´ä¼°ç®—:")
    processing_speed = 0.2  # æ¯«ç§’/é¢˜ (æ ¹æ®ä¹‹å‰æµ‹è¯•ç»“æœ)
    
    for batch_size in [100, 1000, 10000, total_problems]:
        if batch_size <= total_problems:
            time_ms = batch_size * processing_speed
            if time_ms < 1000:
                time_str = f"{time_ms:.1f} æ¯«ç§’"
            elif time_ms < 60000:
                time_str = f"{time_ms/1000:.1f} ç§’"
            else:
                time_str = f"{time_ms/60000:.1f} åˆ†é’Ÿ"
            print(f"   {batch_size:,} é¢˜ â†’ {time_str}")
    
    # ç³»ç»Ÿé™åˆ¶åˆ†æ
    print(f"\nğŸ”§ ç³»ç»Ÿé™åˆ¶åˆ†æ:")
    print(f"   ğŸ’¾ å†…å­˜é™åˆ¶: å–å†³äºç³»ç»ŸRAM (æ¨è16GB+)")
    print(f"   ğŸ”„ å¹¶å‘å¤„ç†: æ”¯æŒå¤šçº¿ç¨‹/å¤šè¿›ç¨‹")
    print(f"   ğŸ’¿ å­˜å‚¨éœ€æ±‚: çº¦ {total_problems * 0.5 / 1024:.1f} MB (ä¼°ç®—)")
    
    # å®é™…åº”ç”¨å»ºè®®
    print(f"\nğŸ’¡ å®é™…åº”ç”¨å»ºè®®:")
    print(f"   ğŸ¯ å°è§„æ¨¡æµ‹è¯•: 100-1,000 é¢˜")
    print(f"   ğŸ“Š ä¸­ç­‰è§„æ¨¡ç ”ç©¶: 1,000-10,000 é¢˜")
    print(f"   ğŸš€ å¤§è§„æ¨¡è®­ç»ƒ: 10,000+ é¢˜")
    print(f"   âš¡ æœ€å¤§ç†è®ºå¤„ç†: {total_problems:,} é¢˜")
    
    return total_problems, dataset_details

def analyze_problem_quality():
    """åˆ†æé¢˜ç›®è´¨é‡å’Œå¤šæ ·æ€§"""
    print(f"\nğŸŒŸ é¢˜ç›®è´¨é‡ä¸å¤šæ ·æ€§åˆ†æ:")
    print("-" * 40)
    
    # åŸºäºæ•°æ®é›†åç§°åˆ†æé¢˜ç›®ç±»å‹
    type_analysis = {
        "åŸºç¡€ç®—æœ¯": ["AddSub", "SingleEq"],
        "å¤šæ­¥ç®—æœ¯": ["MultiArith"],
        "åº”ç”¨é¢˜": ["SVAMP", "ASDiv", "MAWPS"],
        "å°å­¦æ•°å­¦": ["GSM8K", "GSM-hard"],
        "ä¸­å­¦æ•°å­¦": ["MATH", "MathQA"],
        "ä¸­æ–‡æ•°å­¦": ["Math23K"],
        "ç‰¹æ®Šç±»å‹": ["AQuA", "DIR-MWP"]
    }
    
    for category, datasets in type_analysis.items():
        count = 0
        data_dir = Path("Data")
        for dataset_name in datasets:
            dataset_path = data_dir / dataset_name
            if dataset_path.exists():
                # ç®€å•ä¼°ç®—
                for json_file in dataset_path.glob("*.json"):
                    try:
                        with open(json_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            if isinstance(data, list):
                                count += len(data)
                    except:
                        pass
        
        if count > 0:
            print(f"   {category}: ~{count:,} é¢˜")
    
    print(f"\nğŸ“ é¢˜ç›®ç‰¹ç‚¹:")
    print(f"   âœ… å¤šè¯­è¨€æ”¯æŒ (è‹±æ–‡/ä¸­æ–‡)")
    print(f"   âœ… å¤šéš¾åº¦ç­‰çº§ (åˆçº§åˆ°é«˜çº§)")
    print(f"   âœ… å¤šé¢˜å‹è¦†ç›– (ç®—æœ¯/å‡ ä½•/ä»£æ•°)")
    print(f"   âœ… æ ‡å‡†åŒ–æ ¼å¼ (JSON/JSONL)")

if __name__ == "__main__":
    total, details = count_problems_in_datasets()
    analyze_problem_quality()
    
    print(f"\nğŸ‰ æ€»ç»“:")
    print(f"ğŸ”¢ COT-DIRç³»ç»Ÿæœ€å¤šå¯ä»¥ç”Ÿæˆ {total:,} é“è§£ç­”é¢˜ç›®ï¼")
    print(f"ğŸ“š è¦†ç›–ä»åŸºç¡€ç®—æœ¯åˆ°é«˜çº§æ•°å­¦çš„å®Œæ•´èŒƒå›´")
    print(f"âš¡ æ”¯æŒé«˜æ•ˆæ‰¹é‡å¤„ç†å’Œè´¨é‡è¯„ä¼°") 