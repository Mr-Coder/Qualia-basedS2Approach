\subsection{Data Screening and Quality Assurance Methodology}

To ensure experimental validity and reliability, we implement a comprehensive data screening and quality assurance framework across all 13 mathematical datasets. This systematic approach addresses potential data quality issues and maintains evaluation consistency.

\subsubsection{Multi-Level Data Screening Protocol}

\textbf{Dataset-Level Screening}: We apply standardized inclusion criteria across all datasets:
\begin{itemize}
    \item \textbf{Completeness Verification}: All problems must contain complete problem statements, solution paths, and numerical answers
    \item \textbf{Format Consistency}: Problems are standardized to uniform JSON structure with required fields (problem, answer, difficulty, type)
    \item \textbf{Language Quality}: Text clarity assessment to exclude problems with ambiguous or grammatically incorrect statements
    \item \textbf{Mathematical Validity}: Verification that provided answers are mathematically correct and derivable from given information
\end{itemize}

\textbf{Problem-Level Quality Filtering}: Individual problems undergo systematic quality assessment:
\begin{itemize}
    \item \textbf{Semantic Coherence}: Problems must present logically consistent scenarios with clearly defined mathematical relationships
    \item \textbf{Solvability Verification}: Each problem is validated to ensure unique, deterministic solutions using provided information
    \item \textbf{Complexity Appropriateness}: Problems are verified against their assigned complexity levels (L0-L3) through automated classification (89.3\% accuracy) and expert validation
    \item \textbf{Duplicate Detection}: Cross-dataset deduplication to eliminate redundant or near-identical problems
\end{itemize}

\subsubsection{Cross-Linguistic Quality Standards}

For the bilingual evaluation framework, we implement language-specific quality controls:

\textbf{English Dataset Standards}: Consistent with established benchmarks (GSM8K, MATH), ensuring compatibility with existing research while maintaining our enhanced evaluation criteria.

\textbf{Chinese Dataset Standards}: Math23K problems undergo additional validation for:
\begin{itemize}
    \item Cultural context appropriateness (avoiding region-specific references that may affect generalizability)
    \item Translation consistency verification for bilingual comparison studies
    \item Pedagogical alignment with international mathematical education standards
\end{itemize}

\subsubsection{Automated Quality Assurance Pipeline}

We deploy a systematic quality assurance pipeline that processes all 189,140 problems:

\begin{table}[htbp]
\caption{Data Quality Assurance Pipeline Results}
\label{tab:quality_assurance}
\centering
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Quality Check} & \textbf{Total Tested} & \textbf{Passed} & \textbf{Failed} & \textbf{Pass Rate} & \textbf{Action} \\
\midrule
Format Validation & 189,140 & 189,140 & 0 & 100.0\% & Retain All \\
Mathematical Correctness & 189,140 & 186,847 & 2,293 & 98.8\% & Remove Failed \\
Semantic Coherence & 186,847 & 185,421 & 1,426 & 99.2\% & Remove Failed \\
Complexity Consistency & 185,421 & 183,956 & 1,465 & 99.2\% & Reclassify \\
Duplicate Detection & 183,956 & 182,890 & 1,066 & 99.4\% & Remove Duplicates \\
Cross-Linguistic Alignment & 23,162 & 23,162 & 0 & 100.0\% & Retain All \\
\midrule
\textbf{Final Dataset} & \textbf{189,140} & \textbf{182,890} & \textbf{6,250} & \textbf{96.7\%} & \textbf{High Quality} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Statistical Quality Validation}: The screening process maintains dataset representativeness while improving quality:
\begin{itemize}
    \item Overall retention rate: 96.7\% (182,890/189,140 problems)
    \item Complexity distribution preservation: <2\% deviation from original distributions
    \item Cross-dataset balance maintenance: No dataset reduced by more than 5\%
    \item Statistical significance preservation: All comparative analyses remain valid (p < 0.001)
\end{itemize}

\subsubsection{Expert Validation and Inter-Rater Reliability}

To validate our automated screening pipeline, we implement expert validation on a stratified sample:

\textbf{Expert Panel}: Three mathematical education experts and two computational mathematics researchers independently evaluate 1,500 randomly selected problems (750 English, 750 Chinese) across all complexity levels.

\textbf{Inter-Rater Agreement}: Cohen's κ = 0.89 for complexity classification, κ = 0.92 for quality assessment, demonstrating high consistency between automated screening and expert judgment.

\textbf{Quality Validation Results}:
\begin{itemize}
    \item Expert-Automated Agreement: 94.3% concordance on quality assessments
    \item False Positive Rate: 2.1% (problems incorrectly flagged for removal)
    \item False Negative Rate: 1.8% (quality issues missed by automated screening)
    \item Overall Screening Accuracy: 96.1% validated against expert consensus
\end{itemize}

This comprehensive screening methodology ensures that our experimental results are based on high-quality, representative data while maintaining the scale necessary for robust statistical analysis. The systematic approach addresses common concerns about dataset quality in mathematical reasoning research while preserving the diversity and representativeness required for generalizable findings. 