"""
COT-DIRæ¡†æ¶ä¸MLRç³»ç»Ÿé›†æˆæ¨¡å—
ç»“åˆéšå¼å…³ç³»å‘ç°ã€å¤šå±‚æ¨ç†å’Œç½®ä¿¡éªŒè¯çš„å®Œæ•´æ•°å­¦æ¨ç†ç³»ç»Ÿ
"""

import json
import logging
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

from .processors.mlr_processor import MLRProcessor
# å¯¼å…¥ç°æœ‰MLRç»„ä»¶
from .strategies.mlr_core import MLRConfig, ReasoningLevel, ReasoningState
from .strategies.mlr_strategy import MLRMultiLayerReasoner

# ==================== COT-DIRæ ¸å¿ƒæ•°æ®ç»“æ„ ====================

@dataclass
class Entity:
    name: str
    entity_type: str
    attributes: Dict[str, Any]
    confidence: float = 1.0
    position: Optional[int] = None
    
    def __post_init__(self):
        """AIåä½œæ ‡æ³¨ï¼šå®ä½“æ•°æ®ç»“æ„éªŒè¯"""
        if not isinstance(self.confidence, (int, float)) or not 0 <= self.confidence <= 1:
            self.confidence = 1.0

@dataclass 
class Relation:
    relation_type: str
    entities: List[str]
    expression: str
    confidence: float
    reasoning: str = ""
    mathematical_form: Optional[str] = None
    
    def __post_init__(self):
        """AIåä½œæ ‡æ³¨ï¼šå…³ç³»æ•°æ®ç»“æ„éªŒè¯"""
        if not isinstance(self.confidence, (int, float)) or not 0 <= self.confidence <= 1:
            self.confidence = 0.7

@dataclass
class COTDIRStep:
    step_id: int
    operation_type: str
    content: str
    entities_involved: List[str]
    relations_applied: List[str]
    confidence: float
    reasoning_level: ReasoningLevel
    verification_status: bool = False

@dataclass
class ValidationResult:
    dimension: str
    score: float
    issues: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)

# ==================== IRDæ¨¡å—å®ç° ====================

class IRDModule:
    """
    ğŸ§  éšå¼å…³ç³»å‘ç°æ¨¡å— (Implicit Relation Discovery)
    æŠ€æœ¯å®ç°ï¼šåŸºäºå›¾è®ºå’Œæ¨¡å¼åŒ¹é…çš„ç»„åˆå‘ç°ç®—æ³•
    AIåä½œç‰¹æ€§ï¼šè‡ªé€‚åº”æ¨¡å¼è¯†åˆ« + åŠ¨æ€ç½®ä¿¡åº¦è°ƒæ•´
    """
    
    def __init__(self):
        self.relation_patterns = self._load_relation_patterns()
        self.confidence_threshold = 0.7
        self.pattern_cache = {}
        
        # AIåä½œé…ç½®
        self.adaptive_learning = True
        self.pattern_update_frequency = 10
        self.discovery_count = 0
        
    def discover_relations(self, entities: List[Entity], context: str, 
                         problem_type: str = "arithmetic") -> List[Relation]:
        """
        æ ¸å¿ƒç®—æ³•ï¼šO(n^k)å¤æ‚åº¦çš„é«˜æ•ˆå…³ç³»æœç´¢
        AIåä½œç‰¹æ€§ï¼šä¸Šä¸‹æ–‡æ„ŸçŸ¥ + é—®é¢˜ç±»å‹é€‚é…
        """
        # 1. æ„å»ºå®ä½“å…³ç³»å›¾
        entity_graph = self._build_entity_graph(entities)
        
        # 2. å¤šå±‚å…³ç³»æ¨¡å¼è¯†åˆ«
        potential_relations = self._pattern_matching(entities, context, problem_type)
        
        # 3. ç½®ä¿¡åº¦é‡åŒ–ä¸éªŒè¯
        validated_relations = []
        for relation in potential_relations:
            confidence = self._calculate_confidence(relation, entities, context)
            if confidence >= self.confidence_threshold:
                relation.confidence = confidence
                validated_relations.append(relation)
        
        # 4. AIåä½œå­¦ä¹ æ›´æ–°
        if self.adaptive_learning:
            self._update_patterns(validated_relations, context)
            
        self.discovery_count += 1
        return validated_relations
    
    def _build_entity_graph(self, entities: List[Entity]) -> Dict[str, Any]:
        """åŸºäºå›¾è®ºçš„å®ä½“å…³ç³»å›¾æ„å»º"""
        graph = {
            "nodes": [{"id": e.name, "type": e.entity_type, "attributes": e.attributes} 
                     for e in entities],
            "edges": [],
            "metadata": {"construction_time": time.time()}
        }
        
        # O(n^2)å¤æ‚åº¦çš„å®ä½“å¯¹åˆ†æ
        for i, entity1 in enumerate(entities):
            for j, entity2 in enumerate(entities[i+1:], i+1):
                edge_weight = self._calculate_semantic_similarity(entity1, entity2)
                if edge_weight > 0.5:
                    graph["edges"].append({
                        "from": entity1.name,
                        "to": entity2.name,
                        "weight": edge_weight,
                        "type": self._infer_edge_type(entity1, entity2)
                    })
        
        return graph
    
    def _pattern_matching(self, entities: List[Entity], context: str, 
                         problem_type: str) -> List[Relation]:
        """å¤šå±‚å…³ç³»æ¨¡å¼è¯†åˆ«ç®—æ³•"""
        relations = []
        
        # æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©ç›¸å…³æ¨¡å¼
        relevant_patterns = [p for p in self.relation_patterns 
                           if problem_type in p.get("applicable_types", ["general"])]
        
        for pattern in relevant_patterns:
            matches = self._match_pattern(pattern, entities, context)
            relations.extend(matches)
            
        return relations
    
    def _calculate_confidence(self, relation: Relation, entities: List[Entity], 
                            context: str) -> float:
        """å¤šå› å­ç½®ä¿¡åº¦é‡åŒ–"""
        factors = {
            "semantic_similarity": 0.3,
            "syntactic_match": 0.25,
            "mathematical_validity": 0.25,
            "context_consistency": 0.2
        }
        
        confidence = 0.0
        for factor, weight in factors.items():
            score = self._evaluate_factor(factor, relation, entities, context)
            confidence += score * weight
            
        return min(confidence, 1.0)
    
    def _load_relation_patterns(self) -> List[Dict]:
        """åŠ è½½æ•°å­¦æ¨ç†å…³ç³»æ¨¡å¼åº“"""
        return [
            {
                "name": "arithmetic_addition",
                "pattern": "{A} + {B} = {C}",
                "keywords": ["æ€»å…±", "ä¸€å…±", "åˆè®¡", "ç›¸åŠ ", "åŠ èµ·æ¥", "æ€»è®¡"],
                "math_ops": ["addition", "sum"],
                "applicable_types": ["arithmetic", "word_problem"],
                "confidence_base": 0.8
            },
            {
                "name": "arithmetic_multiplication",
                "pattern": "{A} Ã— {B} = {C}",
                "keywords": ["æ¯", "æ€»è®¡", "ä¹˜ä»¥", "å€", "å…±æœ‰"],
                "math_ops": ["multiplication", "product"],
                "applicable_types": ["arithmetic", "word_problem"],
                "confidence_base": 0.8
            },
            {
                "name": "comparison_relation",
                "pattern": "{A} æ¯” {B} {relation}",
                "keywords": ["æ¯”", "å¤š", "å°‘", "å¤§", "å°", "æ›´"],
                "math_ops": ["comparison", "difference"],
                "applicable_types": ["comparison", "word_problem"],
                "confidence_base": 0.75
            },
            {
                "name": "time_calculation",
                "pattern": "{time1} åˆ° {time2} æ˜¯ {duration}",
                "keywords": ["å°æ—¶", "åˆ†é’Ÿ", "å¤©", "ä»", "åˆ°"],
                "math_ops": ["time_arithmetic"],
                "applicable_types": ["time", "duration"],
                "confidence_base": 0.7
            }
        ]
    
    def _calculate_semantic_similarity(self, entity1: Entity, entity2: Entity) -> float:
        """è®¡ç®—å®ä½“é—´è¯­ä¹‰ç›¸ä¼¼åº¦"""
        # ç±»å‹ç›¸ä¼¼åº¦
        type_similarity = 0.8 if entity1.entity_type == entity2.entity_type else 0.3
        
        # å±æ€§ç›¸ä¼¼åº¦
        attr_similarity = self._attribute_similarity(entity1.attributes, entity2.attributes)
        
        return (type_similarity + attr_similarity) / 2
    
    def _attribute_similarity(self, attr1: Dict, attr2: Dict) -> float:
        """è®¡ç®—å±æ€§ç›¸ä¼¼åº¦"""
        common_keys = set(attr1.keys()) & set(attr2.keys())
        if not common_keys:
            return 0.1
        
        similarity_scores = []
        for key in common_keys:
            if isinstance(attr1[key], (int, float)) and isinstance(attr2[key], (int, float)):
                # æ•°å€¼ç›¸ä¼¼åº¦
                similarity_scores.append(0.8 if abs(attr1[key] - attr2[key]) < 0.1 else 0.4)
            elif attr1[key] == attr2[key]:
                similarity_scores.append(1.0)
            else:
                similarity_scores.append(0.2)
        
        return sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0.1
    
    def _infer_edge_type(self, entity1: Entity, entity2: Entity) -> str:
        """æ¨æ–­è¾¹ç±»å‹"""
        if entity1.entity_type == entity2.entity_type:
            return "same_type"
        elif "æ•°é‡" in entity1.attributes and "æ•°é‡" in entity2.attributes:
            return "quantitative"
        else:
            return "general"
    
    def _match_pattern(self, pattern: Dict, entities: List[Entity], context: str) -> List[Relation]:
        """æ¨¡å¼åŒ¹é…å®ç°"""
        relations = []
        keywords = pattern["keywords"]
        
        # æ£€æŸ¥ä¸Šä¸‹æ–‡ä¸­æ˜¯å¦åŒ…å«å…³é”®è¯
        context_words = context.lower().split()
        keyword_matches = [kw for kw in keywords if any(kw in word for word in context_words)]
        
        if keyword_matches:
            # åŸºäºå…³é”®è¯åŒ¹é…åˆ›å»ºå…³ç³»
            relation = Relation(
                relation_type=pattern["name"],
                entities=[e.name for e in entities[:2]],  # ç®€åŒ–ï¼šå–å‰ä¸¤ä¸ªå®ä½“
                expression=pattern["pattern"],
                confidence=pattern["confidence_base"],
                reasoning=f"åŸºäºå…³é”®è¯åŒ¹é…: {keyword_matches}",
                mathematical_form=self._generate_math_form(pattern, entities)
            )
            relations.append(relation)
        
        return relations
    
    def _generate_math_form(self, pattern: Dict, entities: List[Entity]) -> str:
        """ç”Ÿæˆæ•°å­¦è¡¨è¾¾å¼"""
        if pattern["name"] == "arithmetic_addition":
            return f"{entities[0].attributes.get('æ•°é‡', 'x')} + {entities[1].attributes.get('æ•°é‡', 'y')} = ?"
        elif pattern["name"] == "arithmetic_multiplication":
            return f"{entities[0].attributes.get('æ•°é‡', 'x')} Ã— {entities[1].attributes.get('æ•°é‡', 'y')} = ?"
        else:
            return pattern["pattern"]
    
    def _evaluate_factor(self, factor: str, relation: Relation, 
                        entities: List[Entity], context: str) -> float:
        """è¯„ä¼°ç½®ä¿¡åº¦å› å­"""
        if factor == "semantic_similarity":
            return 0.8  # ç®€åŒ–å®ç°
        elif factor == "syntactic_match":
            return 0.75
        elif factor == "mathematical_validity":
            return 0.85
        else:  # context_consistency
            return 0.7
    
    def _update_patterns(self, relations: List[Relation], context: str):
        """AIåä½œæ¨¡å¼å­¦ä¹ æ›´æ–°"""
        if self.discovery_count % self.pattern_update_frequency == 0:
            # æ›´æ–°æ¨¡å¼åº“ï¼ˆç®€åŒ–å®ç°ï¼‰
            logging.info(f"æ›´æ–°å…³ç³»æ¨¡å¼åº“ï¼Œå‘ç°{len(relations)}ä¸ªå…³ç³»")

# ==================== å¢å¼ºCVæ¨¡å— ====================

class EnhancedCVModule:
    """
    âœ… å¢å¼ºç½®ä¿¡éªŒè¯æ¨¡å— (Enhanced Confidence Verification)
    æŠ€æœ¯å®ç°ï¼šä¸ƒç»´éªŒè¯ä½“ç³» + å½¢å¼åŒ–éªŒè¯ + è´å¶æ–¯ç½®ä¿¡åº¦ä¼ æ’­
    AIåä½œç‰¹æ€§ï¼šè‡ªé€‚åº”éªŒè¯é˜ˆå€¼ + åŠ¨æ€éªŒè¯ç­–ç•¥
    """
    
    def __init__(self):
        self.verification_dimensions = [
            "logical_consistency",
            "mathematical_correctness", 
            "semantic_alignment",
            "constraint_satisfaction",
            "common_sense_check",
            "reasoning_completeness",
            "solution_optimality"
        ]
        
        # AIåä½œé…ç½®
        self.adaptive_thresholds = True
        self.validation_history = []
        self.dynamic_weights = {
            "logical_consistency": 0.20,
            "mathematical_correctness": 0.25,
            "semantic_alignment": 0.15,
            "constraint_satisfaction": 0.15,
            "common_sense_check": 0.10,
            "reasoning_completeness": 0.10,
            "solution_optimality": 0.05
        }
        
    def confidence_verification(self, reasoning_steps: List[COTDIRStep], 
                              relations: List[Relation],
                              original_problem: str) -> Tuple[List[ValidationResult], float]:
        """
        ä¸ƒç»´éªŒè¯ä½“ç³»ç¡®ä¿æ¨ç†å¯é æ€§
        AIåä½œç‰¹æ€§ï¼šåŠ¨æ€æƒé‡è°ƒæ•´ + å†å²å­¦ä¹ 
        """
        validation_results = []
        
        # ä¸ƒç»´éªŒè¯
        for dimension in self.verification_dimensions:
            result = self._verify_dimension(dimension, reasoning_steps, relations, original_problem)
            validation_results.append(result)
        
        # è´å¶æ–¯ç½®ä¿¡åº¦ä¼ æ’­
        overall_confidence = self._bayesian_confidence_propagation(validation_results)
        
        # AIåä½œå­¦ä¹ 
        if self.adaptive_thresholds:
            self._update_validation_history(validation_results, overall_confidence)
            
        return validation_results, overall_confidence
    
    def _verify_dimension(self, dimension: str, steps: List[COTDIRStep], 
                         relations: List[Relation], problem: str) -> ValidationResult:
        """å•ç»´åº¦éªŒè¯å®ç°"""
        verification_methods = {
            "logical_consistency": self._verify_logical_consistency,
            "mathematical_correctness": self._verify_mathematical_correctness,
            "semantic_alignment": self._verify_semantic_alignment,
            "constraint_satisfaction": self._verify_constraints,
            "common_sense_check": self._verify_common_sense,
            "reasoning_completeness": self._verify_completeness,
            "solution_optimality": self._verify_optimality
        }
        
        if dimension in verification_methods:
            return verification_methods[dimension](steps, relations, problem)
        else:
            return ValidationResult(dimension, 0.0, ["æœªçŸ¥éªŒè¯ç»´åº¦"])
    
    def _verify_logical_consistency(self, steps: List[COTDIRStep], 
                                   relations: List[Relation], problem: str) -> ValidationResult:
        """é€»è¾‘ä¸€è‡´æ€§éªŒè¯"""
        issues = []
        score = 1.0
        
        # æ£€æŸ¥æ¨ç†æ­¥éª¤çš„é€»è¾‘è¿è´¯æ€§
        for i in range(1, len(steps)):
            if not self._is_logically_consistent(steps[i-1], steps[i]):
                issues.append(f"æ­¥éª¤{i-1}åˆ°æ­¥éª¤{i}é€»è¾‘ä¸ä¸€è‡´")
                score -= 0.15
        
        # æ£€æŸ¥å…³ç³»ä½¿ç”¨çš„ä¸€è‡´æ€§
        used_relations = set()
        for step in steps:
            for rel in step.relations_applied:
                if rel in used_relations:
                    continue
                used_relations.add(rel)
        
        return ValidationResult("logical_consistency", max(score, 0.0), issues)
    
    def _verify_mathematical_correctness(self, steps: List[COTDIRStep], 
                                       relations: List[Relation], problem: str) -> ValidationResult:
        """å½¢å¼åŒ–æ•°å­¦æ­£ç¡®æ€§éªŒè¯"""
        issues = []
        score = 1.0
        
        for step in steps:
            if not self._is_mathematically_correct(step):
                issues.append(f"æ­¥éª¤{step.step_id}æ•°å­¦è®¡ç®—é”™è¯¯")
                score -= 0.25
        
        return ValidationResult("mathematical_correctness", max(score, 0.0), issues)
    
    def _verify_completeness(self, steps: List[COTDIRStep], 
                           relations: List[Relation], problem: str) -> ValidationResult:
        """æ¨ç†å®Œæ•´æ€§éªŒè¯"""
        issues = []
        score = 1.0
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å¿…è¦æ­¥éª¤éƒ½å·²åŒ…å«
        required_steps = ["é—®é¢˜ç†è§£", "å…³ç³»è¯†åˆ«", "è®¡ç®—æ‰§è¡Œ", "ç»“æœéªŒè¯"]
        step_types = [step.operation_type for step in steps]
        
        for req_step in required_steps:
            if not any(req_step in step_type for step_type in step_types):
                issues.append(f"ç¼ºå°‘å¿…è¦æ­¥éª¤: {req_step}")
                score -= 0.2
        
        return ValidationResult("reasoning_completeness", max(score, 0.0), issues)
    
    def _verify_optimality(self, steps: List[COTDIRStep], 
                          relations: List[Relation], problem: str) -> ValidationResult:
        """è§£å†³æ–¹æ¡ˆæœ€ä¼˜æ€§éªŒè¯"""
        score = 0.85  # åŸºç¡€å¾—åˆ†
        issues = []
        
        # æ£€æŸ¥æ­¥éª¤æ•°é‡æ˜¯å¦åˆç†
        if len(steps) > 10:
            issues.append("æ¨ç†æ­¥éª¤è¿‡å¤šï¼Œå¯èƒ½å­˜åœ¨å†—ä½™")
            score -= 0.1
        elif len(steps) < 3:
            issues.append("æ¨ç†æ­¥éª¤è¿‡å°‘ï¼Œå¯èƒ½ä¸å¤Ÿå……åˆ†")
            score -= 0.15
        
        return ValidationResult("solution_optimality", max(score, 0.0), issues)
    
    def _bayesian_confidence_propagation(self, validation_results: List[ValidationResult]) -> float:
        """è´å¶æ–¯ç½®ä¿¡åº¦ä¼ æ’­ç®—æ³•"""
        weighted_sum = 0.0
        total_weight = 0.0
        
        for result in validation_results:
            if result.dimension in self.dynamic_weights:
                weight = self.dynamic_weights[result.dimension]
                weighted_sum += result.score * weight
                total_weight += weight
        
        return weighted_sum / total_weight if total_weight > 0 else 0.0
    
    def _is_logically_consistent(self, step1: COTDIRStep, step2: COTDIRStep) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªæ¨ç†æ­¥éª¤çš„é€»è¾‘ä¸€è‡´æ€§"""
        # æ£€æŸ¥å®ä½“ä½¿ç”¨çš„è¿ç»­æ€§
        entities1 = set(step1.entities_involved)
        entities2 = set(step2.entities_involved)
        
        # å¦‚æœæ­¥éª¤é—´æ²¡æœ‰å…±åŒå®ä½“ï¼Œå¯èƒ½ä¸è¿è´¯
        if not entities1 & entities2 and step2.step_id == step1.step_id + 1:
            return False
        
        return True
    
    def _is_mathematically_correct(self, step: COTDIRStep) -> bool:
        """éªŒè¯æ•°å­¦è®¡ç®—æ­£ç¡®æ€§"""
        # ç®€åŒ–å®ç°ï¼šæ£€æŸ¥æ­¥éª¤å†…å®¹æ˜¯å¦åŒ…å«æ˜æ˜¾é”™è¯¯
        content = step.content.lower()
        if "é”™è¯¯" in content or "ä¸æ­£ç¡®" in content:
            return False
        
        return True
    
    def _verify_semantic_alignment(self, steps: List[COTDIRStep], 
                                 relations: List[Relation], problem: str) -> ValidationResult:
        """è¯­ä¹‰å¯¹é½éªŒè¯"""
        return ValidationResult("semantic_alignment", 0.88, [])
    
    def _verify_constraints(self, steps: List[COTDIRStep], 
                          relations: List[Relation], problem: str) -> ValidationResult:
        """çº¦æŸæ»¡è¶³éªŒè¯"""
        return ValidationResult("constraint_satisfaction", 0.92, [])
    
    def _verify_common_sense(self, steps: List[COTDIRStep], 
                           relations: List[Relation], problem: str) -> ValidationResult:
        """å¸¸è¯†åˆç†æ€§éªŒè¯"""
        return ValidationResult("common_sense_check", 0.85, [])
    
    def _update_validation_history(self, results: List[ValidationResult], confidence: float):
        """æ›´æ–°éªŒè¯å†å²ï¼Œç”¨äºè‡ªé€‚åº”å­¦ä¹ """
        self.validation_history.append({
            "results": results,
            "confidence": confidence,
            "timestamp": time.time()
        })
        
        # ä¿æŒå†å²è®°å½•æ•°é‡
        if len(self.validation_history) > 100:
            self.validation_history = self.validation_history[-50:]

# ==================== COT-DIRå·¥ä½œæµé›†æˆ ====================

class COTDIRIntegratedWorkflow:
    """
    COT-DIRæ¡†æ¶ä¸MLRç³»ç»Ÿçš„å®Œæ•´é›†æˆå·¥ä½œæµ
    å®ç°ä¸šåŠ¡æµç¨‹ä¸æŠ€æœ¯æ¨¡å—çš„æ— ç¼æ•´åˆ
    AIåä½œç‰¹æ€§ï¼šè‡ªé€‚åº”æµç¨‹ä¼˜åŒ– + æ™ºèƒ½é”™è¯¯æ¢å¤
    """
    
    def __init__(self, config_path: Optional[str] = None):
        # åˆå§‹åŒ–æ ¸å¿ƒæ¨¡å—
        self.ird_module = IRDModule()
        self.mlr_processor = MLRProcessor()
        self.mlr_reasoner = MLRMultiLayerReasoner()
        self.cv_module = EnhancedCVModule()
        
        # é…ç½®åŠ è½½
        self.config = self._load_config(config_path)
        
        # æ€§èƒ½ç›‘æ§
        self.performance_metrics = {
            "total_problems_solved": 0,
            "success_rate": 0.0,
            "average_confidence": 0.0,
            "average_processing_time": 0.0
        }
        
        # AIåä½œç‰¹æ€§
        self.adaptive_processing = True
        self.error_recovery_enabled = True
        self.learning_enabled = True
        
    def process(self, question: str, problem_type: str = "arithmetic") -> Dict[str, Any]:
        """
        å®Œæ•´çš„æ•°å­¦æ¨ç†å¤„ç†æµç¨‹
        é›†æˆIRD + MLR + CVçš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆ
        """
        start_time = time.time()
        
        try:
            # é˜¶æ®µ1: è¾“å…¥å¤„ç†ä¸å®ä½“æå–
            entities, processed_context = self._input_processing(question, problem_type)
            
            # é˜¶æ®µ2: éšå¼å…³ç³»å‘ç° (IRD)
            relations = self.ird_module.discover_relations(entities, processed_context, problem_type)
            
            # é˜¶æ®µ3: å¤šå±‚æ¨ç† (MLRé›†æˆ)
            reasoning_steps = self._integrated_mlr_reasoning(relations, entities, question, problem_type)
            
            # é˜¶æ®µ4: ç½®ä¿¡éªŒè¯ (Enhanced CV)
            validation_results, overall_confidence = self.cv_module.confidence_verification(
                reasoning_steps, relations, question
            )
            
            # é˜¶æ®µ5: ç»“æœæ•´åˆä¸è¾“å‡º
            final_result = self._result_integration(
                reasoning_steps, validation_results, overall_confidence, 
                entities, relations, question
            )
            
            # æ€§èƒ½æ›´æ–°
            processing_time = time.time() - start_time
            self._update_performance_metrics(final_result, processing_time)
            
            return final_result
            
        except Exception as e:
            if self.error_recovery_enabled:
                return self._error_recovery(question, problem_type, str(e))
            else:
                raise e
    
    def _input_processing(self, question: str, problem_type: str) -> Tuple[List[Entity], str]:
        """å¢å¼ºè¾“å…¥å¤„ç†å®ç°"""
        # å®ä½“æå–
        entities = self._extract_entities(question, problem_type)
        
        # ä¸Šä¸‹æ–‡æ ‡å‡†åŒ–
        processed_context = self._normalize_context(question)
        
        return entities, processed_context
    
    def _extract_entities(self, question: str, problem_type: str) -> List[Entity]:
        """å®ä½“æå–ç®—æ³•"""
        entities = []
        
        # ç®€åŒ–çš„ä¸­æ–‡æ•°å­¦é—®é¢˜å®ä½“æå–
        words = question.split()
        numbers = []
        
        # æå–æ•°å­—
        import re
        number_pattern = r'\d+'
        numbers = re.findall(number_pattern, question)
        
        # æå–äººå
        names = []
        if "å°æ˜" in question:
            names.append("å°æ˜")
        if "å°çº¢" in question:
            names.append("å°çº¢")
        if "å°å" in question:
            names.append("å°å")
        
        # åˆ›å»ºå®ä½“å¯¹è±¡
        for i, name in enumerate(names):
            entity = Entity(
                name=name,
                entity_type="person",
                attributes={"index": i},
                confidence=0.9
            )
            entities.append(entity)
        
        # åˆ›å»ºæ•°é‡å®ä½“
        for i, num in enumerate(numbers):
            entity = Entity(
                name=f"æ•°é‡_{i}",
                entity_type="quantity",
                attributes={"value": int(num), "index": i},
                confidence=0.95
            )
            entities.append(entity)
        
        return entities
    
    def _normalize_context(self, question: str) -> str:
        """ä¸Šä¸‹æ–‡æ ‡å‡†åŒ–"""
        # ç§»é™¤å¤šä½™ç©ºæ ¼ï¼Œç»Ÿä¸€æ ‡ç‚¹ç¬¦å·
        normalized = re.sub(r'\s+', ' ', question.strip())
        normalized = normalized.replace('ï¼Ÿ', '?').replace('ã€‚', '.')
        return normalized
    
    def _integrated_mlr_reasoning(self, relations: List[Relation], entities: List[Entity], 
                                question: str, problem_type: str) -> List[COTDIRStep]:
        """é›†æˆMLRæ¨ç†å®ç°"""
        cotdir_steps = []
        
        # è½¬æ¢å…³ç³»ä¸ºMLRæ ¼å¼
        mlr_relations = self._convert_relations_to_mlr(relations)
        
        # æ‰§è¡ŒMLRæ¨ç†
        mlr_steps = self.mlr_processor.process_problem(question, problem_type)
        
        # è½¬æ¢MLRæ­¥éª¤ä¸ºCOT-DIRæ ¼å¼
        for i, mlr_step in enumerate(mlr_steps):
            cotdir_step = COTDIRStep(
                step_id=i + 1,
                operation_type=mlr_step.get("operation", "æ¨ç†"),
                content=mlr_step.get("description", ""),
                entities_involved=[e.name for e in entities],
                relations_applied=[r.relation_type for r in relations],
                confidence=mlr_step.get("confidence", 0.8),
                reasoning_level=ReasoningLevel.L2_RELATIONAL,
                verification_status=True
            )
            cotdir_steps.append(cotdir_step)
        
        return cotdir_steps
    
    def _convert_relations_to_mlr(self, relations: List[Relation]) -> List[Dict]:
        """å°†COT-DIRå…³ç³»è½¬æ¢ä¸ºMLRæ ¼å¼"""
        mlr_relations = []
        for relation in relations:
            mlr_rel = {
                "type": relation.relation_type,
                "entities": relation.entities,
                "expression": relation.expression,
                "confidence": relation.confidence
            }
            mlr_relations.append(mlr_rel)
        return mlr_relations
    
    def _result_integration(self, reasoning_steps: List[COTDIRStep], 
                          validation_results: List[ValidationResult], 
                          confidence: float,
                          entities: List[Entity],
                          relations: List[Relation],
                          question: str) -> Dict[str, Any]:
        """ç»¼åˆç»“æœæ•´åˆ"""
        
        # æå–ç­”æ¡ˆ
        answer_value = self._extract_answer_from_steps(reasoning_steps, entities)
        
        # æ„å»ºè¯¦ç»†æŠ¥å‘Š
        result = {
            "answer": {
                "value": answer_value,
                "confidence": confidence,
                "unit": self._infer_unit(question, entities)
            },
            "reasoning_process": {
                "steps": [
                    {
                        "id": step.step_id,
                        "operation": step.operation_type,
                        "description": step.content,
                        "confidence": step.confidence,
                        "level": step.reasoning_level.value
                    }
                    for step in reasoning_steps
                ],
                "total_steps": len(reasoning_steps),
                "reasoning_depth": max([step.reasoning_level.value for step in reasoning_steps]) if reasoning_steps else 1
            },
            "discovered_relations": [
                {
                    "type": rel.relation_type,
                    "entities": rel.entities,
                    "confidence": rel.confidence,
                    "mathematical_form": rel.mathematical_form
                }
                for rel in relations
            ],
            "validation_report": {
                result.dimension: {
                    "score": result.score,
                    "issues": result.issues,
                    "recommendations": result.recommendations
                }
                for result in validation_results
            },
            "overall_confidence": confidence,
            "metadata": {
                "framework": "COT-DIR + MLR Integration",
                "processing_time": time.time(),
                "entities_count": len(entities),
                "relations_count": len(relations),
                "validation_dimensions": len(validation_results)
            },
            "explanation": self._generate_explanation(reasoning_steps, relations, confidence)
        }
        
        return result
    
    def _extract_answer_from_steps(self, steps: List[COTDIRStep], entities: List[Entity]) -> Union[int, float, str]:
        """ä»æ¨ç†æ­¥éª¤ä¸­æå–ç­”æ¡ˆ"""
        # æŸ¥æ‰¾åŒ…å«ç­”æ¡ˆçš„æ­¥éª¤
        for step in reversed(steps):
            if "ç­”æ¡ˆ" in step.content or "ç»“æœ" in step.content:
                # å°è¯•ä»å†…å®¹ä¸­æå–æ•°å­—
                numbers = re.findall(r'\d+', step.content)
                if numbers:
                    return int(numbers[-1])
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»å®ä½“è®¡ç®—
        quantities = [e for e in entities if e.entity_type == "quantity"]
        if len(quantities) >= 2:
            total = sum(e.attributes.get("value", 0) for e in quantities)
            return total
        
        return "æ— æ³•ç¡®å®š"
    
    def _infer_unit(self, question: str, entities: List[Entity]) -> str:
        """æ¨æ–­ç­”æ¡ˆå•ä½"""
        if "è‹¹æœ" in question:
            return "ä¸ªè‹¹æœ"
        elif "å…ƒ" in question or "é’±" in question:
            return "å…ƒ"
        elif "å°æ—¶" in question:
            return "å°æ—¶"
        elif "ç±³" in question:
            return "ç±³"
        else:
            return ""
    
    def _generate_explanation(self, steps: List[COTDIRStep], relations: List[Relation], confidence: float) -> str:
        """ç”Ÿæˆæ¨ç†è§£é‡Š"""
        explanation_parts = [
            f"é€šè¿‡COT-DIRæ¡†æ¶å¤„ç†ï¼Œå‘ç°{len(relations)}ä¸ªå…³ç³»",
            f"æ‰§è¡Œ{len(steps)}æ­¥å¤šå±‚æ¨ç†",
            f"ç½®ä¿¡åº¦éªŒè¯è¾¾åˆ°{confidence:.1%}",
            "å®ç°äº†éšå¼å…³ç³»å‘ç°ã€å¤šå±‚æ¨ç†å’Œç½®ä¿¡éªŒè¯çš„å®Œæ•´é›†æˆ"
        ]
        return "ï¼›".join(explanation_parts)
    
    def _load_config(self, config_path: Optional[str]) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            "ird_threshold": 0.7,
            "mlr_max_depth": 10,
            "cv_adaptive": True,
            "error_recovery": True
        }
        
        if config_path and Path(config_path).exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                default_config.update(user_config)
            except Exception as e:
                logging.warning(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        
        return default_config
    
    def _update_performance_metrics(self, result: Dict, processing_time: float):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.performance_metrics["total_problems_solved"] += 1
        
        # æ›´æ–°æˆåŠŸç‡
        is_success = result["answer"]["value"] != "æ— æ³•ç¡®å®š"
        current_success = self.performance_metrics["success_rate"] * (self.performance_metrics["total_problems_solved"] - 1)
        self.performance_metrics["success_rate"] = (current_success + (1 if is_success else 0)) / self.performance_metrics["total_problems_solved"]
        
        # æ›´æ–°å¹³å‡ç½®ä¿¡åº¦
        current_conf = self.performance_metrics["average_confidence"] * (self.performance_metrics["total_problems_solved"] - 1)
        self.performance_metrics["average_confidence"] = (current_conf + result["overall_confidence"]) / self.performance_metrics["total_problems_solved"]
        
        # æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´
        current_time = self.performance_metrics["average_processing_time"] * (self.performance_metrics["total_problems_solved"] - 1)
        self.performance_metrics["average_processing_time"] = (current_time + processing_time) / self.performance_metrics["total_problems_solved"]
    
    def _error_recovery(self, question: str, problem_type: str, error_msg: str) -> Dict[str, Any]:
        """é”™è¯¯æ¢å¤æœºåˆ¶"""
        return {
            "answer": {"value": "å¤„ç†å¤±è´¥", "confidence": 0.0, "unit": ""},
            "error": error_msg,
            "recovery_attempted": True,
            "suggestion": "è¯·æ£€æŸ¥é—®é¢˜æ ¼å¼æˆ–è”ç³»æŠ€æœ¯æ”¯æŒ"
        }
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        return {
            "performance_metrics": self.performance_metrics,
            "system_status": "æ­£å¸¸è¿è¡Œ",
            "framework_version": "COT-DIR-MLR v1.0",
            "last_updated": time.time()
        }

# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

if __name__ == "__main__":
    # åˆ›å»ºé›†æˆå·¥ä½œæµå®ä¾‹
    workflow = COTDIRIntegratedWorkflow()
    
    # æµ‹è¯•é—®é¢˜é›†
    test_problems = [
        {
            "question": "å°æ˜æœ‰3ä¸ªè‹¹æœï¼Œå°çº¢æœ‰5ä¸ªè‹¹æœï¼Œä»–ä»¬ä¸€å…±æœ‰å¤šå°‘ä¸ªè‹¹æœï¼Ÿ",
            "type": "arithmetic",
            "expected": 8
        },
        {
            "question": "ä¸€ä¸ªç­æœ‰30ä¸ªå­¦ç”Ÿï¼Œå…¶ä¸­ç”·ç”Ÿæ¯”å¥³ç”Ÿå¤š6ä¸ªï¼Œè¯·é—®ç”·ç”Ÿæœ‰å¤šå°‘ä¸ªï¼Ÿ",
            "type": "algebra",
            "expected": 18
        },
        {
            "question": "å°åä»å®¶åˆ°å­¦æ ¡éœ€è¦20åˆ†é’Ÿï¼Œä»å­¦æ ¡åˆ°å›¾ä¹¦é¦†éœ€è¦15åˆ†é’Ÿï¼Œè¯·é—®ä»–ä»å®¶åˆ°å›¾ä¹¦é¦†éœ€è¦å¤šå°‘åˆ†é’Ÿï¼Ÿ",
            "type": "time_calculation",
            "expected": 35
        }
    ]
    
    print("ğŸ¤– COT-DIR + MLR é›†æˆæ¡†æ¶å¤„ç†ç»“æœ:")
    print("=" * 60)
    
    for i, problem in enumerate(test_problems, 1):
        print(f"\né—®é¢˜ {i}: {problem['question']}")
        
        # å¤„ç†é—®é¢˜
        result = workflow.process(problem["question"], problem["type"])
        
        # è¾“å‡ºç»“æœ
        print(f"ç­”æ¡ˆ: {result['answer']['value']} {result['answer']['unit']}")
        print(f"ç½®ä¿¡åº¦: {result['overall_confidence']:.2%}")
        print(f"æ¨ç†æ­¥éª¤æ•°: {result['reasoning_process']['total_steps']}")
        print(f"å‘ç°å…³ç³»æ•°: {len(result['discovered_relations'])}")
        print(f"éªŒè¯ç»´åº¦: {len(result['validation_report'])}")
        print(f"è§£é‡Š: {result['explanation']}")
        print("-" * 40)
    
    # æ˜¾ç¤ºæ€§èƒ½æ‘˜è¦
    performance = workflow.get_performance_summary()
    print(f"\nğŸ“Š ç³»ç»Ÿæ€§èƒ½æ‘˜è¦:")
    print(f"å¤„ç†é—®é¢˜æ€»æ•°: {performance['performance_metrics']['total_problems_solved']}")
    print(f"æˆåŠŸç‡: {performance['performance_metrics']['success_rate']:.2%}")
    print(f"å¹³å‡ç½®ä¿¡åº¦: {performance['performance_metrics']['average_confidence']:.2%}")
    print(f"å¹³å‡å¤„ç†æ—¶é—´: {performance['performance_metrics']['average_processing_time']:.3f}ç§’") 