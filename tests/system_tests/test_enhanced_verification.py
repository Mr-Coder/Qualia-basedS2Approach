"""
Enhanced Verification System Test Suite
======================================

æµ‹è¯•å¢å¼ºéªŒè¯ç³»ç»Ÿåœ¨GSM8Kæ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼ŒéªŒè¯ï¼š
1. æ¨ç†é€»è¾‘å‡†ç¡®æ€§ä¼˜åŒ–
2. è¯­ä¹‰ç†è§£å¢å¼º
3. å¤šæ­¥æ¨ç†é“¾éªŒè¯
4. ç­”æ¡ˆåˆç†æ€§æ£€æŸ¥
5. è‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´

Author: Math Problem Solver Team
Version: 1.0.0
"""

import json
import logging
import time
from pathlib import Path
from typing import Any, Dict, List

from enhanced_verification_system import (EnhancedVerificationSystem,
                                          HighPrecisionCalculator,
                                          MultiStepReasoningValidator,
                                          SemanticUnderstandingEngine,
                                          VerificationLevel)
from robust_reasoning_system import RobustMathematicalReasoningSystem

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('enhanced_verification_test.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class EnhancedVerificationTester:
    """å¢å¼ºéªŒè¯ç³»ç»Ÿæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.robust_system = RobustMathematicalReasoningSystem()
        self.verifier = EnhancedVerificationSystem(VerificationLevel.STANDARD)
        self.test_results = []
        
    def load_gsm8k_sample(self, num_problems: int = 20) -> List[Dict[str, Any]]:
        """åŠ è½½GSM8Kæµ‹è¯•æ ·æœ¬"""
        try:
            gsm8k_path = Path("Data/GSM8K/test.jsonl")
            if not gsm8k_path.exists():
                logger.error(f"GSM8K test file not found: {gsm8k_path}")
                return []
            
            problems = []
            with open(gsm8k_path, 'r', encoding='utf-8') as f:
                for i, line in enumerate(f):
                    if i >= num_problems:
                        break
                    data = json.loads(line.strip())
                    problems.append({
                        'id': i,
                        'question': data['question'],
                        'answer': data['answer'],
                        'expected_numeric': self._extract_numeric_answer(data['answer'])
                    })
            
            logger.info(f"Loaded {len(problems)} GSM8K problems for testing")
            return problems
            
        except Exception as e:
            logger.error(f"Error loading GSM8K sample: {e}")
            return []
    
    def _extract_numeric_answer(self, answer_text: str) -> float:
        """ä»ç­”æ¡ˆæ–‡æœ¬ä¸­æå–æ•°å€¼"""
        import re

        # æŸ¥æ‰¾æœ€åä¸€ä¸ªæ•°å­—ï¼ˆé€šå¸¸æ˜¯æœ€ç»ˆç­”æ¡ˆï¼‰
        numbers = re.findall(r'-?\d+(?:\.\d+)?', answer_text)
        if numbers:
            try:
                return float(numbers[-1])
            except ValueError:
                pass
        return 0.0
    
    def test_single_problem(self, problem: Dict[str, Any]) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªé—®é¢˜çš„å¢å¼ºéªŒè¯"""
        start_time = time.time()
        
        try:
            # 1. ä½¿ç”¨robustç³»ç»Ÿæ±‚è§£é—®é¢˜
            solution = self.robust_system.solve_mathematical_problem(problem['question'])
            
            # 2. å¢å¼ºéªŒè¯åˆ†æ
            verification_result = self.verifier.comprehensive_verification(
                problem['question'],
                solution.get('reasoning_steps', []),
                solution.get('final_answer')
            )
            
            processing_time = time.time() - start_time
            
            # 3. å‡†ç¡®æ€§æ£€æŸ¥
            predicted_answer = solution.get('final_answer', 0)
            expected_answer = problem['expected_numeric']
            is_correct = abs(predicted_answer - expected_answer) < 0.01 if isinstance(predicted_answer, (int, float)) else False
            
            # 4. ç¼–è¯‘æµ‹è¯•ç»“æœ
            test_result = {
                'problem_id': problem['id'],
                'question': problem['question'],
                'expected_answer': expected_answer,
                'predicted_answer': predicted_answer,
                'is_correct': is_correct,
                'processing_time': processing_time,
                'reasoning_steps_count': len(solution.get('reasoning_steps', [])),
                
                # éªŒè¯ç»“æœ
                'verification_score': verification_result['overall_verification_score'],
                'semantic_analysis': {
                    'problem_type': verification_result['semantic_analysis'].problem_type,
                    'understanding_score': verification_result['semantic_analysis'].understanding_score,
                    'semantic_confidence': verification_result['semantic_analysis'].semantic_confidence,
                    'ambiguity_flags': verification_result['semantic_analysis'].ambiguity_flags
                },
                'chain_validation': {
                    'is_logically_consistent': verification_result['chain_validation'].is_logically_consistent,
                    'consistency_score': verification_result['chain_validation'].consistency_score,
                    'logical_errors_count': len(verification_result['chain_validation'].logical_errors),
                    'semantic_coherence': verification_result['chain_validation'].semantic_coherence
                },
                'precision_analysis': {
                    'overall_precision_score': verification_result['precision_results']['overall_precision_score'],
                    'precision_errors_count': len(verification_result['precision_results']['precision_errors'])
                },
                'answer_reasonableness': verification_result['answer_reasonableness'],
                'recommendations': verification_result['recommendations']
            }
            
            logger.info(f"Problem {problem['id']}: {'âœ“' if is_correct else 'âœ—'} "
                       f"(Verification: {verification_result['overall_verification_score']:.3f})")
            
            return test_result
            
        except Exception as e:
            logger.error(f"Error testing problem {problem['id']}: {e}")
            return {
                'problem_id': problem['id'],
                'error': str(e),
                'processing_time': time.time() - start_time
            }
    
    def run_comprehensive_test(self, num_problems: int = 20) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        logger.info(f"Starting comprehensive enhanced verification test on {num_problems} problems")
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        problems = self.load_gsm8k_sample(num_problems)
        if not problems:
            return {'error': 'No test data available'}
        
        # æµ‹è¯•æ¯ä¸ªé—®é¢˜
        test_results = []
        for problem in problems:
            result = self.test_single_problem(problem)
            test_results.append(result)
            self.test_results.append(result)
        
        # è®¡ç®—ç»Ÿè®¡ç»“æœ
        stats = self._calculate_test_statistics(test_results)
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report = self._generate_detailed_report(test_results, stats)
        
        # ä¿å­˜ç»“æœ
        self._save_test_results(test_results, stats, report)
        
        return {
            'test_results': test_results,
            'statistics': stats,
            'report': report
        }
    
    def _calculate_test_statistics(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—æµ‹è¯•ç»Ÿè®¡"""
        valid_results = [r for r in results if 'error' not in r]
        
        if not valid_results:
            return {'error': 'No valid results'}
        
        # åŸºç¡€ç»Ÿè®¡
        total_problems = len(valid_results)
        correct_answers = sum(1 for r in valid_results if r.get('is_correct', False))
        accuracy = correct_answers / total_problems
        
        # å¤„ç†æ—¶é—´ç»Ÿè®¡
        processing_times = [r['processing_time'] for r in valid_results]
        avg_processing_time = sum(processing_times) / len(processing_times)
        
        # æ¨ç†æ­¥éª¤ç»Ÿè®¡
        reasoning_steps = [r['reasoning_steps_count'] for r in valid_results]
        avg_reasoning_steps = sum(reasoning_steps) / len(reasoning_steps)
        
        # éªŒè¯åˆ†æ•°ç»Ÿè®¡
        verification_scores = [r['verification_score'] for r in valid_results]
        avg_verification_score = sum(verification_scores) / len(verification_scores)
        
        # è¯­ä¹‰ç†è§£ç»Ÿè®¡
        understanding_scores = [r['semantic_analysis']['understanding_score'] for r in valid_results]
        avg_understanding_score = sum(understanding_scores) / len(understanding_scores)
        
        # é€»è¾‘ä¸€è‡´æ€§ç»Ÿè®¡
        consistent_chains = sum(1 for r in valid_results if r['chain_validation']['is_logically_consistent'])
        consistency_rate = consistent_chains / total_problems
        
        # ç²¾åº¦ç»Ÿè®¡
        precision_scores = [r['precision_analysis']['overall_precision_score'] for r in valid_results]
        avg_precision_score = sum(precision_scores) / len(precision_scores)
        
        # é—®é¢˜ç±»å‹åˆ†å¸ƒ
        problem_types = {}
        for r in valid_results:
            ptype = r['semantic_analysis']['problem_type']
            problem_types[ptype] = problem_types.get(ptype, 0) + 1
        
        return {
            'total_problems': total_problems,
            'correct_answers': correct_answers,
            'accuracy': accuracy,
            'avg_processing_time': avg_processing_time,
            'avg_reasoning_steps': avg_reasoning_steps,
            'avg_verification_score': avg_verification_score,
            'avg_understanding_score': avg_understanding_score,
            'consistency_rate': consistency_rate,
            'avg_precision_score': avg_precision_score,
            'problem_type_distribution': problem_types,
            'reasoning_steps_distribution': {
                '1-2 steps': sum(1 for r in reasoning_steps if 1 <= r <= 2),
                '3-5 steps': sum(1 for r in reasoning_steps if 3 <= r <= 5),
                '6+ steps': sum(1 for r in reasoning_steps if r >= 6)
            }
        }
    
    def _generate_detailed_report(self, results: List[Dict[str, Any]], stats: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        report = f"""
=== Enhanced Verification System Test Report ===
Test Date: {time.strftime('%Y-%m-%d %H:%M:%S')}
Total Problems Tested: {stats['total_problems']}

=== CORE PERFORMANCE METRICS ===
âœ“ Answer Accuracy: {stats['accuracy']:.1%} ({stats['correct_answers']}/{stats['total_problems']})
âœ“ Average Processing Time: {stats['avg_processing_time']:.4f}s
âœ“ Average Reasoning Steps: {stats['avg_reasoning_steps']:.1f}

=== ENHANCED VERIFICATION METRICS ===
ğŸ” Overall Verification Score: {stats['avg_verification_score']:.3f}
ğŸ§  Semantic Understanding Score: {stats['avg_understanding_score']:.3f}
âš¡ Logic Consistency Rate: {stats['consistency_rate']:.1%}
ğŸ¯ Calculation Precision Score: {stats['avg_precision_score']:.3f}

=== PROBLEM TYPE DISTRIBUTION ==="""
        
        for ptype, count in stats['problem_type_distribution'].items():
            percentage = count / stats['total_problems'] * 100
            report += f"\nâ€¢ {ptype}: {count} problems ({percentage:.1f}%)"
        
        report += f"""

=== REASONING COMPLEXITY ANALYSIS ===
â€¢ Simple (1-2 steps): {stats['reasoning_steps_distribution']['1-2 steps']} problems
â€¢ Medium (3-5 steps): {stats['reasoning_steps_distribution']['3-5 steps']} problems  
â€¢ Complex (6+ steps): {stats['reasoning_steps_distribution']['6+ steps']} problems

=== VERIFICATION QUALITY INSIGHTS ==="""
        
        # åˆ†æéªŒè¯è´¨é‡
        high_verification = sum(1 for r in results if r.get('verification_score', 0) > 0.8)
        report += f"\nâ€¢ High Verification Quality (>0.8): {high_verification}/{stats['total_problems']} ({high_verification/stats['total_problems']:.1%})"
        
        # é€»è¾‘é”™è¯¯åˆ†æ
        total_logical_errors = sum(r['chain_validation']['logical_errors_count'] for r in results if 'error' not in r)
        report += f"\nâ€¢ Total Logical Errors Detected: {total_logical_errors}"
        
        # ç²¾åº¦é”™è¯¯åˆ†æ
        total_precision_errors = sum(r['precision_analysis']['precision_errors_count'] for r in results if 'error' not in r)
        report += f"\nâ€¢ Total Precision Errors Detected: {total_precision_errors}"
        
        # è¯­ä¹‰æ­§ä¹‰åˆ†æ
        ambiguous_problems = sum(1 for r in results if r.get('semantic_analysis', {}).get('ambiguity_flags', []))
        report += f"\nâ€¢ Problems with Semantic Ambiguities: {ambiguous_problems}"
        
        report += f"""

=== SYSTEM RECOMMENDATIONS ==="""
        
        # æ”¶é›†æ¨èå»ºè®®
        all_recommendations = []
        for r in results:
            if 'error' not in r:
                all_recommendations.extend(r.get('recommendations', []))
        
        # ç»Ÿè®¡æ¨èé¢‘ç‡
        rec_counts = {}
        for rec in all_recommendations:
            rec_counts[rec] = rec_counts.get(rec, 0) + 1
        
        for rec, count in sorted(rec_counts.items(), key=lambda x: x[1], reverse=True):
            if count > 1:
                report += f"\nâ€¢ {rec} (å‡ºç° {count} æ¬¡)"
        
        return report
    
    def _save_test_results(self, results: List[Dict[str, Any]], stats: Dict[str, Any], report: str):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = f"enhanced_verification_results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'test_results': results,
                'statistics': stats,
                'timestamp': timestamp
            }, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = f"enhanced_verification_report_{timestamp}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"Test results saved to {results_file}")
        logger.info(f"Test report saved to {report_file}")
        
        # æ‰“å°æŠ¥å‘Šåˆ°æ§åˆ¶å°
        print(report)

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=== Enhanced Verification System Test Suite ===")
    print("Testing improvements in:")
    print("1. æ¨ç†é€»è¾‘å‡†ç¡®æ€§ä¼˜åŒ–")
    print("2. è¯­ä¹‰ç†è§£å¢å¼º") 
    print("3. å¤šæ­¥æ¨ç†é“¾éªŒè¯")
    print("4. ç­”æ¡ˆåˆç†æ€§æ£€æŸ¥")
    print("5. è‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´")
    print("=" * 50)
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = EnhancedVerificationTester()
    
    # è¿è¡Œæµ‹è¯•
    results = tester.run_comprehensive_test(num_problems=20)
    
    if 'error' in results:
        print(f"Test failed: {results['error']}")
        return
    
    print("\n=== Test Completed Successfully ===")
    print(f"Accuracy: {results['statistics']['accuracy']:.1%}")
    print(f"Verification Score: {results['statistics']['avg_verification_score']:.3f}")
    print(f"Processing Time: {results['statistics']['avg_processing_time']:.4f}s")

if __name__ == "__main__":
    main() 