"""
GSM8Kæ•°æ®é›†ä¸Šçš„COT-DIR+MLRé›†æˆç³»ç»Ÿæµ‹è¯•
æµ‹è¯•çœŸå®žæ•°å­¦æŽ¨ç†èƒ½åŠ›å’Œæ€§èƒ½æŒ‡æ ‡

è¿è¡Œæ–¹å¼ï¼š
python gsm8k_cotdir_test.py --num_samples 20 --output_file gsm8k_results.json
"""

import argparse
import json
import logging
import random
import re
import sys
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

# æ·»åŠ srcè·¯å¾„
sys.path.append('src')

def load_gsm8k_dataset(file_path: str, num_samples: Optional[int] = None) -> List[Dict]:
    """åŠ è½½GSM8Kæ•°æ®é›†"""
    problems = []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f):
                if line.strip():
                    try:
                        data = json.loads(line.strip())
                        
                        # æå–ç­”æ¡ˆæ•°å­—
                        answer_text = data.get('answer', '')
                        answer_match = re.search(r'#### (\d+)', answer_text)
                        if answer_match:
                            answer_value = int(answer_match.group(1))
                        else:
                            # å°è¯•ä»Žç­”æ¡ˆæ–‡æœ¬ä¸­æå–æœ€åŽä¸€ä¸ªæ•°å­—
                            numbers = re.findall(r'\d+', answer_text)
                            answer_value = int(numbers[-1]) if numbers else 0
                        
                        problem = {
                            'id': line_num + 1,
                            'question': data.get('question', ''),
                            'answer': answer_value,
                            'solution_steps': answer_text,
                            'difficulty': 'medium'  # GSM8Ké—®é¢˜é€šå¸¸æ˜¯ä¸­ç­‰éš¾åº¦
                        }
                        problems.append(problem)
                        
                        if num_samples and len(problems) >= num_samples:
                            break
                            
                    except json.JSONDecodeError as e:
                        logging.warning(f"ç¬¬{line_num + 1}è¡ŒJSONè§£æžå¤±è´¥: {e}")
                        continue
                        
    except FileNotFoundError:
        logging.error(f"GSM8Kæ•°æ®é›†æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        return []
    
    if num_samples:
        problems = random.sample(problems, min(num_samples, len(problems)))
    
    return problems

class GSM8KProcessor:
    """GSM8Ké—®é¢˜å¤„ç†å™¨"""
    
    def __init__(self):
        # å°è¯•å¯¼å…¥å®Œæ•´ç³»ç»Ÿ
        try:
            from reasoning_engine.cotdir_integration import \
                COTDIRIntegratedWorkflow
            self.workflow = COTDIRIntegratedWorkflow()
            self.use_full_system = True
            print("âœ“ ä½¿ç”¨å®Œæ•´COT-DIR+MLRé›†æˆç³»ç»Ÿ")
        except ImportError:
            print("âš ï¸ ä½¿ç”¨ç®€åŒ–æ•°å­¦æŽ¨ç†ç³»ç»Ÿ")
            self.workflow = None
            self.use_full_system = False
    
    def process_problem(self, problem: Dict) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªGSM8Ké—®é¢˜"""
        question = problem['question']
        expected_answer = problem['answer']
        
        start_time = time.time()
        
        if self.use_full_system and self.workflow:
            # ä½¿ç”¨å®Œæ•´ç³»ç»Ÿ
            try:
                result = self.workflow.process(question, "word_problem")
                
                return {
                    'problem_id': problem['id'],
                    'question': question,
                    'expected_answer': expected_answer,
                    'predicted_answer': result['answer']['value'],
                    'confidence': result['overall_confidence'],
                    'processing_time': time.time() - start_time,
                    'reasoning_steps': result['reasoning_process']['total_steps'],
                    'discovered_relations': len(result.get('discovered_relations', [])),
                    'validation_score': self._calculate_validation_score(result.get('validation_report', {})),
                    'is_correct': result['answer']['value'] == expected_answer,
                    'detailed_result': result,
                    'system_type': 'full_cotdir_mlr'
                }
                
            except Exception as e:
                logging.error(f"å¤„ç†é—®é¢˜{problem['id']}æ—¶å‡ºé”™: {e}")
                return self._create_error_result(problem, str(e), time.time() - start_time)
        else:
            # ä½¿ç”¨ç®€åŒ–ç³»ç»Ÿ
            return self._process_with_simple_system(problem, start_time)
    
    def _process_with_simple_system(self, problem: Dict, start_time: float) -> Dict[str, Any]:
        """ä½¿ç”¨ç®€åŒ–ç³»ç»Ÿå¤„ç†é—®é¢˜"""
        question = problem['question']
        expected_answer = problem['answer']
        
        # ç®€åŒ–çš„æ•°å­¦æŽ¨ç†é€»è¾‘
        numbers = self._extract_numbers(question)
        predicted_answer = self._simple_reasoning(question, numbers)
        
        return {
            'problem_id': problem['id'],
            'question': question,
            'expected_answer': expected_answer,
            'predicted_answer': predicted_answer,
            'confidence': 0.7,  # ç®€åŒ–ç³»ç»Ÿçš„é»˜è®¤ç½®ä¿¡åº¦
            'processing_time': time.time() - start_time,
            'reasoning_steps': 3,
            'discovered_relations': 1,
            'validation_score': 0.75,
            'is_correct': predicted_answer == expected_answer,
            'system_type': 'simple_math'
        }
    
    def _extract_numbers(self, text: str) -> List[int]:
        """ä»Žæ–‡æœ¬ä¸­æå–æ•°å­—"""
        numbers = re.findall(r'\d+', text)
        return [int(num) for num in numbers]
    
    def _simple_reasoning(self, question: str, numbers: List[int]) -> int:
        """ç®€åŒ–çš„æ•°å­¦æŽ¨ç†"""
        if not numbers:
            return 0
        
        # åŸºäºŽå…³é”®è¯çš„ç®€å•æŽ¨ç†
        if any(keyword in question.lower() for keyword in ['total', 'altogether', 'sum', 'æ€»å…±', 'ä¸€å…±']):
            return sum(numbers)
        elif any(keyword in question.lower() for keyword in ['difference', 'more than', 'less than', 'å¤š', 'å°‘']):
            return max(numbers) - min(numbers) if len(numbers) >= 2 else numbers[0]
        elif any(keyword in question.lower() for keyword in ['times', 'multiply', 'å€', 'ä¹˜']):
            result = 1
            for num in numbers[:2]:  # åªå–å‰ä¸¤ä¸ªæ•°å­—
                result *= num
            return result
        else:
            # é»˜è®¤è¿”å›žæœ€å¤§æ•°å­—æˆ–æ•°å­—å’Œ
            return max(numbers) if len(numbers) == 1 else sum(numbers[:2])
    
    def _calculate_validation_score(self, validation_report: Dict) -> float:
        """è®¡ç®—éªŒè¯åˆ†æ•°"""
        if not validation_report:
            return 0.0
        
        scores = []
        for dimension, result in validation_report.items():
            if isinstance(result, dict) and 'score' in result:
                scores.append(result['score'])
        
        return sum(scores) / len(scores) if scores else 0.0
    
    def _create_error_result(self, problem: Dict, error_msg: str, processing_time: float) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯ç»“æžœ"""
        return {
            'problem_id': problem['id'],
            'question': problem['question'],
            'expected_answer': problem['answer'],
            'predicted_answer': 'ERROR',
            'confidence': 0.0,
            'processing_time': processing_time,
            'reasoning_steps': 0,
            'discovered_relations': 0,
            'validation_score': 0.0,
            'is_correct': False,
            'error': error_msg,
            'system_type': 'error'
        }

def evaluate_results(results: List[Dict]) -> Dict[str, Any]:
    """è¯„ä¼°æµ‹è¯•ç»“æžœ"""
    total_problems = len(results)
    correct_answers = sum(1 for r in results if r['is_correct'])
    accuracy = correct_answers / total_problems if total_problems > 0 else 0
    
    # è®¡ç®—å„ç§æŒ‡æ ‡
    avg_confidence = sum(r['confidence'] for r in results) / total_problems if total_problems > 0 else 0
    avg_processing_time = sum(r['processing_time'] for r in results) / total_problems if total_problems > 0 else 0
    avg_reasoning_steps = sum(r['reasoning_steps'] for r in results) / total_problems if total_problems > 0 else 0
    avg_validation_score = sum(r.get('validation_score', 0) for r in results) / total_problems if total_problems > 0 else 0
    
    # é”™è¯¯åˆ†æž
    error_count = sum(1 for r in results if r['predicted_answer'] == 'ERROR')
    error_rate = error_count / total_problems if total_problems > 0 else 0
    
    # ç½®ä¿¡åº¦åˆ†æž
    confidence_bins = {'high': 0, 'medium': 0, 'low': 0}
    for result in results:
        conf = result['confidence']
        if conf >= 0.8:
            confidence_bins['high'] += 1
        elif conf >= 0.6:
            confidence_bins['medium'] += 1
        else:
            confidence_bins['low'] += 1
    
    return {
        'total_problems': total_problems,
        'correct_answers': correct_answers,
        'accuracy': accuracy,
        'error_rate': error_rate,
        'metrics': {
            'average_confidence': avg_confidence,
            'average_processing_time': avg_processing_time,
            'average_reasoning_steps': avg_reasoning_steps,
            'average_validation_score': avg_validation_score
        },
        'confidence_distribution': confidence_bins,
        'performance_summary': {
            'excellent': sum(1 for r in results if r['is_correct'] and r['confidence'] >= 0.8),
            'good': sum(1 for r in results if r['is_correct'] and 0.6 <= r['confidence'] < 0.8),
            'fair': sum(1 for r in results if r['is_correct'] and r['confidence'] < 0.6),
            'incorrect': sum(1 for r in results if not r['is_correct'])
        }
    }

def display_results(evaluation: Dict, results: List[Dict]):
    """æ˜¾ç¤ºæµ‹è¯•ç»“æžœ"""
    print("\n" + "="*80)
    print("ðŸ§® GSM8K æ•°æ®é›†æµ‹è¯•ç»“æžœ")
    print("="*80)
    
    # åŸºæœ¬ç»Ÿè®¡
    print(f"\nðŸ“Š åŸºæœ¬ç»Ÿè®¡:")
    print(f"æµ‹è¯•é—®é¢˜æ€»æ•°: {evaluation['total_problems']}")
    print(f"æ­£ç¡®ç­”æ¡ˆæ•°: {evaluation['correct_answers']}")
    print(f"å‡†ç¡®çŽ‡: {evaluation['accuracy']:.2%}")
    print(f"é”™è¯¯çŽ‡: {evaluation['error_rate']:.2%}")
    
    # æ€§èƒ½æŒ‡æ ‡
    metrics = evaluation['metrics']
    print(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
    print(f"å¹³å‡ç½®ä¿¡åº¦: {metrics['average_confidence']:.3f}")
    print(f"å¹³å‡å¤„ç†æ—¶é—´: {metrics['average_processing_time']:.3f}ç§’")
    print(f"å¹³å‡æŽ¨ç†æ­¥éª¤: {metrics['average_reasoning_steps']:.1f}")
    print(f"å¹³å‡éªŒè¯åˆ†æ•°: {metrics['average_validation_score']:.3f}")
    
    # ç½®ä¿¡åº¦åˆ†å¸ƒ
    conf_dist = evaluation['confidence_distribution']
    print(f"\nðŸŽ¯ ç½®ä¿¡åº¦åˆ†å¸ƒ:")
    print(f"é«˜ç½®ä¿¡åº¦ (â‰¥0.8): {conf_dist['high']} ({conf_dist['high']/evaluation['total_problems']:.1%})")
    print(f"ä¸­ç­‰ç½®ä¿¡åº¦ (0.6-0.8): {conf_dist['medium']} ({conf_dist['medium']/evaluation['total_problems']:.1%})")
    print(f"ä½Žç½®ä¿¡åº¦ (<0.6): {conf_dist['low']} ({conf_dist['low']/evaluation['total_problems']:.1%})")
    
    # æ€§èƒ½åˆ†çº§
    perf = evaluation['performance_summary']
    print(f"\nðŸ† æ€§èƒ½åˆ†çº§:")
    print(f"ä¼˜ç§€ (æ­£ç¡®+é«˜ç½®ä¿¡åº¦): {perf['excellent']}")
    print(f"è‰¯å¥½ (æ­£ç¡®+ä¸­ç­‰ç½®ä¿¡åº¦): {perf['good']}")
    print(f"ä¸€èˆ¬ (æ­£ç¡®+ä½Žç½®ä¿¡åº¦): {perf['fair']}")
    print(f"é”™è¯¯: {perf['incorrect']}")
    
    # å±•ç¤ºä¸€äº›å…·ä½“ä¾‹å­
    print(f"\nðŸ” æµ‹è¯•æ ·ä¾‹:")
    correct_samples = [r for r in results if r['is_correct']][:2]
    incorrect_samples = [r for r in results if not r['is_correct']][:2]
    
    for i, sample in enumerate(correct_samples, 1):
        print(f"\nâœ“ æ­£ç¡®æ ·ä¾‹ {i}:")
        print(f"  é—®é¢˜: {sample['question'][:60]}...")
        print(f"  é¢„æœŸç­”æ¡ˆ: {sample['expected_answer']}")
        print(f"  é¢„æµ‹ç­”æ¡ˆ: {sample['predicted_answer']}")
        print(f"  ç½®ä¿¡åº¦: {sample['confidence']:.2%}")
    
    for i, sample in enumerate(incorrect_samples, 1):
        print(f"\nâœ— é”™è¯¯æ ·ä¾‹ {i}:")
        print(f"  é—®é¢˜: {sample['question'][:60]}...")
        print(f"  é¢„æœŸç­”æ¡ˆ: {sample['expected_answer']}")
        print(f"  é¢„æµ‹ç­”æ¡ˆ: {sample['predicted_answer']}")
        print(f"  ç½®ä¿¡åº¦: {sample['confidence']:.2%}")

def save_results(results: List[Dict], evaluation: Dict, output_file: str):
    """ä¿å­˜æµ‹è¯•ç»“æžœ"""
    output_data = {
        'metadata': {
            'framework': 'COT-DIR + MLR Integration',
            'dataset': 'GSM8K',
            'test_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'total_samples': len(results)
        },
        'evaluation': evaluation,
        'detailed_results': results
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nðŸ’¾ è¯¦ç»†ç»“æžœå·²ä¿å­˜è‡³: {output_file}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='GSM8Kæ•°æ®é›†ä¸Šçš„COT-DIR+MLRæµ‹è¯•')
    parser.add_argument('--dataset_path', default='Data/GSM8K/test.jsonl', help='GSM8Kæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--num_samples', type=int, default=20, help='æµ‹è¯•æ ·æœ¬æ•°é‡')
    parser.add_argument('--output_file', default=None, help='ç»“æžœè¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    log_level = logging.INFO if args.verbose else logging.WARNING
    logging.basicConfig(level=log_level, format='%(asctime)s - %(levelname)s - %(message)s')
    
    print(f"ðŸ§® GSM8Kæ•°æ®é›† COT-DIR+MLR é›†æˆç³»ç»Ÿæµ‹è¯•")
    print(f"æ•°æ®é›†è·¯å¾„: {args.dataset_path}")
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {args.num_samples}")
    
    # åŠ è½½æ•°æ®é›†
    print(f"\nðŸ“š åŠ è½½GSM8Kæ•°æ®é›†...")
    problems = load_gsm8k_dataset(args.dataset_path, args.num_samples)
    
    if not problems:
        print("âŒ æ•°æ®é›†åŠ è½½å¤±è´¥ï¼è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„ã€‚")
        return
    
    print(f"âœ“ æˆåŠŸåŠ è½½ {len(problems)} ä¸ªé—®é¢˜")
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    print(f"\nðŸ”§ åˆå§‹åŒ–å¤„ç†ç³»ç»Ÿ...")
    processor = GSM8KProcessor()
    
    # å¤„ç†é—®é¢˜
    print(f"\nðŸš€ å¼€å§‹å¤„ç†é—®é¢˜...")
    results = []
    
    for i, problem in enumerate(problems, 1):
        if args.verbose:
            print(f"\nå¤„ç†é—®é¢˜ {i}/{len(problems)}: {problem['question'][:50]}...")
        else:
            if i % 5 == 0:
                print(f"è¿›åº¦: {i}/{len(problems)}")
        
        result = processor.process_problem(problem)
        results.append(result)
        
        if args.verbose:
            status = "âœ“" if result['is_correct'] else "âœ—"
            print(f"  {status} é¢„æµ‹: {result['predicted_answer']} (æœŸæœ›: {result['expected_answer']})")
    
    # è¯„ä¼°ç»“æžœ
    print(f"\nðŸ“Š è¯„ä¼°æµ‹è¯•ç»“æžœ...")
    evaluation = evaluate_results(results)
    
    # æ˜¾ç¤ºç»“æžœ
    display_results(evaluation, results)
    
    # ä¿å­˜ç»“æžœ
    if args.output_file:
        output_file = args.output_file
    else:
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        output_file = f'gsm8k_cotdir_results_{timestamp}.json'
    
    save_results(results, evaluation, output_file)
    
    print(f"\nâœ¨ æµ‹è¯•å®Œæˆï¼")
    print("="*80)

if __name__ == "__main__":
    main() 