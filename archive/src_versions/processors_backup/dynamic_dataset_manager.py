"""
ğŸš€ Dynamic Dataset Manager - é›¶ä»£ç æ·»åŠ æ–°é¢˜ç›®
åŠ¨æ€ä»æ•°æ®é›†åŠ è½½ï¼Œæ”¯æŒè‡ªåŠ¨å‘ç°å’Œçƒ­åŠ è½½
"""

import hashlib
import json
import os
import threading
import time
from concurrent.futures import ThreadPoolExecutor
from dataclasses import asdict, dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Set

try:
    import yaml
    YAML_AVAILABLE = True
except ImportError:
    YAML_AVAILABLE = False


@dataclass
class DatasetMetadata:
    """æ•°æ®é›†å…ƒæ•°æ®"""
    name: str
    path: str
    size: int
    format: str  # json, jsonl, yaml, csv
    last_modified: float
    encoding: str = 'utf-8'
    complexity_distribution: Optional[Dict[str, float]] = None
    language: str = 'unknown'
    domain: str = 'unknown'
    checksum: str = ''
    auto_detected: bool = False


@dataclass
class ProblemBatch:
    """é—®é¢˜æ‰¹æ¬¡"""
    problems: List[Dict[str, Any]]
    source_dataset: str
    batch_id: str
    complexity_level: str = 'unknown'
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()


class DynamicDatasetManager:
    """ğŸš€ é›¶ä»£ç åŠ¨æ€æ•°æ®é›†ç®¡ç†å™¨"""
    
    def __init__(self, 
                 data_dirs: List[str] = ["Data", "datasets", "data"],
                 watch_mode: bool = True,
                 auto_reload: bool = True,
                 cache_enabled: bool = True):
        """
        åˆå§‹åŒ–åŠ¨æ€æ•°æ®é›†ç®¡ç†å™¨
        
        Args:
            data_dirs: æ•°æ®ç›®å½•åˆ—è¡¨
            watch_mode: æ˜¯å¦å¯ç”¨æ–‡ä»¶ç›‘æ§æ¨¡å¼
            auto_reload: æ˜¯å¦è‡ªåŠ¨é‡æ–°åŠ è½½å˜æ›´çš„æ•°æ®é›†
            cache_enabled: æ˜¯å¦å¯ç”¨ç¼“å­˜
        """
        self.data_dirs = [Path(d) for d in data_dirs if Path(d).exists()]
        self.watch_mode = watch_mode
        self.auto_reload = auto_reload
        self.cache_enabled = cache_enabled
        
        # æ•°æ®é›†æ³¨å†Œè¡¨
        self.datasets: Dict[str, DatasetMetadata] = {}
        self.dataset_cache: Dict[str, List[Dict]] = {}
        self.file_checksums: Dict[str, str] = {}
        
        # ç›‘æ§çŠ¶æ€
        self._watching = False
        self._watch_thread = None
        self._callbacks: List[Callable] = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'datasets_discovered': 0,
            'problems_loaded': 0,
            'auto_reloads': 0,
            'last_scan': None
        }
        
        # åˆå§‹åŒ–æ‰«æ
        self.discover_datasets()
        
        # å¯åŠ¨ç›‘æ§
        if self.watch_mode:
            self.start_watching()
    
    def discover_datasets(self) -> int:
        """ğŸ” è‡ªåŠ¨å‘ç°æ•°æ®é›†"""
        print(f"ğŸ” æ‰«ææ•°æ®é›†ç›®å½•: {[str(d) for d in self.data_dirs]}")
        
        discovered_count = 0
        supported_formats = {'.json', '.jsonl', '.yaml', '.yml'}
        
        for data_dir in self.data_dirs:
            for root, dirs, files in os.walk(data_dir):
                for file in files:
                    file_path = Path(root) / file
                    
                    # è·³è¿‡éšè—æ–‡ä»¶å’Œéæ”¯æŒæ ¼å¼
                    if file.startswith('.') or file_path.suffix.lower() not in supported_formats:
                        continue
                    
                    # è·³è¿‡å·²çŸ¥æ–‡ä»¶
                    if str(file_path) in self.datasets:
                        continue
                    
                    try:
                        metadata = self._analyze_dataset_file(file_path)
                        if metadata and metadata.size > 0:
                            self.datasets[metadata.name] = metadata
                            discovered_count += 1
                            print(f"  âœ… å‘ç°æ•°æ®é›†: {metadata.name} ({metadata.size} é—®é¢˜)")
                    
                    except Exception as e:
                        print(f"  âš ï¸ è·³è¿‡æ–‡ä»¶ {file_path}: {e}")
        
        self.stats['datasets_discovered'] = len(self.datasets)
        self.stats['last_scan'] = datetime.now()
        
        print(f"ğŸ‰ å‘ç° {discovered_count} ä¸ªæ–°æ•°æ®é›†ï¼Œæ€»è®¡ {len(self.datasets)} ä¸ªæ•°æ®é›†")
        return discovered_count
    
    def _analyze_dataset_file(self, file_path: Path) -> Optional[DatasetMetadata]:
        """åˆ†ææ•°æ®é›†æ–‡ä»¶"""
        try:
            # è®¡ç®—æ–‡ä»¶æ ¡éªŒå’Œ
            checksum = self._calculate_checksum(file_path)
            
            # åŸºæœ¬ä¿¡æ¯
            stat = file_path.stat()
            name = self._generate_dataset_name(file_path)
            format_type = file_path.suffix.lower().lstrip('.')
            
            # è¯»å–å¹¶åˆ†æå†…å®¹
            content = self._load_file_content(file_path)
            if not content or not isinstance(content, list):
                return None
            
            # åˆ†æå¤æ‚åº¦åˆ†å¸ƒ
            complexity_dist = self._analyze_complexity_distribution(content)
            
            # æ£€æµ‹è¯­è¨€å’Œé¢†åŸŸ
            language = self._detect_language(content[:5])  # ä½¿ç”¨å‰5ä¸ªæ ·æœ¬æ£€æµ‹
            domain = self._detect_domain(content[:5])
            
            return DatasetMetadata(
                name=name,
                path=str(file_path),
                size=len(content),
                format=format_type,
                last_modified=stat.st_mtime,
                complexity_distribution=complexity_dist,
                language=language,
                domain=domain,
                checksum=checksum,
                auto_detected=True
            )
            
        except Exception as e:
            print(f"åˆ†ææ–‡ä»¶ {file_path} å¤±è´¥: {e}")
            return None
    
    def _load_file_content(self, file_path: Path) -> List[Dict]:
        """åŠ è½½æ–‡ä»¶å†…å®¹"""
        with open(file_path, 'r', encoding='utf-8') as f:
            if file_path.suffix.lower() == '.jsonl':
                return [json.loads(line.strip()) for line in f if line.strip()]
            elif file_path.suffix.lower() in ['.yaml', '.yml']:
                if YAML_AVAILABLE:
                    return yaml.safe_load(f) or []
                else:
                    print(f"âš ï¸ YAMLæ–‡ä»¶éœ€è¦å®‰è£…PyYAML: {file_path}")
                    return []
            else:
                data = json.load(f)
                if isinstance(data, dict):
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œå°è¯•æå–é—®é¢˜åˆ—è¡¨
                    if 'problems' in data:
                        return data['problems']
                    elif 'data' in data:
                        return data['data']
                    else:
                        return [data]
                return data if isinstance(data, list) else []
    
    def _generate_dataset_name(self, file_path: Path) -> str:
        """ç”Ÿæˆæ•°æ®é›†åç§°"""
        # ä½¿ç”¨çˆ¶ç›®å½•å + æ–‡ä»¶å
        parent_name = file_path.parent.name
        file_name = file_path.stem
        
        if parent_name != "Data" and parent_name != "datasets":
            return f"{parent_name}_{file_name}"
        return file_name
    
    def _calculate_checksum(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶æ ¡éªŒå’Œ"""
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    
    def _analyze_complexity_distribution(self, content: List[Dict]) -> Dict[str, float]:
        """åˆ†æå¤æ‚åº¦åˆ†å¸ƒ"""
        if not content:
            return {}
        
        # ç®€å•çš„å¤æ‚åº¦å¯å‘å¼åˆ†æ
        l0_count = 0  # ç®€å•ç®—æœ¯
        l1_count = 0  # å•æ­¥æ¨ç†
        l2_count = 0  # å¤šæ­¥æ¨ç†
        l3_count = 0  # å¤æ‚æ¨ç†
        
        for item in content[:100]:  # åˆ†æå‰100ä¸ªæ ·æœ¬
            text = str(item.get('problem', '') or item.get('question', '') or item.get('text', ''))
            
            # åŸºäºå…³é”®è¯çš„ç®€å•åˆ†ç±»
            if any(word in text.lower() for word in ['what is', 'calculate', '+', '-', '*', '/']):
                l0_count += 1
            elif any(word in text.lower() for word in ['find', 'solve', 'determine']):
                l1_count += 1
            elif any(word in text.lower() for word in ['step', 'first', 'then', 'next']):
                l2_count += 1
            else:
                l3_count += 1
        
        total = l0_count + l1_count + l2_count + l3_count
        if total == 0:
            return {}
        
        return {
            'L0': round(l0_count / total * 100, 1),
            'L1': round(l1_count / total * 100, 1),
            'L2': round(l2_count / total * 100, 1),
            'L3': round(l3_count / total * 100, 1)
        }
    
    def _detect_language(self, samples: List[Dict]) -> str:
        """æ£€æµ‹è¯­è¨€"""
        if not samples:
            return 'unknown'
        
        chinese_chars = 0
        total_chars = 0
        
        for item in samples:
            text = str(item.get('problem', '') or item.get('question', '') or item.get('text', ''))
            for char in text:
                total_chars += 1
                if '\u4e00' <= char <= '\u9fff':  # ä¸­æ–‡å­—ç¬¦èŒƒå›´
                    chinese_chars += 1
        
        if total_chars == 0:
            return 'unknown'
        
        chinese_ratio = chinese_chars / total_chars
        if chinese_ratio > 0.3:
            return 'Chinese'
        elif chinese_ratio < 0.05:
            return 'English'
        else:
            return 'Mixed'
    
    def _detect_domain(self, samples: List[Dict]) -> str:
        """æ£€æµ‹é¢†åŸŸ"""
        if not samples:
            return 'unknown'
        
        domain_keywords = {
            'Elementary': ['grade', 'elementary', 'primary', 'basic'],
            'Competition': ['competition', 'contest', 'olympiad', 'AMC'],
            'Grade School': ['school', 'student', 'class'],
            'Multi-domain': ['science', 'physics', 'chemistry', 'biology']
        }
        
        text = ' '.join(str(item.get('problem', '') or item.get('question', '') or item.get('text', '')) 
                       for item in samples).lower()
        
        for domain, keywords in domain_keywords.items():
            if any(keyword in text for keyword in keywords):
                return domain
        
        return 'General'
    
    def load_dataset(self, dataset_name: str, 
                    max_samples: Optional[int] = None,
                    complexity_filter: Optional[str] = None) -> List[Dict]:
        """ğŸ”„ åŠ è½½æ•°æ®é›†ï¼ˆæ”¯æŒç¼“å­˜å’Œè¿‡æ»¤ï¼‰"""
        if dataset_name not in self.datasets:
            raise ValueError(f"æ•°æ®é›† '{dataset_name}' ä¸å­˜åœ¨ã€‚å¯ç”¨æ•°æ®é›†: {list(self.datasets.keys())}")
        
        metadata = self.datasets[dataset_name]
        
        # æ£€æŸ¥ç¼“å­˜
        if self.cache_enabled and dataset_name in self.dataset_cache:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å˜æ›´
            current_checksum = self._calculate_checksum(Path(metadata.path))
            if current_checksum == metadata.checksum:
                data = self.dataset_cache[dataset_name]
            else:
                # æ–‡ä»¶æœ‰å˜æ›´ï¼Œé‡æ–°åŠ è½½
                data = self._load_file_content(Path(metadata.path))
                self.dataset_cache[dataset_name] = data
                metadata.checksum = current_checksum
                print(f"ğŸ”„ æ£€æµ‹åˆ°æ–‡ä»¶å˜æ›´ï¼Œé‡æ–°åŠ è½½ {dataset_name}")
        else:
            # åŠ è½½æ•°æ®
            data = self._load_file_content(Path(metadata.path))
            if self.cache_enabled:
                self.dataset_cache[dataset_name] = data
        
        # åº”ç”¨è¿‡æ»¤å™¨
        if complexity_filter:
            data = self._filter_by_complexity(data, complexity_filter)
        
        # é™åˆ¶æ ·æœ¬æ•°
        if max_samples:
            data = data[:max_samples]
        
        self.stats['problems_loaded'] += len(data)
        return data
    
    def _filter_by_complexity(self, data: List[Dict], complexity_level: str) -> List[Dict]:
        """æ ¹æ®å¤æ‚åº¦è¿‡æ»¤æ•°æ®"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„è¿‡æ»¤é€»è¾‘
        # ç›®å‰è¿”å›åŸæ•°æ®
        return data
    
    def get_dynamic_batch(self, 
                         batch_size: int = 10,
                         datasets: Optional[List[str]] = None,
                         complexity_mix: Optional[Dict[str, float]] = None) -> ProblemBatch:
        """ğŸ“¦ è·å–åŠ¨æ€é—®é¢˜æ‰¹æ¬¡"""
        if not datasets:
            datasets = list(self.datasets.keys())
        
        problems = []
        selected_datasets = []
        
        for dataset_name in datasets:
            if len(problems) >= batch_size:
                break
            
            try:
                dataset_problems = self.load_dataset(dataset_name, max_samples=batch_size//len(datasets) + 1)
                problems.extend(dataset_problems)
                selected_datasets.append(dataset_name)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æ•°æ®é›† {dataset_name} å¤±è´¥: {e}")
        
        # é™åˆ¶æ‰¹æ¬¡å¤§å°
        problems = problems[:batch_size]
        
        batch_id = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        return ProblemBatch(
            problems=problems,
            source_dataset=','.join(selected_datasets),
            batch_id=batch_id
        )
    
    def start_watching(self):
        """ğŸ” å¯åŠ¨æ–‡ä»¶ç›‘æ§"""
        if self._watching:
            return
        
        self._watching = True
        self._watch_thread = threading.Thread(target=self._watch_files, daemon=True)
        self._watch_thread.start()
        print("ğŸ” å¯åŠ¨æ–‡ä»¶ç›‘æ§æ¨¡å¼")
    
    def stop_watching(self):
        """â¹ï¸ åœæ­¢æ–‡ä»¶ç›‘æ§"""
        self._watching = False
        if self._watch_thread:
            self._watch_thread.join()
        print("â¹ï¸ åœæ­¢æ–‡ä»¶ç›‘æ§")
    
    def _watch_files(self):
        """æ–‡ä»¶ç›‘æ§å¾ªç¯"""
        while self._watching:
            try:
                # æ£€æŸ¥æ–‡ä»¶å˜æ›´
                changes_detected = False
                
                for dataset_name, metadata in self.datasets.items():
                    file_path = Path(metadata.path)
                    if file_path.exists():
                        current_checksum = self._calculate_checksum(file_path)
                        if current_checksum != metadata.checksum:
                            print(f"ğŸ”„ æ£€æµ‹åˆ°æ•°æ®é›†å˜æ›´: {dataset_name}")
                            if self.auto_reload:
                                self._reload_dataset(dataset_name)
                            changes_detected = True
                
                # æ‰«ææ–°æ•°æ®é›†
                new_count = self.discover_datasets()
                if new_count > 0:
                    changes_detected = True
                
                # é€šçŸ¥å›è°ƒ
                if changes_detected and self._callbacks:
                    for callback in self._callbacks:
                        try:
                            callback(self)
                        except Exception as e:
                            print(f"å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")
                
                time.sleep(5)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                print(f"æ–‡ä»¶ç›‘æ§é”™è¯¯: {e}")
                time.sleep(10)
    
    def _reload_dataset(self, dataset_name: str):
        """é‡æ–°åŠ è½½æ•°æ®é›†"""
        try:
            metadata = self.datasets[dataset_name]
            
            # é‡æ–°åˆ†ææ–‡ä»¶
            new_metadata = self._analyze_dataset_file(Path(metadata.path))
            if new_metadata:
                self.datasets[dataset_name] = new_metadata
                
                # æ¸…é™¤ç¼“å­˜
                if dataset_name in self.dataset_cache:
                    del self.dataset_cache[dataset_name]
                
                self.stats['auto_reloads'] += 1
                print(f"âœ… é‡æ–°åŠ è½½æ•°æ®é›†: {dataset_name}")
            
        except Exception as e:
            print(f"é‡æ–°åŠ è½½æ•°æ®é›†å¤±è´¥ {dataset_name}: {e}")
    
    def add_change_callback(self, callback: Callable):
        """æ·»åŠ å˜æ›´å›è°ƒ"""
        self._callbacks.append(callback)
    
    def get_stats(self) -> Dict[str, Any]:
        """ğŸ“Š è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.stats,
            'total_datasets': len(self.datasets),
            'cached_datasets': len(self.dataset_cache),
            'watching_enabled': self._watching,
            'available_datasets': list(self.datasets.keys())
        }
    
    def list_datasets(self, detailed: bool = False) -> List[Dict[str, Any]]:
        """ğŸ“‹ åˆ—å‡ºæ‰€æœ‰æ•°æ®é›†"""
        if not detailed:
            return list(self.datasets.keys())
        
        datasets_info = []
        for name, metadata in self.datasets.items():
            info = asdict(metadata)
            info['cached'] = name in self.dataset_cache
            datasets_info.append(info)
        
        return datasets_info
    
    def export_config(self, config_path: str = "dataset_config.yaml"):
        """ğŸ’¾ å¯¼å‡ºé…ç½®"""
        config = {
            'data_dirs': [str(d) for d in self.data_dirs],
            'watch_mode': self.watch_mode,
            'auto_reload': self.auto_reload,
            'cache_enabled': self.cache_enabled,
            'datasets': {name: asdict(meta) for name, meta in self.datasets.items()},
            'stats': self.stats
        }
        
        if YAML_AVAILABLE and config_path.endswith(('.yaml', '.yml')):
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        else:
            # å¦‚æœæ²¡æœ‰yamlæˆ–è·¯å¾„ä¸æ˜¯yamlæ ¼å¼ï¼Œä½¿ç”¨json
            if config_path.endswith(('.yaml', '.yml')):
                config_path = config_path.replace('.yaml', '.json').replace('.yml', '.json')
            elif not config_path.endswith('.json'):
                config_path += '.json'
            
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ’¾ é…ç½®å·²å¯¼å‡ºåˆ°: {config_path}")
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if self._watching:
            self.stop_watching()


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
def demo_dynamic_dataset_manager():
    """æ¼”ç¤ºåŠ¨æ€æ•°æ®é›†ç®¡ç†å™¨"""
    print("ğŸš€ Dynamic Dataset Manager Demo")
    print("=" * 50)
    
    # åˆ›å»ºç®¡ç†å™¨
    manager = DynamicDatasetManager(
        data_dirs=["Data", "datasets"],
        watch_mode=True,
        auto_reload=True
    )
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = manager.get_stats()
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    # åˆ—å‡ºæ•°æ®é›†
    print(f"\nğŸ“‹ å¯ç”¨æ•°æ®é›† ({len(manager.datasets)}):")
    for name in manager.list_datasets():
        metadata = manager.datasets[name]
        print(f"  âœ… {name}: {metadata.size} é—®é¢˜ ({metadata.language}, {metadata.domain})")
    
    # è·å–åŠ¨æ€æ‰¹æ¬¡
    print(f"\nğŸ“¦ è·å–åŠ¨æ€æ‰¹æ¬¡:")
    batch = manager.get_dynamic_batch(batch_size=5)
    print(f"  æ‰¹æ¬¡ID: {batch.batch_id}")
    print(f"  æºæ•°æ®é›†: {batch.source_dataset}")
    print(f"  é—®é¢˜æ•°é‡: {len(batch.problems)}")
    
    # æ˜¾ç¤ºå‰2ä¸ªé—®é¢˜
    for i, problem in enumerate(batch.problems[:2]):
        print(f"  é—®é¢˜ {i+1}: {str(problem)[:100]}...")
    
    return manager


if __name__ == "__main__":
    demo_dynamic_dataset_manager() 