#!/usr/bin/env python3
"""
Refactored System Demo
=====================

Demonstration of the refactored mathematical reasoning system with:
1. Modular architecture
2. Comprehensive testing framework
3. Advanced evaluation system
"""

import logging
import os
import sys
import time
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def demo_reasoning_strategies():
    """Demonstrate the new reasoning strategy architecture"""
    print("\n" + "="*60)
    print("ğŸ§  REASONING STRATEGIES DEMO")
    print("="*60)
    
    try:
        from reasoning_core.strategies.base_strategy import (ReasoningResult,
                                                             ReasoningStep)
        from reasoning_core.strategies.chain_of_thought import \
            ChainOfThoughtStrategy
        from reasoning_core.strategies.enhanced_cotdir_strategy import \
            EnhancedCOTDIRStrategy

        # Test problems with different complexity levels
        problems = [
            {
                "text": "å°æ˜æœ‰3ä¸ªè‹¹æœï¼Œå°çº¢æœ‰5ä¸ªè‹¹æœï¼Œä»–ä»¬ä¸€å…±æœ‰å¤šå°‘ä¸ªè‹¹æœï¼Ÿ",
                "expected_answer": 8,
                "complexity": "L1"
            },
            {
                "text": "å°æ˜åŸæœ‰15ä¸ªè‹¹æœï¼Œç»™äº†å°çº¢5ä¸ªï¼Œåˆä¹°äº†8ä¸ªï¼Œå°æ˜ç°åœ¨æœ‰å¤šå°‘ä¸ªè‹¹æœï¼Ÿ",
                "expected_answer": 18,
                "complexity": "L2"
            },
            {
                "text": "ç­çº§æœ‰28ä¸ªå­¦ç”Ÿï¼Œè¦å¹³å‡åˆ†æˆ4ç»„ï¼Œæ¯ç»„æœ‰å¤šå°‘äººï¼Ÿ",
                "expected_answer": 7,
                "complexity": "L2"
            },
            {
                "text": "Janetæ¯å¤©äº§16ä¸ªé¸¡è›‹ï¼Œæ—©é¤åƒ3ä¸ªï¼Œåšè›‹ç³•ç”¨4ä¸ªï¼Œå‰©ä¸‹çš„ä»¥æ¯ä¸ª2ç¾å…ƒå–æ‰ï¼Œå¥¹æ¯å¤©èµšå¤šå°‘é’±ï¼Ÿ",
                "expected_answer": 18,
                "complexity": "L3"
            }
        ]
        
        # Test with basic Chain of Thought
        print("\nğŸ”— åŸºç¡€æ€ç»´é“¾ç­–ç•¥æµ‹è¯•:")
        basic_strategy = ChainOfThoughtStrategy()
        print(f"ç­–ç•¥: {basic_strategy.name}")
        
        for i, problem in enumerate(problems[:2], 1):
            print(f"\n--- é—®é¢˜ {i} (å¤æ‚åº¦: {problem['complexity']}) ---")
            print(f"é—®é¢˜: {problem['text']}")
            
            start_time = time.time()
            result = basic_strategy.solve(problem['text'])
            end_time = time.time()
            
            print(f"ç­”æ¡ˆ: {result.final_answer}")
            print(f"æœŸæœ›: {problem['expected_answer']}")
            print(f"æ­£ç¡®: {'âœ…' if str(result.final_answer) == str(problem['expected_answer']) else 'âŒ'}")
            print(f"ç½®ä¿¡åº¦: {result.confidence:.3f}")
            print(f"å¤„ç†æ—¶é—´: {end_time - start_time:.3f}s")
        
        # Test with Enhanced COT-DIR
        print(f"\nğŸš€ å¢å¼ºCOT-DIRç­–ç•¥æµ‹è¯•:")
        enhanced_strategy = EnhancedCOTDIRStrategy({
            'max_steps': 10,
            'confidence_threshold': 0.7,
            'validation_threshold': 0.8
        })
        print(f"ç­–ç•¥: {enhanced_strategy.name}")
        
        total_correct = 0
        total_problems = len(problems)
        
        for i, problem in enumerate(problems, 1):
            print(f"\n{'='*80}")
            print(f"ğŸ§® é—®é¢˜ {i} (å¤æ‚åº¦: {problem['complexity']})")
            print(f"é—®é¢˜: {problem['text']}")
            print(f"æœŸæœ›ç­”æ¡ˆ: {problem['expected_answer']}")
            
            start_time = time.time()
            result = enhanced_strategy.solve(problem['text'])
            end_time = time.time()
            
            # ç»“æœåˆ†æ
            is_correct = abs(float(result.final_answer or 0) - problem['expected_answer']) < 0.01
            if is_correct:
                total_correct += 1
            
            print(f"\nğŸ“Š æ¨ç†ç»“æœ:")
            print(f"  æœ€ç»ˆç­”æ¡ˆ: {result.final_answer}")
            print(f"  æ­£ç¡®æ€§: {'âœ… æ­£ç¡®' if is_correct else 'âŒ é”™è¯¯'}")
            print(f"  æ€»ä½“ç½®ä¿¡åº¦: {result.confidence:.3f}")
            print(f"  å¤„ç†æ—¶é—´: {end_time - start_time:.3f}s")
            print(f"  æˆåŠŸçŠ¶æ€: {result.success}")
            
            # æ˜¾ç¤ºæ¨ç†å±‚ä¿¡æ¯
            if result.metadata and 'reasoning_layers' in result.metadata:
                print(f"\nğŸ—ï¸ æ¨ç†å±‚åˆ†æ:")
                for layer in result.metadata['reasoning_layers']:
                    print(f"  {layer['level']}: ç½®ä¿¡åº¦={layer['confidence']:.3f}, æ“ä½œæ•°={layer['operations_count']}")
            
            # æ˜¾ç¤ºéªŒè¯ç»“æœ
            if result.metadata and 'validation_results' in result.metadata:
                print(f"\nâœ… 5ç»´éªŒè¯ç»“æœ:")
                for vr in result.metadata['validation_results']:
                    status = "é€šè¿‡" if vr['passed'] else "å¤±è´¥"
                    print(f"  {vr['dimension']}: {status} (åˆ†æ•°: {vr['score']:.3f})")
            
            # æ˜¾ç¤ºè¯¦ç»†æ¨ç†æ­¥éª¤
            print(f"\nğŸ” è¯¦ç»†æ¨ç†æ­¥éª¤:")
            for j, step in enumerate(result.reasoning_steps, 1):
                print(f"  æ­¥éª¤{j}: {step.explanation}")
                if step.metadata and "reasoning_level" in step.metadata:
                    print(f"    â””â”€ å±‚æ¬¡: {step.metadata['reasoning_level']}, ç½®ä¿¡åº¦: {step.confidence:.3f}")
            
            # æ˜¾ç¤ºå‘ç°çš„å®ä½“å’Œå…³ç³»
            if result.metadata:
                print(f"\nğŸ” å‘ç°ä¿¡æ¯:")
                print(f"  å®ä½“æ•°é‡: {result.metadata.get('entities_found', 0)}")
                print(f"  å…³ç³»æ•°é‡: {result.metadata.get('relations_discovered', 0)}")
                print(f"  å¤æ‚åº¦åˆ†çº§: {result.metadata.get('complexity_level', 'Unknown')}")
        
        # æ€»ä½“æ€§èƒ½æ‘˜è¦
        print(f"\n" + "="*80)
        print(f"ğŸ“ˆ Enhanced COT-DIR æ€§èƒ½æ‘˜è¦")
        print(f"="*80)
        
        accuracy = total_correct / total_problems * 100
        print(f"å‡†ç¡®ç‡: {accuracy:.1f}% ({total_correct}/{total_problems})")
        
        performance_summary = enhanced_strategy.get_performance_summary()
        print(f"å¹³å‡ç½®ä¿¡åº¦: {performance_summary['average_confidence']:.3f}")
        print(f"å¹³å‡å¤„ç†æ—¶é—´: {performance_summary['average_processing_time']:.3f}s")
        print(f"éªŒè¯é€šè¿‡ç‡: {performance_summary['validation_pass_rate']:.3f}")
        
        print(f"\nğŸ¯ COT-DIRæ ¸å¿ƒä¼˜åŠ¿å±•ç°:")
        print(f"1ï¸âƒ£ æ˜¾å¼å…³ç³»å‘ç° vs éšå¼æ¨ç†")
        print(f"   * ä¼ ç»Ÿæ–¹æ³•: ChatGPT/Qwenéšå¼å¤„ç†å…³ç³»")
        print(f"   * COT-DIR: æ˜ç¡®è¯†åˆ«æ¯ä¸ªå…³ç³»ï¼Œæä¾›æ•°å­¦å…¬å¼å’Œç½®ä¿¡åº¦")
        
        print(f"2ï¸âƒ£ ç»“æ„åŒ–å¤šå±‚æ¨ç† vs çº¿æ€§æ€ç»´")
        print(f"   * ä¼ ç»Ÿæ–¹æ³•: ç®€å•æ­¥éª¤åºåˆ—")
        print(f"   * COT-DIR: L1â†’L2â†’L3å±‚æ¬¡åŒ–æ¨ç†ï¼Œæ¯å±‚æœ‰æ˜ç¡®ä»»åŠ¡")
        
        print(f"3ï¸âƒ£ å…¨é¢ç½®ä¿¡åº¦éªŒè¯ vs æ— éªŒè¯")
        print(f"   * ä¼ ç»Ÿæ–¹æ³•: æ— éªŒè¯æœºåˆ¶")
        print(f"   * COT-DIR: 5ç»´åº¦éªŒè¯(è¯­æ³•ã€æ•°å­¦ã€é€»è¾‘ã€è¯­ä¹‰ã€ç›®æ ‡)")
        
        print(f"4ï¸âƒ£ å®Œæ•´ä¸­é—´ç»“æœè¿½è¸ª vs é»‘ç›’è¾“å‡º")
        print(f"   * ä¼ ç»Ÿæ–¹æ³•: åªæœ‰æœ€ç»ˆç­”æ¡ˆ")
        print(f"   * COT-DIR: æ¯æ­¥ä¸­é—´ç»“æœå…¨éƒ¨å¯è§å’Œè¿½è¸ª")
        
        print("\nâœ… æ¨ç†ç­–ç•¥æ¼”ç¤ºå®Œæˆ!")
        
    except ImportError as e:
        print(f"âŒ Could not import reasoning modules: {e}")
        return False
    
    return True


def demo_tool_integration():
    """Demonstrate external tool integration"""
    print("\n" + "="*60)
    print("ğŸ”§ TOOL INTEGRATION DEMO")
    print("="*60)
    
    try:
        from reasoning_core.tools.symbolic_math import SymbolicMathTool
        
        tool = SymbolicMathTool()
        
        print(f"Tool: {tool.name}")
        print(f"Available: {tool.is_available}")
        print(f"Supported operations: {tool.get_supported_operations()}")
        
        if tool.is_available:
            # Test operations
            test_operations = [
                ("solve_equation", "x + 2 - 5", "x"),
                ("simplify", "x + x + 2*x"),
                ("factor", "x**2 - 4")
            ]
            
            for operation, *args in test_operations:
                print(f"\n--- Testing {operation} ---")
                print(f"Input: {args}")
                
                result = tool.execute(operation, *args)
                print(f"Success: {result.success}")
                if result.success:
                    print(f"Result: {result.result}")
                else:
                    print(f"Error: {result.error_message}")
        
        print("\nâœ… Tool integration demo completed successfully!")
        
    except ImportError as e:
        print(f"âŒ Could not import tool modules: {e}")
        return False
    
    return True


def demo_evaluation_system():
    """Demonstrate the comprehensive evaluation system"""
    print("\n" + "="*60)
    print("ğŸ“Š EVALUATION SYSTEM DEMO")
    print("="*60)
    
    try:
        from evaluation.evaluator import ComprehensiveEvaluator
        from evaluation.metrics import AccuracyMetric, EfficiencyMetric

        # Sample data for evaluation
        predictions = [8, 120, 4, 15, 25]
        ground_truth = [8, 120, 4, 16, 25]  # One incorrect answer
        
        # Sample metadata
        metadata = {
            'processing_times': [0.5, 1.2, 0.8, 2.1, 0.9],
            'confidence_scores': [0.9, 0.85, 0.92, 0.7, 0.88],
            'reasoning_steps': [
                [{"confidence": 0.9}] * 3,  # 3 steps for first problem
                [{"confidence": 0.85}] * 4,  # 4 steps for second problem  
                [{"confidence": 0.92}] * 2,  # 2 steps for third problem
                [{"confidence": 0.7}] * 5,   # 5 steps for fourth problem
                [{"confidence": 0.88}] * 3   # 3 steps for fifth problem
            ],
            'explanations': [
                "Step by step addition calculation",
                "Distance = speed Ã— time calculation", 
                "Algebraic equation solving",
                "Complex arithmetic problem",
                "Simple multiplication"
            ]
        }
        
        # Initialize evaluator
        evaluator = ComprehensiveEvaluator()
        
        print("Evaluator initialized with metrics:")
        for metric_name in evaluator.get_available_metrics():
            print(f"  - {metric_name}")
        
        print(f"\nMetric weights:")
        weights = evaluator.get_metric_weights()
        for metric, weight in weights.items():
            print(f"  - {metric}: {weight:.2f}")
        
        # Run evaluation
        print(f"\nRunning evaluation on {len(predictions)} predictions...")
        
        result = evaluator.evaluate(
            predictions=predictions,
            ground_truth=ground_truth,
            dataset_name="demo_dataset",
            model_name="chain_of_thought_v1",
            metadata=metadata
        )
        
        print(f"\n--- Evaluation Results ---")
        print(f"Dataset: {result.dataset_name}")
        print(f"Model: {result.model_name}")
        print(f"Overall Score: {result.overall_score:.3f}")
        
        print(f"\nDetailed Metrics:")
        for metric_name, metric_result in result.metric_results.items():
            print(f"  {metric_name}: {metric_result.score:.3f}/{metric_result.max_score}")
            if 'error' not in metric_result.details:
                # Show some details
                if metric_name == 'accuracy':
                    details = metric_result.details
                    print(f"    Accuracy: {details['accuracy_percentage']:.1f}% ({details['correct']}/{details['total']})")
                elif metric_name == 'efficiency':
                    details = metric_result.details
                    print(f"    Avg time: {details['average_time']:.3f}s")
        
        print("\nâœ… Evaluation system demo completed successfully!")
        
    except ImportError as e:
        print(f"âŒ Could not import evaluation modules: {e}")
        return False
    
    return True


def demo_dataset_loading():
    """Demonstrate dataset loading capabilities"""
    print("\n" + "="*60)
    print("ğŸ“š DATASET LOADING DEMO")
    print("="*60)
    
    try:
        # Add Data directory to path
        sys.path.insert(0, str(Path(__file__).parent / "Data"))
        from dataset_loader import MathDatasetLoader
        
        loader = MathDatasetLoader()
        
        # List available datasets
        datasets = loader.list_datasets()
        print(f"Available datasets: {len(datasets)}")
        
        for dataset in datasets[:5]:  # Show first 5
            print(f"  - {dataset}")
        
        if datasets:
            # Load a small sample from first dataset
            print(f"\nLoading sample from {datasets[0]}...")
            data = loader.load_dataset(datasets[0], max_samples=3)
            
            print(f"Loaded {len(data)} samples:")
            for i, item in enumerate(data):
                print(f"  Sample {i+1}: {str(item)[:100]}...")
        
        print("\nâœ… Dataset loading demo completed successfully!")
        
    except ImportError as e:
        print(f"âŒ Could not import dataset loader: {e}")
        return False
    except Exception as e:
        print(f"âŒ Dataset loading error: {e}")
        return False
    
    return True


def demo_testing_framework():
    """Demonstrate the testing framework"""
    print("\n" + "="*60)
    print("ğŸ§ª TESTING FRAMEWORK DEMO")
    print("="*60)
    
    # Show test structure
    test_dir = Path(__file__).parent / "tests"
    if test_dir.exists():
        print("Test directory structure:")
        for item in test_dir.iterdir():
            if item.is_dir():
                test_count = len(list(item.glob("test_*.py")))
                print(f"  {item.name}/: {test_count} test files")
    
    # Show pytest configuration
    pytest_ini = Path(__file__).parent / "pytest.ini"
    if pytest_ini.exists():
        print(f"\nPytest configuration found: {pytest_ini}")
        with open(pytest_ini) as f:
            lines = f.readlines()[:10]  # Show first 10 lines
            for line in lines:
                print(f"  {line.rstrip()}")
    
    # Show available test commands
    print(f"\nAvailable test commands:")
    print(f"  pytest tests/unit_tests/ -v          # Run unit tests")
    print(f"  pytest tests/integration_tests/ -v   # Run integration tests")
    print(f"  pytest tests/performance_tests/ -v   # Run performance tests")
    print(f"  pytest -m smoke                      # Run smoke tests")
    print(f"  pytest -m 'not slow'                 # Run fast tests only")
    
    print("\nâœ… Testing framework demo completed successfully!")
    return True


def run_full_demo():
    """Run complete demonstration of refactored system"""
    print("ğŸš€ REFACTORED MATHEMATICAL REASONING SYSTEM DEMO")
    print("=" * 80)
    
    demos = [
        ("Reasoning Strategies", demo_reasoning_strategies),
        ("Tool Integration", demo_tool_integration), 
        ("Evaluation System", demo_evaluation_system),
        ("Dataset Loading", demo_dataset_loading),
        ("Testing Framework", demo_testing_framework)
    ]
    
    results = {}
    
    for demo_name, demo_func in demos:
        try:
            print(f"\nStarting {demo_name} demo...")
            start_time = time.time()
            success = demo_func()
            end_time = time.time()
            
            results[demo_name] = {
                'success': success,
                'time': end_time - start_time
            }
            
        except Exception as e:
            logger.error(f"Error in {demo_name} demo: {e}")
            results[demo_name] = {
                'success': False,
                'time': 0,
                'error': str(e)
            }
    
    # Summary
    print("\n" + "="*80)
    print("ğŸ“‹ DEMO SUMMARY")
    print("="*80)
    
    total_time = sum(r['time'] for r in results.values())
    successful_demos = sum(1 for r in results.values() if r['success'])
    
    print(f"Total demos run: {len(demos)}")
    print(f"Successful demos: {successful_demos}")
    print(f"Total time: {total_time:.2f}s")
    
    print(f"\nDetailed results:")
    for demo_name, result in results.items():
        status = "âœ… PASS" if result['success'] else "âŒ FAIL"
        time_str = f"{result['time']:.2f}s"
        print(f"  {demo_name:20} {status:8} {time_str:>8}")
        
        if not result['success'] and 'error' in result:
            print(f"    Error: {result['error']}")
    
    # Architecture overview
    print(f"\n" + "="*80)
    print("ğŸ—ï¸  REFACTORED ARCHITECTURE OVERVIEW")
    print("="*80)
    
    print("New modular structure:")
    print("  src/")
    print("  â”œâ”€â”€ reasoning_core/        # Core reasoning engine")
    print("  â”‚   â”œâ”€â”€ strategies/       # Different reasoning strategies")
    print("  â”‚   â”œâ”€â”€ tools/           # External tool integration")
    print("  â”‚   â””â”€â”€ validation/      # Validation mechanisms")
    print("  â”œâ”€â”€ evaluation/           # Comprehensive evaluation system")
    print("  â”‚   â”œâ”€â”€ metrics.py       # Individual metrics")
    print("  â”‚   â”œâ”€â”€ evaluator.py     # Main evaluation engine")
    print("  â”‚   â””â”€â”€ reports.py       # Report generation")
    print("  â””â”€â”€ ... (existing modules)")
    print("")
    print("  tests/")
    print("  â”œâ”€â”€ unit_tests/          # Unit tests")
    print("  â”œâ”€â”€ integration_tests/   # Integration tests")
    print("  â””â”€â”€ performance_tests/   # Performance benchmarks")
    print("")
    print("  demos/                   # Organized demo files")
    print("  config_files/            # Configuration files")
    print("  legacy/                  # Legacy code")
    
    return successful_demos == len(demos)


if __name__ == "__main__":
    success = run_full_demo()
    
    if success:
        print(f"\nğŸ‰ All demos completed successfully!")
        print(f"The refactored system is ready for use.")
    else:
        print(f"\nâš ï¸  Some demos failed. Check the error messages above.")
        print(f"You may need to install missing dependencies or fix import paths.")
    
    sys.exit(0 if success else 1) 